{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:Center; color:orange;\">- Brooks Tools - Microbiome Analysis Pipeline -</h1>\n",
    "<h1 style=\"text-align:center; color:black;\">------------------------------------------------------------------------------</h1>\n",
    "<h4 style=\"text-align:center; color:blue;\">Andrew W. Brooks</h4>\n",
    "<h4 style=\"text-align:center; color:blue;\">Vanderbilt Genetics Institute</h4>\n",
    "<h4 style=\"text-align:center; color:blue;\">andrew.w.brooks(at)vanderbilt.edu</h4>\n",
    "<h1 style=\"text-align:center; color:black;\">------------------------------------------------------------------------------</h1>\n",
    "<h4 style=\"text-align:center; color:black;\">Released under MIT License</h4>\n",
    "<h4 style=\"text-align:center; color:black;\">Copyright (c) 2017 Andrew W. Brooks</h4>\n",
    "<h4 style=\"text-align:center; color:black;\">Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. The software is provided \"as is\", without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. In no event shall the authors or copyright holders be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with the software or the use or other dealings in the software.</h4>\n",
    "<h4 style=\"text-align:center; color:red;\"></h4>\n",
    "<h1 style=\"text-align:center; color:black;\">------------------------------------------------------------------------------</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; color:orange;\"> - Prepare Toolkit - </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align:center; color:blue;\"> - Load Libraries and Environment - </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "__author__ = \"Andrew Brooks\"\n",
    "__version__ = \"1.0\"\n",
    "__license__ = \"MIT\"\n",
    "__copyright__ = \"Andrew Brooks\"\n",
    "__email__ = \"andrew.w.brooks@vanderbilt.edu\"\n",
    "__status__ = \"Alpha\"\n",
    "\n",
    "##### BROOKS UTILITIES #####################################################\n",
    "\n",
    "##### LOAD PACKAGES #####\n",
    "### GENERAL UTILITIES ###\n",
    "import glob         # TO REGEX SEARCH FOR FILES \n",
    "import os           # TO MAKE COMMAND LINE CALLS ()\n",
    "import random       # TO GENERATE RANDOM VALUES\n",
    "import copy         # TO COPY COMPLICATED OBJECTS\n",
    "\n",
    "### DATA STRUCTURES ###\n",
    "import pandas as pd # PANDAS DATAFRAMES\n",
    "from IPython.display import display # USE TO DISPLAY PANDAS\n",
    "import numpy as np  # NUMPY NUMERICAL AND LINEAR ALGEBRA TOOLKIT\n",
    "random.seed(54321)  # SET RANDOM SEED FOR REPRODUCIBILITY #\n",
    "\n",
    "### DATA PROCESSING\n",
    "import scipy as sp  # SCIPY SCIENTIFIC TOOLKIT\n",
    "from scipy.cluster.hierarchy import linkage # UPGMA CLUSTER TOOL\n",
    "import skbio as sk  # SCIKIT-BIO TOOLS FOR BIOM ANALYSES\n",
    "from skbio.stats.distance import DistanceMatrix # DISTANCE MATRIX OBJECT FOR BETA DIVERSITY\n",
    "from skbio.tree import TreeNode # BACTERIAL PHYLOGENY OBJECT\n",
    "import itertools # ACCESS DATA ITERATION COMBINATIONS\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests # PERFORM FDR P-VALUE CORRECTION\n",
    "import statsmodels.api as sm\n",
    "from decimal import Decimal\n",
    "import patsy # FORMAT DATA FOR REGRESSION ANALYSES \n",
    "\n",
    "### BIOM JSON OBJECTS ###\n",
    "from biom.table import Table # BIOM TABLE COMPLEX DATA STRUCTURE\n",
    "from biom import load_table  # LOAD BIOM TABLE FROM FILE (.BIOM)\n",
    "\n",
    "### PLOTTING ###\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt # MATPLOTLIB PLOTTING TOOLS\n",
    "from matplotlib.backends.backend_pdf import PdfPages # MATPLOTLIB SAVE MULTIPAGE PDFS\n",
    "import seaborn as sns # SEABORN PLOTTING TOOLS (FRIENDLY WITH PANDAS)\n",
    "from emperor import Emperor # EMPEROR PCOA PLOTING\n",
    "\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align:center; color:blue;\"> - Load Basic Tools - </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0,
     23
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### BASIC TOOLS - I/O ###################################################################\n",
    "\n",
    "##### LIST #####\n",
    "### PRINT A LIST LINE BY LINE TO CONSOLE ###\n",
    "def list_print(listIn): \n",
    "    for listValIn in listIn: print(listValIn)\n",
    "### WRITE A LIST LINE BY LINE TO FILE (OVERWRITES FILE) ###\n",
    "def list_write(listIn, filePathIn):\n",
    "    fOut = open(filePathIn, 'w')\n",
    "    for listValIn in listIn:\n",
    "        fOut.write((listValIn+\"\\n\"))\n",
    "    fOut.close()\n",
    "    \n",
    "### WRITE A LIST LINE BY LINE TO FILE (APPENDS TO FILE) ###\n",
    "def list_append(listIn, filePathIn, addBlankTail=True):\n",
    "    fOut = open(filePathIn, 'a')\n",
    "    for listValIn in listIn:\n",
    "        fOut.write((listValIn+\"\\n\"))\n",
    "    if addBlankTail == True: \n",
    "        fOut.write(\"\\n\")\n",
    "    fOut.close()\n",
    "    \n",
    "### TAKE IN LIST OF STRINGS AND CONVERT TO SINGLE STRING SEPARATED BY \\r ###\n",
    "def list_to_string(listIn):\n",
    "    strOut = \"\"\n",
    "    for listCycle in listIn: strOut = strOut + listCycle + \"\\r\"\n",
    "    return strOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align:center; color:blue;\"> - Load Statistics Tools - </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### STATISTICS - FREQUENTIST ############################################################\n",
    "    \n",
    "##### MANN WHITNEY U TEST ##### list of results\n",
    "def stats_mannwhitney(l1, l2, l1Name=None, l2Name=None, bonferroniComparisons=1):\n",
    "    # use_continuity = Whether a continuity correction (1/2.) should be taken into account. Default is True. [Scipy]\n",
    "    outMann = sp.stats.mannwhitneyu(l1, l2, use_continuity=True, alternative='two-sided')\n",
    "    outStrList = [\"Mann Whitney U - Nonparametric Rank Test\"]\n",
    "    if (l1Name != None) & (l2Name != None): outStrList.append(\"    Comparing: \"+l1Name+\" with \"+l2Name)\n",
    "    outStrList.append(\"    List #1 Length: \"+str(len(l1))+\" | List #2 Length: \"+str(len(l2)))\n",
    "    outStrList.append(\"    Test Statistic: \"+str(outMann[0]))\n",
    "    # PRINT P-VALUES FOR ONE AND TWO TAILED\n",
    "    outStrList.append(\"    P-Value (onetailed): \"+str(outMann[1]/2))\n",
    "    outStrList.append(\"    P-Value (twotailed): \"+str(outMann[1]))\n",
    "    # IF BONFERRONI CORRECTION FOR MORE THAN ONE TEST: CORRECT AND CHECK IF LARGER THAN 1\n",
    "    if (bonferroniComparisons != 1) and (bonferroniComparisons != None):\n",
    "        if (outMann[1]/2)*bonferroniComparisons <= 1: outStrList.append(\"    P-Value Bonferroni Corrected for \"+str(bonferroniComparisons)+\" tests: \"+str((outMann[1]/2)*bonferroniComparisons))\n",
    "        else: outStrList.append(\"    P-Value Bonferroni Corrected for \"+str(bonferroniComparisons)+\" tests: \"+str(1.0))\n",
    "        if ((outMann[1])*bonferroniComparisons) <= 1: outStrList.append(\"    P-Value Bonferroni Corrected for \"+str(bonferroniComparisons)+\" tests (twotailed): \"+str(outMann[1]*bonferroniComparisons))\n",
    "        else: outStrList.append(\"    P-Value Bonferroni Corrected for \"+str(bonferroniComparisons)+\" tests (twotailed): \"+str(1.0))\n",
    "    return outStrList, outMann[0], outMann[1]\n",
    "                    \n",
    "\n",
    "### TAKE IN LIST OF P-VALUES AND RETURN DATAFRAME OF P-VALUES & FDR &  BONFERRONI CORRECTED P-VALUES ###\n",
    "def stats_pcorrect(pValList,alphaIn=0.05):\n",
    "    ### GET FDR P-VALUES ###\n",
    "    fdrResIn = multipletests(pValList, alpha=alphaIn, method='fdr_bh')\n",
    "    fdrPVals = fdrResIn[1]\n",
    "    ### GET BONFERRONI P-VALUES ###\n",
    "    bonferroniPVals = np.multiply(pValList, len(pValList))\n",
    "    ### FORMAT RESULTS AS DATAFRAME ###\n",
    "    dfResults = pd.DataFrame({'p-raw':pValList, 'p-fdr':fdrPVals, 'p-bonferroni':bonferroniPVals})\n",
    "    ### REMOVE BONFERRONI P-VALUES > 1 ###\n",
    "    dfResults[dfResults['p-bonferroni'] > 1] = 1.0\n",
    "    ### RETURN DATAFRAME OF RAW, FDR, AND BONFERRONI CORRECTED P-VALUES ###\n",
    "    return dfResults[['p-raw','p-fdr','p-bonferroni']]  \n",
    "\n",
    "### LINEAR REGRESSION - ORDINARY LEAST SQUARES\n",
    "# regResults, regDependent, regPredictors = stats_regression(mapDF, regEquation=\" age_years ~ bmi + race + race:sex \")\n",
    "def stats_regression(pdDFIn, regEquation=\" age_years ~ bmi + sex:race \"):\n",
    "    ### GET VARIABLES INTO X[n,p] predictors and y[n,1] outcome\n",
    "    y, X = patsy.dmatrices(regEquation, pdDFIn, return_type='dataframe')\n",
    "    ### GENERATE OLS REGRESSION MODEL ###\n",
    "    statsModel = sm.OLS(y, X)\n",
    "    ### FIT DATA TO A LINE USING OLS MINIMIZATION ###\n",
    "    statsModelFit = statsModel.fit()\n",
    "    ### PRINT RESULTS ###\n",
    "    print(statsModelFit.summary())\n",
    "    ### RETURN: resultsObject, y, X\n",
    "    return statsModelFit, y, X\n",
    "\n",
    "### LINEAR REGRESSION - RIDGE REGRESSION - COEFFICIENTS PENALTIES\n",
    "# regResults, regDependent, regPredictors = stats_regression_ridge(mapDF, regEquation=\" age_years ~ bmi + race + race:sex \", alphaIn=0.5)\n",
    "def stats_regression_ridge(pdDFIn, regEquation=\" age_years ~ bmi + sex:race \", alphaIn=0.5):\n",
    "    ### GET VARIABLES INTO X[n,p] predictors and y[n,1] outcome\n",
    "    y, X = patsy.dmatrices(regEquation, pdDFIn, return_type='dataframe')\n",
    "    ### GENERATE OLS REGRESSION MODEL ###\n",
    "    statsModel = sl.linear_model.Ridge(alpha = alphaIn)\n",
    "    ### FIT DATA TO A LINE USING OLS MINIMIZATION ###\n",
    "    statsModelFit = statsModel.fit(X,y)\n",
    "    ### PRINT RESULTS ###\n",
    "    ###print(statsModelFit.summary())\n",
    "    \n",
    "    # The coefficients\n",
    "    print('Coefficients: \\n', statsModelFit.coef_)\n",
    "    # The mean squared error\n",
    "    print(\"Mean squared error: %.2f\"\n",
    "          % np.mean((statsModelFit.predict(X) - y) ** 2))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Variance score: %.2f' % statsModelFit.score(X, y))\n",
    "    ### RETURN: resultsObject, y, X\n",
    "    return statsModelFit, y, X\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align:center; color:blue;\"> - Load Pandas Tools - </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     147,
     177,
     190,
     196,
     259
    ],
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####### PANDAS & STATISTICAL TOOLS (DataFrame - pandas ) ####################################\n",
    "# Class to contain a pandas dataframe and additional info\n",
    "# Tools include: Statistics, Plotting\n",
    "class DF:\n",
    "    \"\"\"\n",
    "    ### DF PANDAS DATAFRAME TOOLKIT ###\n",
    "    \n",
    "    \n",
    "    INPUT:\n",
    "        - objIn can be:\n",
    "            - str: will search as path for delimited table\n",
    "            - pd.DataFrame(): will initialize as copy\n",
    "            - ndarray(structured dtype), list of tuples, dict, or DataFrame\n",
    "            \n",
    "            \n",
    "            \n",
    "            - dict{key:item,key:item}: will convert\n",
    "            - !!! List[of tuples (1,'a',True),(2,'b',False)]\n",
    "    \n",
    "    USE: \n",
    "        \n",
    "    \n",
    "        DF( pathIn=None, sepIn='\\t', ### IMPORT FROM FILE... ###\n",
    "            dictIn=None, ### ...OR IMPORT FROM DICT... ###\n",
    "            dataIn=None, columnsIn=None, indexIn=None, ### ...OR BUILD FROM DATA ###\n",
    "            verboseIn=True, ### WHETHER TO PRINT ###\n",
    "                )\n",
    "    INPUT:\n",
    "       keys           : column name or list[column names]\n",
    "       dropIn=True      : delete columns to become index\n",
    "       appendIn=False   : add to existing \n",
    "       inplace=False  : perform on target dataframe\n",
    "    RETURN: None\n",
    "    TYPE: function\n",
    "    STRUCTURE: DF()\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ### CLASS INITIALIZATION ##################################################\n",
    "    def __init__(self, objIn, columnsIn=None, indexIn=None, sepIn='\\t', verboseIn=True):\n",
    "        ### DATA STRUCTURES ##############################\n",
    "        ##### VERBOSE BOOL .verbose #########\n",
    "        self.verbose = verboseIn\n",
    "        ##### SEPARATOR BOOL .sep #########- Default '\\t'\n",
    "        self.sep=sepIn\n",
    "        ##### INPUT PATH .path #########\n",
    "        self.source=objIn\n",
    "        ### INDEX AND COLUMNS ###\n",
    "        if columnsIn is None: columnsIn=[]\n",
    "        if indexIn is None: indexIn=[]\n",
    "        self.columns=columnsIn;self.c=columnsIn\n",
    "        self.index=indexIn;self.i=indexIn\n",
    "        self.palette = None\n",
    "        \n",
    "        ##### DATAFRAME .df #########\n",
    "        iB=False # BOOL INPUT SUCCESS #\n",
    "        if isinstance(self.source, str): self.in_tsv();iB=True ### IF STRING IMPORT AS TSV PATH ###\n",
    "        elif self.source is None: ### IF NONE THEN MAKE EMPTY DATAFRAME\n",
    "            self.df = pd.DataFrame(index=indexIn, columns=columnsIn, dtype=None, copy=False)\n",
    "            self.df.fillna(0); iB = True\n",
    "        elif (isinstance(self.source, int)) or (isinstance(self.source,float)):\n",
    "            self.df = pd.DataFrame(index=indexIn, columns=columnsIn, dtype=None, copy=False)\n",
    "            self.df.fillna(self.source); iB = True\n",
    "        elif isinstance(self.source, pd.DataFrame): self.df = self.source.copy();iB=True ### IF DF THEN COPY ##\n",
    "        elif isinstance(self.source, dict): self.in_dict(self.source);iB=True ### IF DICT THEN FORMAT AS DF ###\n",
    "        else:  ###  TRY IMPORTING AS OBJECT ### \n",
    "            self.in_obj(self.source);iB=True\n",
    "             \n",
    "        if not iB: \n",
    "            if self.verbose is True: \" - Initializing as Nan Dataframe - \"\n",
    "            self.df = pd.DataFrame(index=indexIn, columns=columnsIn, dtype=None, copy=False);self.df.fillna(0)\n",
    "        \n",
    "        ### RESET SOURCE FOR RESET FUNCTION ###\n",
    "        self.source = self.copy()\n",
    "        \n",
    "        ##### UPDATE FROM DATAFRAME ######################\n",
    "        self.update()\n",
    "        \n",
    "    ##########################################################################\n",
    "    ########################## COPY ##########################################\n",
    "    ### COPY .copy() Return unconnected copy of class object.\n",
    "    def copy(self): return copy.deepcopy(self)\n",
    "\n",
    "    ##########################################################################\n",
    "    ########################## DISPLAY #######################################\n",
    "    \n",
    "    ### DISPLAY IN NOTEBOOK ###\n",
    "    def d(self): display(self.df)\n",
    "    def display(self): display(self.df)\n",
    "    ### DISPLAY PANDAS - SCIENTIFIC NOTATION ###\n",
    "    def display_scientific(self, dispPos=3): pd.set_option('display.float_format', '{:.'+str(dispPos)+'g}'.format)\n",
    "    ### DISPLAY PANDAS - FLOAT NOTATION ###\n",
    "    def display_float(self, dispPos=3): \n",
    "        pd.set_option('display.float_format', '{:.5f}'.format)\n",
    "    ### DISPLAY PANDAS - MAX COLUMN WIDTH (-1 = No Wrap) ###\n",
    "    def display_colwidth(self, widthIn=-1): pd.set_option('display.max_colwidth', widthIn)\n",
    " \n",
    "    ##########################################################################\n",
    "    ########################### I/O ##########################################    \n",
    "    \n",
    "    ### DF FROM TSV ### -> self\n",
    "    def in_tsv(self):\n",
    "        if self.verbose is True: print(\" - Loading DF File - \" + self.source);\n",
    "        self.df = pd.read_csv(self.source, sep=self.sep, index_col=None, skiprows=0, verbose=False, low_memory=False) \n",
    "        if self.verbose is True: print(\" - SUCCESS: Loaded DF from: \"+self.source+\" - \")\n",
    "        return self\n",
    "    ### DF FROM DICTIONARY ### -> self\n",
    "    def in_dict(self, dictIn):\n",
    "        if self.verbose is True: print(\" - Creating DF from Dictionary - \")\n",
    "        self.df = pd.DataFrame.from_dict(dictIn, orient='columns', dtype=None)\n",
    "        if self.verbose is True: print(\" - SUCCESS: Created DF from Dictionary - \")\n",
    "        return self\n",
    "    ### DF FROM RECORD ###  -> self (objIn i.e. ndarray(i.e.structured dtype), list of tuples, dict, or DataFrame\n",
    "    def in_obj(self, objIn):\n",
    "        if self.verbose is True: print(\" - Creating DF from Object - \"+self.columns)\n",
    "        self.df = pd.DataFrame.from_records(data=objIn, columns=self.columns, index=self.index, coerce_float=True)\n",
    "        if self.verbose is True: print(\" - SUCCESS: Created DF from Object - \")\n",
    "        return self\n",
    "    ### DF TO TSV FILE ### -> self\n",
    "    def out_tsv(self, outPath):\n",
    "        if self.verbose is True: print(\" - Writing DF to File \"+outPath+\" - \")\n",
    "        self.df.to_csv(outPath, sep=self.sep)\n",
    "        if self.verbose is True: print(\" - Written DF to File \"+outPath+\" - \")\n",
    "        return self\n",
    "    \n",
    "    ##########################################################################\n",
    "    ############################ PALETTE #####################################\n",
    "    ### DISPLAY A PREVIEW OF THE PALETTE ...ooooh pretty ###\n",
    "    def palette_preview(self): sns.palplot(self.palette)\n",
    "    ### SET .palette FOR A RAINBOW OF COLORS ###\n",
    "    def palette_heatmap(self, numColors): self.palette = sns.palplot(sns.color_palette(\"coolwarm\", numColors))\n",
    "    ### SET .palette FOR A HEATMAP OF COLORS ###\n",
    "    def palette_rainbow(self, numColors, lightIn=0.5, saturationIn=0.8, previewIn=False): self.palette = sns.hls_palette(numColors, l=lightIn, s=saturationIn)\n",
    "\n",
    " \n",
    "    ##########################################################################\n",
    "    ########################### PLOTTING #####################################\n",
    "    ### LINEAR MODEL PLOT ### continuousXcontinuous, 95%CI, can color and split plots by categories.\n",
    "    def plot_lm(self, continuousColX, continuousColY, categoricalColor=None, categoricalRow=None, categoricalColumn=None):\n",
    "        ### SET STYLE ###\n",
    "        sns.set(style=\"ticks\", context=\"talk\"); pal=None\n",
    "        ### GET COLOR PALETTE ###\n",
    "        self.palette_rainbow(len(self.df[categoricalColor].unique()), lightIn=0.7, saturationIn=1.0 )\n",
    "        plt.clf();plt.title('Winning')\n",
    "        ### MAKE LINEAR MODEL PLOT ###\n",
    "        return sns.lmplot(x=continuousColX,y=continuousColY,row=categoricalRow,col=categoricalColumn,hue=categoricalColor,data=self.df,size=7, palette=self.palette, ci=95)    \n",
    "    ### VIOLIN PLOT ### categoricalXcontinuous, color ideal with two groups.\n",
    "    def plot_violin(self, categoricalColX, continuousColY, categoricalColor=None):\n",
    "        ### IF CATEGORICAL COLOR ON EACH HALF OF VIOLIN ###\n",
    "        splitIn = False; \n",
    "        if len(self.df[categoricalColor].unique()) == 2: splitIn = True\n",
    "        ### GET COLOR PALETTE ###\n",
    "        self.palette_rainbow(len(self.df[categoricalColor].unique()), lightIn=0.7, saturationIn=1.0 )\n",
    "        ### MAKE VIOLIN PLOT ###\n",
    "        return sns.violinplot(x=categoricalColX, y=continuousColY, hue=categoricalColor, data=self.df, split=splitIn, palette=self.palette)\n",
    "        #sns.despine(left=True)\n",
    "    \n",
    "    ##########################################################################\n",
    "    ########################## RESET #########################################\n",
    "    ### RESET .reset()\n",
    "    def reset(self): return self.__init__(self.source)\n",
    "    \n",
    "    ##########################################################################\n",
    "    ########################## STATS #########################################\n",
    "    \n",
    "    ### REPLACE IN DATAFRAME ###\n",
    "    def replace(self, toReplace, replaceWith): self.df.replace(to_replace=toReplace, value=replaceWith, inplace=True, limit=None, regex=False, method='pad')\n",
    "  \n",
    "    ### REPLACE STRING WITH REGEX ###\n",
    "    def replace_string(self, regexToReplace, replaceWithStr): self.df.replace(to_replace=regexToReplace, value=replaceWithStr, inplace=True, limit=None, regex=True)    \n",
    "\n",
    "    ##########################################################################\n",
    "    ########################## STATS ####################################\n",
    "    ##### .stats_X STATISTICS FUNCTIONS #####################\n",
    "    \n",
    "    ### LINEAR REGRESSION - ORDINARY LEAST SQUARES\n",
    "    # regResults, regDependent, regPredictors = stats_regression(mapDF, regEquation=\" age_years ~ bmi + race + race:sex \")\n",
    "    def stats_regression(self, regEquation=\" age_years ~ bmi + sex:race \"):\n",
    "        ### GET VARIABLES INTO X[n,p] predictors and y[n,1] outcome\n",
    "        y, X = patsy.dmatrices(regEquation, self.df, return_type='dataframe')\n",
    "        ### GENERATE OLS REGRESSION MODEL ###\n",
    "        statsModel = sm.OLS(y, X)\n",
    "        ### FIT DATA TO A LINE USING OLS MINIMIZATION ###\n",
    "        statsModelFit = statsModel.fit()\n",
    "        ### PRINT RESULTS ###\n",
    "        print(statsModelFit.summary())\n",
    "        ### RETURN: resultsObject, y, X\n",
    "        return statsModelFit, y, X\n",
    "    \n",
    "    ### MANN-WHITNEY-U ON TWO COLUMNS ###\n",
    "    def stats_mwu(self,continuousColOne,continuousColTwo):\n",
    "        testStat, pValue = sp.stats.mannwhitneyu(self.df[continuousColOne], self.df[continuousColTwo], use_continuity=True, alternative='two-sided')\n",
    "        if self.verbose is True: print(\" - Mann-Whitney-U: P=\"+str('%.2E' % Decimal(pValue))+\" TS=\"+str(testStat)+\" [ \"+continuousColOne+\" : \"+continuousColTwo+\" ] - \")\n",
    "        return pValue, testStat \n",
    "    \n",
    "    ### MANN-WHITNEY-U PAIRWISE CONTINUOUS VARIABLE BY CATEGORICAL COLUMN ###\n",
    "    def stats_mwu_pairwise(self,continuousColumn,categoricalColumn):\n",
    "        if self.verbose is True: print(\" - Comparing groups of \"+categoricalColumn+\" by \"+continuousColumn+\" - \")\n",
    "        resDF = DF(None, columnsIn=['group1','group2','p','t'])\n",
    "        ### GET ALL PAIRWISE COMBINATIONS OF DISTANCE MATRICES ### \n",
    "        for catIDX, (cat1,cat2) in enumerate(itertools.combinations(self.df[categoricalColumn].unique(),2)):\n",
    "            ### CALCULATE MANN-WHITNEY-U ###\n",
    "            testStat, pValue = sp.stats.mannwhitneyu(self.df[self.df[categoricalColumn]==cat1][continuousColumn] , self.df[self.df[categoricalColumn]==cat2][continuousColumn], use_continuity=True, alternative='two-sided')\n",
    "            if self.verbose is True: \n",
    "                if pValue > 0.00001: pPrint = str(pValue)\n",
    "                else: pPrint = str('%.2E' % Decimal(pValue))\n",
    "                print(\" - Mann-Whitney-U: p=\"+ pPrint+\" | t=\"+str(testStat)+\" [ \"+cat1+\"(n=\"+str(len(self.df[self.df[categoricalColumn]==cat1][continuousColumn]))+\") : \"+cat2+\"(n=\"+str(len(self.df[self.df[categoricalColumn]==cat2][continuousColumn]))+\")] - \")\n",
    "            resDF.df.loc[catIDX] = [cat1,cat2,pValue,testStat]\n",
    "        return resDF\n",
    "    ##########################################################################\n",
    "    ############################ SET #####################################\n",
    "    ##### .set_X SET VALUES IN DATAFRAME ##########################\n",
    "    \n",
    "    \n",
    "    ### SET ###\n",
    "    # INPUT: value - value to add to dataframe\n",
    "    #              - can be None, int, str - fills whole df or rows or columns provided\n",
    "    #              - can be list\n",
    "    # INPUT COMBINATIONS:\n",
    "    #   - if no column or row: fill whole dataframe with value\n",
    "    #   - if column: fill column with value (can be int, str, None, or List[n=])\n",
    "    def s(self, value, column=None, index=None, fillNone=None):\n",
    "        \n",
    "        # IF NO COLUMN OR INDEX PROVIDED... FILL WHOLE DF WITH VALUE\n",
    "        if (column is None) and (index is None):self.df.loc[self.df.index][seld.df.columns] = value;return self\n",
    "        \n",
    "        # IF COLUMN PROVIDED...\n",
    "        if (column is not None):\n",
    "            # IF COLUMN IS LIST... \n",
    "            if isinstance(column, list):\n",
    "                # FOR EACH COLUMN IN LIST... SET TO VALUE #\n",
    "                for curCol in column: self.df[curCol] = value\n",
    "            # IF COLUMN IS NOT LIST... TRY TO SET COLUMN TO VALUE #\n",
    "            else: self.df[column] = value\n",
    "        \n",
    "        # IF INDEX PROVIDED...\n",
    "        if (index is not None):\n",
    "            # IF index IS LIST...\n",
    "            if isinstance(index, list):\n",
    "                # FOR EACH INDEX IN LIST... SET TO VALUE #\n",
    "                for curIdx in index: self.df.loc[curIdx] = value\n",
    "            # IF index IS NOT LIST... TRY TO SET COLUMN TO VALUE #\n",
    "            else: self.df.loc[index] = value\n",
    "        \n",
    "        # SET BY INDEX AND COLUMN\n",
    "        self.df.loc[index][column] = value\n",
    "         \n",
    "        ### IF NOT A DATAFRAME THEN CREATE WITH index AND columns FILLED BY value ###\n",
    "        #self.df = pd.DataFrame(index=index, columns=column, dtype=None, copy=False);self.df.fillna(value)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def set_(self, value, indexName, colName):\n",
    "        self.df.set_value(indexName, colName, value); return self\n",
    "\n",
    "    ### SET INDEX COLUMN OR COLUMNS ###\n",
    "    def set_i(self, colNameOrList, drop=True, append=False):\n",
    "        \"\"\"\n",
    "        USE: DF.set_index('name', dropIn=True, appendIn=False)\n",
    "        INPUT:\n",
    "           keys           : column name or list[column names]\n",
    "           drop=True      : delete columns to become index\n",
    "           append=False   : add to existing \n",
    "           inplace=False  : perform on target dataframe\n",
    "        RETURN: None\n",
    "        TYPE: function\n",
    "        STRUCTURE: DF()\n",
    "        \"\"\"\n",
    "        self.df.set_index(colNameOrList, drop=drop, append=append, inplace=True, verify_integrity=False); return self\n",
    "    \n",
    "    def set_r(self, newRow, index): self.df.loc[index] = newRow; return self\n",
    "    def set_c(self, newCol, colName): self.df[colName] = newCol; return self\n",
    "    \n",
    "    ##########################################################################\n",
    "    ############################# UPDATE #####################################\n",
    "    ### UPDATE WRAPPER .i .index .c .columns ###\n",
    "    def update(self):\n",
    "        self.update_i() # Update Index .i .index\n",
    "        self.update_c() # Update Columns .c .columns\n",
    "        self.palette_rainbow(20)\n",
    "        return self\n",
    "    ### UPDATE INDEX .i ###\n",
    "    def update_i(self): self.i = self.df.index; self.index = self.df.index; return self\n",
    "    ### UPDATE COLUMNS .c ###\n",
    "    def update_c(self): self.c = self.df.columns; self.columns = self.df.columns; return self\n",
    "    \n",
    "    \n",
    "    ##########################################################################\n",
    "    ############################## YIELD #####################################\n",
    "    ##### .yield_X YIELD ITERABLES ##########################\n",
    "    \n",
    "    ### YIELD BY INDICES (rows) ###\n",
    "    def yield_i(self): \n",
    "        for iCur in self.i: yield self.df.loc[[iCur]]\n",
    "    ### YIELD BY COLUMNS (cols) ###\n",
    "    def yield_c(self): \n",
    "        for cCur in self.c: yield self.df[[cCur]]\n",
    "    \n",
    "    \n",
    "    ##########################################################################\n",
    "    ############################## PLAYPEN ###################################\n",
    "    \n",
    "    #### ADD COLUMN TO DATAFRAME ####\n",
    "    #def add_c(self, newColumn, name=None, overwrite=True):\n",
    "    #    newColumn = pd.Series(data=newColumn, index=self.df.index, dtype=None, name=name, copy=False)\n",
    "    #    self.df = pd.concat([self.df,newColumn], axis=0, join='outer', join_axes=useIndex, ignore_index=overwrite,\n",
    "    #                            keys=None, levels=None, names=None, verify_integrity=False,copy=True)\n",
    "    #### ADD ROW TO DATAFRAME ###\n",
    "    #def add_r(self, newRow, name=None, overwrite=True):    \n",
    "    #    newRow = pd.Series(data=newRow, index=self.df.columns, dtype=None, name=name, copy=False)\n",
    "    #    self.df = pd.concat([self.df,newRow], axis=0, join='outer', join_axes=None, ignore_index=overwrite,\n",
    "    #                            keys=None, levels=None, names=None, verify_integrity=False,copy=True)\n",
    "    \n",
    "    #### MERGE DATAFRAME CLASS OBJECT INTO CURRENT OBJECT ###\n",
    "    #def merge(self, inDF, useIndex=None, unionIn=False, replaceColnames=False):\n",
    "    #    if unionIn is true: joinIn = 'inner'\n",
    "    #    else: joinIn = 'outer'\n",
    "    #    return DF(pd.concat([self.df,inDF], axis=1, join=joinIn, join_axes=useIndex, ignore_index=overwrite,\n",
    "     \n",
    "    ##########################################################################\n",
    "    ############################ TODO ########################################\n",
    "    \n",
    "    ### MISSINGNESS TOOLS - BEAUTIFUL FIGURES \n",
    "    #import missingno as msno\n",
    "    #msno.matrix(collisions.sample(250))\n",
    "        \n",
    "### EXAMPLE USAGE ###\n",
    "#X = DF(\"test_data/ag_analysis/1_1_qc_1000_map.txt\")    \n",
    "#X = DF(None, columnsIn=[\"Ace\",\"Duce\",\"Cinco\"], indexIn=np.arange(6))\n",
    "### PERFORM MANN-WHITNEY-U ON PAIRSWISE COMBINATIONS OF AGE BY RACE ###\n",
    "# X.stats_mwu_pairwise('age_years','race')\n",
    "#X.df\n",
    "### PLOT A LINEAR MODEL ###\n",
    "#f = X.plot_violin(categoricalColX='race',continuousColY='age_years',categoricalColor='economic_region')\n",
    "#g = X.plot_lm(continuousColX='age_years',continuousColY='bmi',categoricalColor='economic_region')#,categoricalColumn='race',categoricalRow='sex')\n",
    "#X.stats_regression(\"bmi ~ age_years + race + sex\")[0]\n",
    "#X.set_c(colName='winning',newCol=np.arange(len(X.i))).d()\n",
    "#X.set_r(index='winning',newCol=).d()\n",
    "#X.set_r(index=6, newRow = ['Nan','Nan','Nan'])\n",
    "#X.update()\n",
    "\n",
    "### LOAD MAPPING FILE & SET SAMPLID TO INDEX ###\n",
    "#mapX = DF(mapPath,sepIn='\\t',verboseIn=True)\n",
    "#mapX.set_index(\"#SampleID\")\n",
    "\n",
    "### EXAMPLE USAGE ###\n",
    "# SET DISPLAY TO FLOAT #\n",
    "#mapX.display_float()\n",
    "\n",
    "# PERFORM PAIRWISE MANN-WHITNEY-U #\n",
    "#mwuRes = mapX.stats_mwu_pairwise(categoricalColumn='race',continuousColumn='bmi')\n",
    "#mwuRes.out_tsv('t98.txt'); mwuRes.display()\n",
    "\n",
    "# PERFORM REGRESSION #\n",
    "#mapX.stats_regression(\"age_years ~ race\")\n",
    "\n",
    "# PLOT VIOLIN #\n",
    "#mapX.plot_violin(categoricalColX='sex',continuousColY='bmi',categoricalColor='race')\n",
    "\n",
    "# PLOT LINEAR MODEL #\n",
    "#xLM = mapX.plot_lm(continuousColX='age_years',continuousColY='bmi',categoricalColor='economic_region')\n",
    "#xLM.savefig('t99.pdf')\n",
    "\n",
    "# x.s(0, index=[2,3,6],column=['Duce','Ave']).df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align:center; color:blue;\"> - Load Tree Tools - </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### TREE TOOLS (TreeNode - skbio) #####################################################\n",
    "\n",
    "### LOAD NEWICK TREE (i.e. BACTERIAL PHYLOGENY) INTO TREENODE OBJECT ###\n",
    "def tree_load(trePathIn): \n",
    "    print(\" - Loading Bacterial Phylogeny - \" + trePathIn)\n",
    "    ### LOAD TREE USING SKBIO TREENODE OBJECT ###\n",
    "    treeInIn = TreeNode.read(trePathIn)\n",
    "    ### TRAVERSE TREE AND SET NONE TO 0.0 (AVOID ERRORS DOWNSTREAM) ###\n",
    "    for idx, e in enumerate(treeInIn.traverse()): \n",
    "        if e.length == None: e.length = 0.0\n",
    "    return treeInIn\n",
    "\n",
    "### TRIM TREE TO OTUS IN BIOM TABLE ###\n",
    "def tree_filter_biom(btIn, treeFIn):\n",
    "    \"\"\"\n",
    "    tree_filter_biom is adapted from the GNEISS PACKAGE\n",
    "    Morton JT... Knight R. 2017. Balance trees reveal microbial niche differentiation. \n",
    "    mSystems 2:e00162-16. https://doi.org/10.1128/mSystems.00162-16.\n",
    "    DOWNLOADED FROM: https://github.com/biocore/gneiss/\n",
    "    \"\"\"\n",
    "    print(\" - Filtering Bacterial Phylogeny to OTUs in BIOM Table - \")\n",
    "    treeFIn2 = treeFIn.shear(names=btIn.ids(axis='observation'))\n",
    "    treeFIn2.bifurcate()\n",
    "    treeFIn2.prune()\n",
    "    return treeFIn2\n",
    "\n",
    "### WRITE TREE TO NEWICK FILE ###\n",
    "def tree_write(treeInOut, treePathOut):\n",
    "    print(\" - Writing Tree to Newick File \" + treePathOut + \" - \")\n",
    "    treeInOut.write(treePathOut, format='newick')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align:center; color:blue;\"> - Load Distance Matrix Tools - </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### DISTANCE MATRIX (DistanceMatrix - skbio) ##########################################\n",
    "\n",
    "### WRITE DISTANCE MATRIX ###\n",
    "def dm_write(dmIn, savePath): dmIn.write(savePath, format='lsmat')\n",
    "\n",
    "##### SPLIT DISTANCE MATRIX TO ALL COMBINATIONS OF GROUPS IN A METADATA CATEGORY #####\n",
    "# pairwise, triwise...\n",
    "# i.e. table with groups: g1, g2, g3\n",
    "# [g1&g2, g2&g3, g1&g3]\n",
    "# TO USE:\n",
    "# dmS = dm_metadata_combinations(consensusDM, mapDf, 'race')\n",
    "# for i in dmS:\n",
    "#    print(i[0]) # NAME OF GROUPS\n",
    "#    i[1]  # DISTANCE MATRIX\n",
    "def dm_metadata_combinations(dmIn, mapDfIn, mapCatIn):\n",
    "    ### FOR EACH LEVEL OF COMBINATIONS ###\n",
    "    for combNumber in np.arange(2,len(mapDfIn[mapCatIn].unique())+1):\n",
    "        print(\" - All Table Combinations of \"+str(combNumber)+\" groups - \")\n",
    "        \n",
    "        ### FOR EACH SET OF COMBINATIONS AT THAT LEVEL ###\n",
    "        for combPairs in itertools.combinations(mapDfIn[mapCatIn].unique(),combNumber):\n",
    "            print(\"   - Group: \"+str(combPairs))\n",
    "            \n",
    "            ### GET SAMPLES IN GROUPS ###\n",
    "            samplesInGroups = []\n",
    "            for combGroup in combPairs:samplesInGroups.extend(mapDfIn[mapDfIn[mapCatIn]==combGroup].index)\n",
    "            \n",
    "            ### FILTER DISTANCE MATRIX ###\n",
    "            yield combPairs, dmIn.filter(samplesInGroups, strict=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align:center; color:blue;\"> - Load Plotting Tools - </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####### PLOTTING (Various) ################################################################\n",
    "\n",
    "#########################################################################################\n",
    "##### ADDITIONAL FUNCTIONS #####\n",
    "\n",
    "### OPEN INTERFACE TO SELECT CONTINOUS SEQUENTIAL COLORMAP ###\n",
    "#continuousColorMap = sns.choose_colorbrewer_palette('sequential', as_cmap=True)\n",
    "\n",
    "### OPEN INTERFACE TO SELECT CONTINOUS DIVERGING COLORMAP ###\n",
    "#continuousColorMap = sns.choose_colorbrewer_palette('diverging', as_cmap=True)\n",
    "\n",
    "### OPEN INTERFACE TO SELECT CATEGORICAL COLOR PALETTE (NOT COLORMAP) ###\n",
    "#categoricalColorPalette = sns.choose_colorbrewer_palette('qualitative', as_cmap=False)\n",
    "\n",
    "# fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex=True, sharey=True)\n",
    "# ax1.plot(x)\n",
    "\n",
    "PAP = \"\"\"\n",
    "def plot_area_pie(dfIn, radiiContinuousColumn=None, widthContinuousColumn=None,):\n",
    "    \n",
    "    ### GET EVENLY DISTRIBUTED LINESPACE IN DIMENSION AND RANGE OF RADII LINESPACE ###\n",
    "    thetaX = np.linspace(np.min(dfIn[radiiContinuousColumn]),np.max(dfIn[radiiContinuousColumn]), len(dfIn[radiiContinuousColumn]), endpoint=False)\n",
    "    print(thetaX)\n",
    "    ### INITIALIZE PLOTSPACE ###\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    \n",
    "    ### GENERATE PLOT ###\n",
    "    bars = ax.bar(thetaX, dfIn[radiiContinuousColumn], width=(dfIn[widthContinuousColumn]/100), bottom=0.0)\n",
    "\n",
    "    ### SET COLOR AND ALPHA ###\n",
    "    for r, bar in zip(dfIn[radiiContinuousColumn], bars):\n",
    "        bar.set_facecolor(plt.cm.viridis(8 / 10))\n",
    "        bar.set_alpha(0.5)\n",
    "        \n",
    "    ### SAVE AND SHOW ###\n",
    "    plt.show()\n",
    "    return\n",
    "plot_area_pie(mapDf[mapDf['race']=='Hispanic'], radiiContinuousColumn='age_years',widthContinuousColumn='bmi')\n",
    "\"\"\"\n",
    "# HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center; color:blue;\"> - Load BIOM Table Tools - </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### BIOM TABLE (Table - Biom) - I/O ###################################################\n",
    "\n",
    "### READ BIOM TABLE FROM JSON OR TSV OBJECT ###\n",
    "def table_in(filePath): return load_table(filePath)\n",
    "\n",
    "### WRITE OTU TABLE TO TSV ###\n",
    "def table_tsv(bt, fileName, printOut=False):\n",
    "    if printOut == True: print(\" - Writing BIOM Table TSV - \" + fileName)\n",
    "    f = open(fileName,'w')\n",
    "    f.write(bt.to_tsv())\n",
    "    f.close()\n",
    "\n",
    "### WRITE OTU TABLE AS JSON ###\n",
    "def table_json(bt, fileName, printOut=False):\n",
    "    if printOut == True: print(\" - Writing BIOM Table JSON - \" + fileName)\n",
    "    f = open(fileName,'w')\n",
    "    f.write(bt.to_json(\"biom\", direct_io=None))\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### BIOM TABLE (Table - Biom) - INFO ##################################################\n",
    "\n",
    "### RETURN LIST OF OBSERVATIONS ###\n",
    "def table_observations(bt, writePath=None): \n",
    "    otusInF = bt.ids(axis='observation')\n",
    "    if writePath != None: \n",
    "        outListFile = open(writePath, 'w') \n",
    "        for itemCycle in otusInF: outListFile.write(\"%s\\n\" % itemCycle)\n",
    "        outListFile.close() \n",
    "    return otusInF\n",
    "\n",
    "### Return List of Observation Total Counts ###\n",
    "def table_observations_counts(bt, writePath=None): \n",
    "    otusCountsInF = bt.sum('observation')\n",
    "    if writePath != None: \n",
    "        outListFile = open(writePath, 'w')\n",
    "        for itemCycle in otusCountsInF: outListFile.write(\"%s\\n\" % itemCycle)\n",
    "        outListFile.close() \n",
    "    return otusCountsInF\n",
    "\n",
    "### Return List of Samples ###\n",
    "def table_samples(bt, writePath=None): \n",
    "    samplesInF = bt.ids(axis='sample')\n",
    "    if writePath != None: \n",
    "        outListFile = open(writePath, 'w')\n",
    "        for itemCycle in samplesInF: outListFile.write(\"%s\\n\" % itemCycle)\n",
    "        outListFile.close() \n",
    "    return samplesInF\n",
    "\n",
    "### Return List of Counts for Each Sample ###\n",
    "def table_samples_counts(bt, writePath=None): \n",
    "    samplesCountsInF = bt.sum('sample')\n",
    "    if writePath != None: \n",
    "        outListFile = open(writePath, 'w')\n",
    "        for itemCycle in samplesCountsInF: outListFile.write(\"%s\\n\" % itemCycle)\n",
    "        outListFile.close() \n",
    "    return samplesCountsInF\n",
    "\n",
    "### GET GENERAL INFO ABOUT TABLE ###\n",
    "def table_info(bt, writePath=None, printOut = True):\n",
    "    otusIn = table_observations(bt, writePath=None)\n",
    "    otusCountsIn = table_observations_counts(bt, writePath=None)\n",
    "    samplesIn = table_samples(bt, writePath=None)\n",
    "    samplesCountsIn = table_samples_counts(bt, writePath=None)\n",
    "    if printOut == True:\n",
    "        print(\" - Table Info - \")\n",
    "        print('   Total Observations: '+str(len(otusIn)))\n",
    "        print('   Total Samples: '+str(len(samplesIn)))\n",
    "        print('   Total Counts: '+str(bt.sum()))\n",
    "        print('   Non-Zero Entries: '+str(bt.nnz))\n",
    "        print('   Table Density: '+str(bt.get_table_density()))\n",
    "    if writePath != None: \n",
    "        fOut = open(writePath,'w')\n",
    "        fOut.write(\" - Table Info - \\n\")\n",
    "        fOut.write(\"   Total Observations: \"+str(len(otusIn))+\"\\n\")\n",
    "        fOut.write(\"   Total Samples: \"+str(len(samplesIn))+\"\\n\")\n",
    "        fOut.write(\"   Total Counts: \"+str(bt.sum())+\"\\n\")\n",
    "        fOut.write(\"   Non-Zero Entries: \"+str(bt.nnz)+\"\\n\")\n",
    "        fOut.write(\"   Table Density: \"+str(bt.get_table_density()))\n",
    "        fOut.close()\n",
    "    return otusIn,otusCountsIn,samplesIn,samplesCountsIn\n",
    "\n",
    "### GET METADATA CATEGORIES ###\n",
    "def table_metadata(bt, mappingFileDataframe):\n",
    "    print(\" - Getting Metadata Categories - \")\n",
    "    # GET METADATA CATEGORIES FROM MAPPING FILE #\n",
    "    metaC = []\n",
    "    for i in mappingFileDataframe.columns: metaC.append(i)\n",
    "    # CREATE METADATA DICTIONARY FORMAT ALLOWING TO ADD TO BIOM TABLE STRUCTURE #\n",
    "    print(\" - Constructing Dictionary of Metadata by Samples - \")\n",
    "    iterSamples = bt.iter(axis='sample'); metaD={}\n",
    "    # Loop through samples #\n",
    "    \n",
    "    for values, idCur, metadata in iterSamples: \n",
    "        metaD[idCur] = {}\n",
    "        for idx, i in enumerate(mappingFileDataframe.loc[idCur]): \n",
    "            metaD[idCur][metaC[idx]] = i            \n",
    "    return metaC, metaD\n",
    "\n",
    "### PIPELINE TO WRITE TABLE TO TSV AND OUTPUT INFO & LISTS ###\n",
    "# EACH FILE WILL BE SAVED WITH tableID_table_(file description).txt IN dirPath FOLDER\n",
    "def table_write(bt, outDirPath, tableID, toTSV=True, toJSON=False):\n",
    "    ##### WRITE INPUT BIOM TABLE TO TSV #####\n",
    "    if toTSV == True: table_tsv(bt, outDirPath+tableID+\"_table.txt\", printOut = True)\n",
    "    ##### WRITE INPUT BIOM TABLE TO JSON ##### (Sometimes errors with metadata)\n",
    "    if toJSON == True: table_json(bt, outDirPath+tableID+\"_table.biom\", printOut=True)\n",
    "    ##### PRINT AND WRITE TABLE INFO TO FILE #####\n",
    "    table_info(bt, writePath=outDirPath+tableID+\"_table_summary.txt\", printOut = False)\n",
    "    ##### WRITE SAMPLE LIST #####\n",
    "    table_observations(bt, writePath=outDirPath+tableID+\"_table_otus.txt\")\n",
    "    ##### WRITE SAMPLE COUNTS #####\n",
    "    table_observations_counts(bt, writePath=outDirPath+tableID+\"_table_otus_counts.txt\")\n",
    "    ##### WRITE OTUS LIST #####\n",
    "    table_samples(bt, writePath=outDirPath+tableID+\"_table_samples.txt\")\n",
    "    ##### WRITE SAMPLES COUNTS #####\n",
    "    table_samples_counts(bt, writePath=outDirPath+tableID+\"_table_samples_counts.txt\")\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### BIOM TABLE (Table - Biom) - FILTERING #############################################\n",
    "\n",
    "##### PIPELINE TO PERFORM BASIC QUALITY CONTROL - WILL RETURN FILTERED BIOM TABLE & MAP #####\n",
    "### DEFAULT REMOVES EMPTY OTUS AND SAMPLES ###\n",
    "def table_qc(bt, minimumOTUCountIn=1, minimumSampleCountIn=1, minimumOTUUbiquityIn=0.0):\n",
    "    ##### FILTER OTUS BY MINIMUM COUNT #####\n",
    "    bt = table_filter_otu_mincount(bt, minimumOTUCountIn)\n",
    "    ##### FILTER OTUS BY MINIMUM SAMPLES #####\n",
    "    if minimumOTUUbiquityIn > 0.0: bt = filter_otu_minubiquity(bt, minimumOTUUbiquityIn)\n",
    "    ##### FILTER SAMPLES BY MINIMUM COUNT #####\n",
    "    bt = table_filter_sample_mincount(bt, minimumSampleCountIn)\n",
    "    ##### FILTER OTUS BY MINIMUM COUNT (AGAIN TO GUARENTEE ALL HAVE 10+ COUNTS AFTER SAMPLE FILTER) #####\n",
    "    bt = table_filter_otu_mincount(bt, minimumOTUCountIn)\n",
    "    ##### FILTER SAMPLES BY MINIMUM COUNT (AGAIN TO GUARENTEE SAMPLES IN TABLE ARE SAME AS RAREFIED) #####\n",
    "    bt = table_filter_sample_mincount(bt, minimumSampleCountIn)\n",
    "    return bt\n",
    "\n",
    "### FILTER OTU MINCOUNT ### - remove otus with < mincount\n",
    "def table_filter_otu_mincount(bt, mincount):\n",
    "    filter_func = lambda values, id, md: sum(values) >= mincount\n",
    "    return bt.filter(filter_func, axis='observation', inplace=False)\n",
    "\n",
    "### FILTER OTU MAXCOUNT ### - remove otus with > maxcount\n",
    "def table_filter_otu_maxcount(bt, maxcount):\n",
    "    filter_func = lambda values, id, md: sum(values) <= maxcount\n",
    "    return bt.filter(filter_func, axis='observation', inplace=False)\n",
    "\n",
    "### FILTER SAMPLE MINCOUNT ### - remove otus with < mincount\n",
    "def table_filter_sample_mincount(bt, mincount):\n",
    "    filter_func = lambda values, id, md: sum(values) >= mincount\n",
    "    return bt.filter(filter_func, axis='sample', inplace=False)\n",
    "\n",
    "### FILTER SAMPLE MAXCOUNT ### - remove otus with > maxcount\n",
    "def table_filter_sample_maxcount(bt, maxcount):\n",
    "    filter_func = lambda values, id, md: sum(values) <= maxcount\n",
    "    return bt.filter(filter_func, axis='sample', inplace=False)\n",
    "\n",
    "### FILTER OTU MINIMUM UBIQUITY BY PERCENT SAMPLES ### - remove otus in < minubiq fraction of samples\n",
    "def table_filter_otu_minubiquity(bt, minubiq):\n",
    "    filter_func = lambda val, id_, md: sum(val>0)/len(val) >= minubiq\n",
    "    return bt.filter(filter_func, axis='observation', inplace=False)\n",
    "\n",
    "### FILTER OTU MAXIMUM UBIQUITY BY PERCENT SAMPLES ### - remove otus in > maxubiq fraction of samples\n",
    "def table_filter_otu_maxubiquity(bt, maxubiq):\n",
    "    filter_func = lambda val, id_, md: sum(val>0)/len(val) <= maxubiq\n",
    "    return bt.filter(filter_func, axis='observation', inplace=False)\n",
    "    \n",
    "### FILTER OTU LISTKEEP ### - remove all otus from table not im listkeep\n",
    "def table_filter_otu_listkeep(bt, list_to_keep):\n",
    "    filter_func = lambda values, id, md: id in list_to_keep\n",
    "    return bt.filter(filter_func, axis='observation', inplace=False)\n",
    "\n",
    "### FILTER OTU LISTREMOVE ### - give a list of otu's to remove from table\n",
    "def table_filter_otu_listremove(bt, list_to_remove):\n",
    "    filter_func = lambda values, id, md: id not in list_to_remove\n",
    "    return bt.filter(filter_func, axis='observation', inplace=False)\n",
    "\n",
    "### FILTER SAMPLE LISTKEEP ### - remove all otus from table not in listkeep\n",
    "def table_filter_sample_listkeep(bt, list_to_keep): \n",
    "    filter_func = lambda values, id, md: id in list_to_keep \n",
    "    return bt.filter(filter_func, axis='sample', inplace=False)\n",
    "\n",
    "### FILTER SAMPLE LISTREMOVE ### - give a list of otu's to remove from table\n",
    "def table_filter_sample_listremove(bt, list_to_remove):\n",
    "    filter_func = lambda values, id, md: id not in list_to_remove\n",
    "    return bt.filter(filter_func, axis='sample', inplace=False)\n",
    "\n",
    "### FILTER TO SAMPLES IN METADATA CATEGORY:GROUP ### - keep only samples in the specified group\n",
    "def table_filter_metadata_contain(bt, metadata_category, metadata_group):\n",
    "    filter_f = lambda values, id_, md: md[metadata_category] == metadata_group\n",
    "    return bt.filter(filter_f, axis='sample', inplace=False)\n",
    "\n",
    "### FILTER TO SAMPLES NOT IN METADATA CATEGORY:GROUP ### - keep only samples not in the specified group \n",
    "def table_filter_metadata_exclude(bt, metadata_category, metadata_group):\n",
    "    filter_f = lambda values, id_, md: md[metadata_category] != metadata_group\n",
    "    return bt.filter(filter_f, axis='sample', inplace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### BIOM TABLE (Table - Biom) - RAREFACTION ###########################################\n",
    "\n",
    "# Take in BIOM Table and Subsample each Sample to samplingDepth\n",
    "# Returns Rarefied BIOM Table Object\n",
    "def table_rarefaction(bt, samplingDepth = 1000, warnSkippedSamples=False):\n",
    "    dataCounts = []; samplesDataCounts = []\n",
    "    # Iterate over All of the Samples #\n",
    "    iterSamples = bt.iter(axis='sample')\n",
    "    for values, id, metadata in iterSamples:\n",
    "        # Subsample to the Specified Depth #\n",
    "        if sum(values) >= samplingDepth:\n",
    "            # Store Subsampled Counts for Sample #\n",
    "            dataCounts.append(sk.stats.subsample_counts(values.astype(int), samplingDepth, replace=False))\n",
    "            # Store Sample ID #\n",
    "            samplesDataCounts.append(id)\n",
    "        # If Sample has < Counts than samplingDepth: Print Warning #\n",
    "        elif warnSkippedSamples==True: print(\"   Warning Skipped Sample : \", id)\n",
    "    # Return BIOM Table Object #\n",
    "    return Table(np.matrix(dataCounts).T, bt.ids(axis='observation'), samplesDataCounts)\n",
    "    \n",
    "\n",
    "### FUNCTION TO PERFORM MULTIPLE RAREFACTIONS ON AN OTU TABLE AT AN EVEN DEPTH ###\n",
    "def table_rarefactions_even_depth(bt, depthRare, numRare):\n",
    "    ### ARRAY TO STORE RAREFIED TABLES ###\n",
    "    rareArray = []\n",
    "    ### RAREFY TABLES ###\n",
    "    for numRareIncrement in np.arange(numRare): rareArray.append(rarefaction(bt, samplingDepth = depthRare, warnSkippedSamples=False))\n",
    "    return rareArray\n",
    "\n",
    "### FUNCTION WRAPPER TO PERFORM MULTIPLE RAREFACTIONS ON AN OTU TABLE AT MULTIPLE DEPTHS ###\n",
    "def table_rarefactions_multiple_depths(bt, minDepthRare, maxDepthRare, rareStep, numRare):\n",
    "    ### DICTIONARY TO HOLD ARRAYS OF RAREFIED TABLES AT EACH DEPTH ###\n",
    "    rareDictIn = {}\n",
    "    ### LOOP THROUGH RAREFACTIONS AT EACH DEPTH...\n",
    "    for rareCurDepth in np.arange(minDepthRare, maxDepthRare, rareStep):\n",
    "        ### GET LIST OF RAREFIED TABLES AT THAT DEPTH ###\n",
    "        rareDictIn[rareCurDepth] = rarefactions_even_depth(bt, rareCurDepth, numRare)\n",
    "    return rareDictIn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### BIOM TABLE (Table - Biom) - TRANSFORMATION ########################################\n",
    "\n",
    "### RETURN RELATIVE ABUNDANCE TABLE ###\n",
    "def table_relative(bt):\n",
    "    tablerel = bt.norm(axis='sample', inplace=False)\n",
    "    return tablerel\n",
    "\n",
    "### GET BIOMTABLE COUNTS AS MATRIX WITH SAMPLES AS COLUMNS (TRANSFORM ROW<->COLS AND CONVERT TO INT) ###\n",
    "def table_array(bt): return bt.matrix_data.todense().T\n",
    "\n",
    "### BIOM COUNTS AS PANDAS DATAFRAME ###\n",
    "def table_dataframe(bt): return pd.DataFrame(bt.matrix_data.todense().T.astype('float'), index=bt.ids(axis='sample'), columns=bt.ids(axis='observation'))\n",
    "\n",
    "### GET OTU TAXONOMY AS PANDAS DATAFRAME ###\n",
    "def table_dataframe_taxa(bt):\n",
    "    metaDictTaxonomy = {}; iterOTUs = bt.iter(axis='observation')\n",
    "    ### ITERATE OVER OTUS AND CREATE DICTIONARY OF TAXONOMY ###\n",
    "    for idx, (values, id, metadata) in enumerate(iterOTUs): metaDictTaxonomy[id] = metadata['taxonomy']\n",
    "    ### CREATE AND RETURN AS PANDAS DATAFRAME ###\n",
    "    dfTaxPD = pd.DataFrame.from_dict(metaDictTaxonomy, orient='index')\n",
    "    dfTaxPD.columns = ['kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species']\n",
    "    return dfTaxPD\n",
    "\n",
    "### PARTITION SAMPLES BY METADATA CATEGORY ### -> Dictionary of Tables\n",
    "def table_partition_metadata(bt, metadata_category, print_summary=False):\n",
    "    print(\" - Partitioning Table by \" + metadata_category + \" - \")\n",
    "    part_f = lambda id_, md: md[metadata_category]\n",
    "    partTabs = bt.partition(part_f, axis='sample')\n",
    "    partTables = {}\n",
    "    for partition, partTab in partTabs: \n",
    "        partTables[partition] = partTab\n",
    "        if print_summary == True: print(partition)\n",
    "        if print_summary == True: table_info(partTab)\n",
    "        if print_summary == True: print(str(partTab.sum('sample')))\n",
    "    return partTables\n",
    "\n",
    "### COLLAPSE SAMPLES BY METADATA CATEGORY ###\n",
    "def table_collapse_metadata(bt, metaDataCatIn):\n",
    "    ### DICTIONARY OF COUNTS FOR EACH METADATA GROUPING ###\n",
    "    metaCountsCollapsed = {}; iterSamples = bt.iter(axis='sample')\n",
    "    ### ITERATE OVER SAMPLES ###\n",
    "    for idx, (values, id, metadata) in enumerate(iterSamples):\n",
    "        ### CHECK IF METADATA GROUP IN DICTIONARY AND INITIALIZE COUNTS IF NOT ###\n",
    "        if metadata[metaDataCatIn] not in metaCountsCollapsed.keys(): metaCountsCollapsed[metadata[metaDataCatIn]] = values\n",
    "        ### ELSE ADD THE COUNTS TO DICTIONARY ###\n",
    "        else: metaCountsCollapsed[metadata[metaDataCatIn]] += values\n",
    "    ### COUNTS AS ARRAY OF ARRAYS AND METADATA IDS AS LIST AND RETURN TO BIOM TABLE FORMAT ###\n",
    "    metaCountIds = []; metaCountMatrix = []\n",
    "    for metaLoopId in metaCountsCollapsed.keys(): metaCountIds.append(metaLoopId); metaCountMatrix.append(metaCountsCollapsed[metaLoopId])\n",
    "    return Table(np.array(metaCountMatrix).T, bt.ids(axis='observation'), metaCountIds, observation_metadata=None, sample_metadata=None, table_id=None, type=None, create_date=None, generated_by=None, observation_group_metadata=None, sample_group_metadata=None) \n",
    "\n",
    "### COLLAPSE OTUs AT TAXONOMIC LEVEL ###\n",
    "# tax_level: 0 = Kingdom | 1 = Phylum | 2 = Class | 3 = Order | 4 = Family | 5 = Genus | 6 = Species\n",
    "def table_collapse_taxonomy(bt, tax_level):\n",
    "    collapse_f = lambda id_, md: '; '.join(md['taxonomy'][:tax_level + 1])\n",
    "    tabletaxacollapse = bt.collapse(collapse_f, axis='observation',norm=False)\n",
    "    return tabletaxacollapse\n",
    "\n",
    "### CONVERT TO PRESENCE / ABSENCE TABLE ###\n",
    "# Table contains 1 if count >0 or 0\n",
    "def table_presence_absence(bt): return bt.pa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### BIOM TABLE (Table - Biom) - ALPHA DIVERSITY #######################################\n",
    "##### COMPUTES ALPHA DIVERSITY METRICS FOR BIOM TABLE #####\n",
    "# !!! DO NOT RUN ON RELATIVE ABUNDANCE - DEPENDENT ON INT CONVERSION !!! #\n",
    "def alpha_diversity(bt, alphaMetricsIn=['chao1','observed_otus','shannon','simpson'], vocalAlphaIn=True):\n",
    "    alphaResultsDict = {}\n",
    "    ### FOR EACH ALPHA METRIC...\n",
    "    for alphaMetricCycle in alphaMetricsIn:\n",
    "        if vocalAlphaIn == True: print(\" - Calculating Alpha Diversity Metric \"+alphaMetricCycle+\" - \")\n",
    "        ### CALCULATE ALPHA DIVERSITY AND STORE IN DICT ###\n",
    "        alphaResultsDict[alphaMetricCycle] = sk.diversity.alpha_diversity(alphaMetricCycle, table_array(bt).astype(int), ids=bt.ids(axis='sample'), validate=True)\n",
    "    ### AND RETURN AS PANDAS DICTIONARY ###\n",
    "    return pd.DataFrame.from_dict(alphaResultsDict)\n",
    "\n",
    "### CALCULATES ALPHA DIVERSITY FOR RAREFIED TABLES ###\n",
    "def alpha_pipeline(rareTablesIn):\n",
    "    alphaDictSum = None\n",
    "    ### FOR EACH RAREFIED TABLE...\n",
    "    for idx, rareTableCycle in enumerate(rareTablesIn):\n",
    "        ### CALCULATE ALPHA DIVERSITY AND SUM TOGETHER ACROSS TABLES ###\n",
    "        if idx == 0: alphaDictSum = alpha_diversity(rareTableCycle, vocalAlphaIn=True); print(\" - Alpha Pipeling Continuing Silently for All Tables - \")\n",
    "        else: alphaDictSum += alpha_diversity(rareTableCycle, vocalAlphaIn=False)\n",
    "    ### DIVIDE BY NUMBER OF TABLES AND RETURN ###\n",
    "    return alphaDictSum / (idx+1) \n",
    "\n",
    "##### ALPHA DIVERSITY ANALYSES #####\n",
    "# GENERATES PLOTS OF ALPHA DIVERSITY RESULTS FOR SAMPLES #\n",
    "# Requires a dataframe of alpha diversity for each sample and mapping file\n",
    "# Provide: mapCat1 as a categorical column in mapDfIn\n",
    "# Provide: mapCat2 to examine how alpha diversity structures by both factors\n",
    "# Provide: savePath where alpha diversity results will be saved\n",
    "def alpha_pipeline_analysis(alphaDfIn, mapDfIn, mapCat1, mapCat2=None, metricsIn = ['shannon','observed_otus','chao1','simpson'],\n",
    "                         savePath=None, plotDisplay = True, plotIndividualMetrics = True, plotPairplotAllMetrics = False):\n",
    "    \n",
    "    ### SAVE TO PDF IF PATH IS PASSED ###\n",
    "    if (savePath != None):\n",
    "        # SAVE CORRELATION MATRIX OF ALPHA METRICS #\n",
    "        alphaDfIn.corr(method=\"spearman\").to_csv(savePath+'_alpha_correlation.txt',sep='\\t')\n",
    "        # IF SECOND METRIC PASSED ADD TO NAME WITH FIRST METRIC #\n",
    "        if mapCat2 != None: pdf = PdfPages(savePath+\"_alpha_\"+mapCat1+\"_\"+mapCat2+\".pdf\")\n",
    "        # ELSE SAVE WITH ONLY FIRST METRIC #\n",
    "        else: pdf = PdfPages(savePath+\"_alpha_\"+mapCat1+\".pdf\")\n",
    "        \n",
    "    ### TEMPORARLY DISABLE WARNINGS ###\n",
    "    import warnings; warnings.filterwarnings('ignore')\n",
    "    #pd.set_option('display.width', 1000)\n",
    "    \n",
    "    ### ADD FIRST MAP CATEGORY TO ALPHA DF ###\n",
    "    alphaDfIn[mapCat1] = mapDfIn[mapCat1]\n",
    "    ### ADD SECOND MAP CATEGORY TO ALPHA DF ###\n",
    "    if mapCat2 != None: alphaDfIn[mapCat2] = mapDfIn[mapCat2]\n",
    "        \n",
    "    ### GET MEAN DIVERSITIES FOR CATEGORY ONE ###\n",
    "    if (savePath != None) or (plotDisplay == True): meanCat1DF = alphaDfIn.groupby(mapCat1).mean()\n",
    "    if savePath != None: meanCat1DF.to_csv(path_or_buf=savePath+\"_alpha_mean_\"+mapCat1+\".txt\", sep='\\t')\n",
    "    if plotDisplay == True: print(\" - Mean Alpha Diversity by \"+mapCat1+\" - \"); display(meanCat1DF)\n",
    "    ### GET MEAN DIVERSITIES FOR CATEGORY TWO ###\n",
    "    if mapCat2 != None:\n",
    "        if (savePath != None) or (plotDisplay == True): meanCat2DF = alphaDfIn.groupby(mapCat2).mean()\n",
    "        if savePath != None: meanCat2DF.to_csv(path_or_buf=savePath+\"_alpha_mean_\"+mapCat2+\".txt\", sep='\\t')\n",
    "        if plotDisplay == True: print(\" - Mean Alpha Diversity by \"+mapCat2+\" - \"); display(meanCat2DF)\n",
    "        ### AND BOTH CATEGORIES TOGETHER ###\n",
    "        if (savePath != None) or (plotDisplay == True): meanCatBothDF = alphaDfIn.groupby([mapCat1,mapCat2]).mean()\n",
    "        if savePath != None: meanCatBothDF.to_csv(path_or_buf=savePath+\"_alpha_mean_\"+mapCat1+\"_\"+mapCat2+\".txt\", sep='\\t')\n",
    "        if plotDisplay == True: print(\" - Mean Alpha Diversity by \"+mapCat1+\" and \"+mapCat2+\" - \"); display(meanCatBothDF)\n",
    "\n",
    "    ##### INDIVIDUAL PLOTS #####\n",
    "    # DISTRIBUTION PLOTS FOR EACH INDIVIDUAL METRIC #\n",
    "    if plotIndividualMetrics == True:\n",
    "        fig = plt.subplots(4,1,figsize=[10,8])\n",
    "        plt.title(\"Metric Distributions\")\n",
    "        ### FOR EACH METRIC GENERATE PLOT ###\n",
    "        for idx, i in enumerate(metricsIn):\n",
    "            plt.subplot(4,1,idx+1)\n",
    "            sns.distplot(alphaDfIn[i], rug=True)\n",
    "            ymin, ymax = plt.gca().get_ylim(); plt.vlines(np.mean(alphaDfIn[i]),ymin,ymax,color='r')\n",
    "            curAx = plt.gca(); curAx.axes.get_yaxis().set_visible(False)\n",
    "            plt.xlabel(i)\n",
    "        plt.tight_layout(); \n",
    "        if savePath != None: pdf.savefig()\n",
    "        if plotDisplay == True: plt.show(); \n",
    "        plt.close()\n",
    "    \n",
    "    ##### PAIRPLOT ALL METRIC COLOR BY CATEGORIES #####\n",
    "    if plotPairplotAllMetrics == True:\n",
    "        gbX = sns.pairplot(data=alphaDfIn, hue=mapCat1, vars=metricsIn); \n",
    "        if savePath != None: pdf.savefig()\n",
    "        if plotDisplay == True: plt.show(gbX); \n",
    "        plt.close()\n",
    "        ### SECOND CATEGORY ###\n",
    "        if mapCat2 != None: \n",
    "            gbX = sns.pairplot(data=alphaDfIn, hue=mapCat2, vars=metricsIn); \n",
    "            if savePath != None: pdf.savefig()\n",
    "            if plotDisplay == True: plt.show(gbX); \n",
    "            plt.close()\n",
    "            \n",
    "    ##### BOXPLOTS #####\n",
    "    for boxplotMetric in metricsIn:\n",
    "        ### PLOT ALL SAMPLES TOGETHER ###\n",
    "        plt.figure(figsize=[10,5]); \n",
    "        sns.boxplot(y=boxplotMetric, data=alphaDfIn, saturation=0.5, meanline=True, showmeans=True);\n",
    "        plt.title(\"All Individuals\"); plt.tight_layout();\n",
    "        if savePath != None: pdf.savefig()\n",
    "        if plotDisplay == True: plt.show(); \n",
    "        plt.close()\n",
    "            \n",
    "        ### BOXPLOT PLOT SPLIT BY MAPCAT1 ###\n",
    "        if mapCat1 != None: \n",
    "            plt.figure(figsize=[10,5]); sns.boxplot(x=mapCat1, y=boxplotMetric, data=alphaDfIn, saturation=0.5, meanline=True, showmeans=True); \n",
    "            plt.title(\"Individuals by \"+mapCat1); plt.tight_layout();\n",
    "            # CALCULATE STATS #\n",
    "            statStringOut = (\"ALPHA STATISTIC COMPARISON FOR METRIC \"+boxplotMetric+\" by \"+mapCat1+\"\\r\")\n",
    "            for manuResult in pandas_stats_mannwhitneyu(alphaDfIn, boxplotMetric, mapCat1): statStringOut += (list_to_string(manuResult)+\"\\r\")\n",
    "            list_write([statStringOut], savePath+\"_alpha_statistics_\"+boxplotMetric+\"_\"+mapCat1+\".txt\")\n",
    "            # SAVE AND CLOSE #\n",
    "            if savePath != None: pdf.attach_note(statStringOut, positionRect=[-100, -100, 0, 0]); pdf.savefig()\n",
    "            if plotDisplay == True: print(statStringOut); plt.show(); \n",
    "            plt.close()\n",
    "    \n",
    "        if mapCat2 != None:\n",
    "            ### BOXPLOT SPLIT BY MAPCAT2 ###\n",
    "            plt.figure(figsize=[10,5]); sns.boxplot(x=mapCat2, y=boxplotMetric, data=alphaDfIn, saturation=0.5, meanline=True, showmeans=True);\n",
    "            plt.title(\"Individuals by \"+mapCat2); plt.tight_layout();\n",
    "            # CALCULATE STATS #\n",
    "            statStringOut = (\"ALPHA STATISTIC COMPARISON FOR METRIC \"+boxplotMetric+\" by \"+mapCat2+\"\\r\")\n",
    "            for manuResult in pandas_stats_mannwhitneyu(alphaDfIn, boxplotMetric, mapCat2): statStringOut += (list_to_string(manuResult)+\"\\r\")\n",
    "            list_write([statStringOut], savePath+\"_alpha_statistics_\"+boxplotMetric+\"_\"+mapCat2+\".txt\")\n",
    "            # SAVE AND CLOSE #\n",
    "            if savePath != None: pdf.attach_note(statStringOut, positionRect=[-100, -100, 0, 0]); pdf.savefig()\n",
    "            if plotDisplay == True: print(statStringOut); plt.show(); \n",
    "            plt.close()\n",
    "        \n",
    "            ####### BOXPLOT SPLIT BY MAPCAT1 AND MAPCAT2 #######\n",
    "            plt.figure(figsize=[10,5]); sns.boxplot(x=mapCat1, y=boxplotMetric, hue=mapCat2, data=alphaDfIn, saturation=0.5, meanline=True, showmeans=True);\n",
    "            plt.title(\"Individuals by \"+mapCat1+\" and \"+mapCat2);plt.tight_layout();\n",
    "            # CALCULATE STATS #\n",
    "            statStringOut = (\"ALPHA STATISTIC COMPARISON FOR METRIC \"+boxplotMetric+\" by \"+mapCat1+\" and \"+mapCat2+\"\\r\")\n",
    "            # FOR EACH GROUP IN FIRST CATEGORY... CALCULATE SECOND CATEGORY STATS ###\n",
    "            for cat1Cycling in alphaDfIn[mapCat1].unique():\n",
    "                statStringOut += (\"\\rWithin: \"+cat1Cycling+\"\\r\")\n",
    "                for manuResult in pandas_stats_mannwhitneyu(alphaDfIn[alphaDfIn[mapCat1]==cat1Cycling], boxplotMetric, mapCat2): statStringOut += (list_to_string(manuResult)+\"\\r\")\n",
    "            list_write([statStringOut], savePath+\"_alpha_statistics_\"+boxplotMetric+\"_\"+mapCat1+\"_\"+mapCat2+\".txt\")\n",
    "            # SAVE AND CLOSE #\n",
    "            if savePath != None: pdf.attach_note(statStringOut, positionRect=[-100, -100, 0, 0]); pdf.savefig()\n",
    "            if plotDisplay == True: print(statStringOut); plt.show(); \n",
    "            plt.close()\n",
    "        \n",
    "            ####### BOXPLOT SPLIT BY MAPCAT2 AND MAPCAT1 #######\n",
    "            plt.figure(figsize=[10,5]); sns.boxplot(x=mapCat2, y=boxplotMetric, hue=mapCat1, data=alphaDfIn, saturation=0.5, meanline=True, showmeans=True);\n",
    "            plt.title(\"Individuals by \"+mapCat2+\" and \"+mapCat1);plt.tight_layout();\n",
    "            # CALCULATE STATS #\n",
    "            statStringOut = (\"ALPHA STATISTIC COMPARISON FOR METRIC \"+boxplotMetric+\" by \"+mapCat2+\" and \"+mapCat1+\"\\r\")\n",
    "            # FOR EACH GROUP IN FIRST CATEGORY... CALCULATE SECOND CATEGORY STATS ###\n",
    "            for cat1Cycling in alphaDfIn[mapCat2].unique():\n",
    "                statStringOut += (\"\\rWithin: \"+cat1Cycling+\"\")\n",
    "                for manuResult in pandas_stats_mannwhitneyu(alphaDfIn[alphaDfIn[mapCat2]==cat1Cycling], boxplotMetric, mapCat1): statStringOut += (list_to_string(manuResult)+\"\\r\")\n",
    "            list_write([statStringOut], savePath+\"_alpha_statistics_\"+boxplotMetric+\"_\"+mapCat2+\"_\"+mapCat1+\".txt\")\n",
    "            # SAVE AND CLOSE #\n",
    "            if savePath != None: pdf.attach_note(statStringOut, positionRect=[-100, -100, 0, 0]); pdf.savefig()\n",
    "            if plotDisplay == True: print(statStringOut); plt.show(); \n",
    "            plt.close()\n",
    "            \n",
    "    if savePath != None: pdf.close()\n",
    "    \n",
    "    ### RESET WARNINGS ###\n",
    "    warnings.filterwarnings('default')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### BIOM TABLE (Table - Biom) - BETA DIVERSITY ########################################\n",
    "\n",
    "### CALCULATE BRAY CURTIS BETA DIVERSITY METRIC FOR TABLE ###\n",
    "def table_beta_bray_curtis(bt):\n",
    "    # GET BIOMTABLE COUNTS AS MATRIX WITH SAMPLES AS COLUMNS (TRANSFORM ROW<->COLS AND CONVERT TO INT)\n",
    "    matrixDataAsInt = bt.matrix_data.todense().T.astype(int)\n",
    "    # CALCULATE BETA DIVERSITY - BRAY CURTIS\n",
    "    return sk.diversity.beta_diversity('braycurtis', matrixDataAsInt, ids=bt.ids(axis='sample')) \n",
    "\n",
    "### CALCULATE BINARY JACCARD BETA DIVERSITY METRIC FOR TABLE ###\n",
    "def table_beta_binary_jaccard(bt):\n",
    "    # GET TABLE AS PRESENCE / ABSENCE #\n",
    "    bt = table_presence_absence(bt)\n",
    "    # GET BIOMTABLE COUNTS AS MATRIX WITH SAMPLES AS COLUMNS (TRANSFORM ROW<->COLS AND CONVERT TO INT)\n",
    "    matrixDataAsInt = bt.matrix_data.todense().T.astype(int)\n",
    "    # CALCULATE BETA DIVERSITY - BRAY CURTIS\n",
    "    return sk.diversity.beta_diversity('jaccard', matrixDataAsInt, ids=bt.ids(axis='sample')) \n",
    "\n",
    "### CALCULATE BINARY JACCARD BETA DIVERSITY METRIC FOR TABLE ###\n",
    "def table_beta_unweighted_unifrac(bt, treeBetaIn):\n",
    "    # GET BIOMTABLE COUNTS AS MATRIX WITH SAMPLES AS COLUMNS (TRANSFORM ROW<->COLS AND CONVERT TO INT)\n",
    "    matrixDataAsInt = bt.matrix_data.todense().T.astype(int)\n",
    "    # CALCULATE BETA DIVERSITY - BRAY CURTIS\n",
    "    return sk.diversity.beta_diversity('unweighted_unifrac', matrixDataAsInt, ids=bt.ids(axis='sample'), \n",
    "                                       otu_ids=bt.ids(axis='observation'), tree=treeBetaIn) \n",
    "\n",
    "### CALCULATE BINARY JACCARD BETA DIVERSITY METRIC FOR TABLE ###\n",
    "def table_beta_weighted_unifrac(bt, treeBetaIn):\n",
    "    # GET BIOMTABLE COUNTS AS MATRIX WITH SAMPLES AS COLUMNS (TRANSFORM ROW<->COLS AND CONVERT TO INT)\n",
    "    matrixDataAsInt = bt.matrix_data.todense().T.astype(int)\n",
    "    # CALCULATE BETA DIVERSITY - BRAY CURTIS\n",
    "    return sk.diversity.beta_diversity('weighted_unifrac', matrixDataAsInt, ids=bt.ids(axis='sample'), \n",
    "                                       otu_ids=bt.ids(axis='observation'), tree=treeBetaIn) \n",
    "\n",
    "### CALCULATE AN AVERAGE BETA DIVERSITY DISTANCE MATRIX FROM A LIST OF DATA MATRICES ###\n",
    "def table_beta_avg_matrices(distanceMatrixList):\n",
    "    ### STORAGE FOR AVERAGE DISTANCE MATRIX ###\n",
    "    dmAvg = None\n",
    "    ### FOR EACH DISTANCE MATRIX\n",
    "    for idx, dmIncrement in enumerate(distanceMatrixList):\n",
    "        ### GET NUMPY ARRAY OF FIRST DISTANCE MATRIX ###\n",
    "        if idx == 0: dmAvg = dmIncrement.data\n",
    "        ### ELSE ADD SUBSEQUENT MATRICES TO FIRST DISTANCE MATRIX ###\n",
    "        else: dmAvg = dmAvg + dmIncrement.data\n",
    "    ### RETURN DISTANCE MATRIX OBJECT DIVIDED BY THE NUMBER OF MATRICES ADDED TOGETHER ###\n",
    "    return DistanceMatrix(dmAvg/(idx+1),dmIncrement.ids)\n",
    "\n",
    "### UPGMA CLUSTERING ON BETA DIVERSITY DISTANCE MATRIX ###\n",
    "def table_beta_upgma_cluster(betaDiversityMatrixIn):\n",
    "    ### CALCULATE BETA DIVERSITY LINKAGE MATRIX ###\n",
    "    betaLinkage = linkage(betaDiversityMatrixIn.condensed_form(), method='average')\n",
    "    ### CALCULATE UPGMA TREE FROM LINKAGE MATRIX ###\n",
    "    upgmaTree = TreeNode.from_linkage_matrix(betaLinkage, betaDiversityMatrixIn.ids)\n",
    "    return upgmaTree\n",
    "\n",
    "### CALCULATE CONSENSUS TREE FROM LIST OF UPGMA TREES ###\n",
    "def table_beta_upgma_consensus(upgmaListIn): return sk.tree.majority_rule(upgmaListIn, weights=None, cutoff=0.5)\n",
    "\n",
    "##### PIPELINE TO PERFORM BETA DIVERSITY CALCULATIONS #####\n",
    "### THE INPUT BIOM TABLE bt WILL BE RAREFIED rarefactionsIn TIMES TO A DEPTH OF depthRareIn COUNTS PER SAMPLE ###\n",
    "### BETA DIVERSITY METRIC TO USE: 'bray_curtis','binary_jaccard','unweighted_unifrac','weighted_unifrac'   \n",
    "# UNIFRAC METRICS REQUIRE treeInIn BACTERIAL TREE\n",
    "def table_beta_pipeline(rarefiedTablesIn, betaMetricIn='bray_curtis', treeInIn=None, saveFolder=None):\n",
    "    print(\" - Metric In: \"+betaMetricIn)\n",
    "    \n",
    "    ### MAKE OUTPUT DIRECTORIES IF WRITING ###\n",
    "    if saveFolder != None:\n",
    "        if not os.path.isdir(saveFolder+\"/\"): os.makedirs(saveFolder+\"/\")\n",
    "        if not os.path.isdir(saveFolder+\"/rareDMs/\"): os.makedirs(saveFolder+\"/rareDMs/\")\n",
    "        if not os.path.isdir(saveFolder+\"/rareUPGMAs/\"): os.makedirs(saveFolder+\"/rareUPGMAs/\")\n",
    "\n",
    "    ### CALCULATE BETA DIVERSITY DISTANCE MATRIX (DM) FOR EACH RAREFIED TABLE ###\n",
    "    print(\" - Creating List of Distance Matrices for Metric \"+betaMetricIn+\" -> rareDMs\")\n",
    "    rarefiedDMsIn = []\n",
    "    for rareIdx, rareCycle in enumerate(rarefiedTablesIn): \n",
    "        \n",
    "        if betaMetricIn == 'bray_curtis':\n",
    "            dmCycleIn = beta_bray_curtis(rareCycle)\n",
    "            rarefiedDMsIn.append(dmCycleIn)\n",
    "            ### SAVE DISTANCE MATRICES ###\n",
    "            if saveFolder != None: \n",
    "                if not os.path.isfile(saveFolder+\"/rareDMs/dm_\"+str(rareIdx)+\".txt\"):\n",
    "                    dmCycleIn.write(saveFolder+\"/rareDMs/dm_\"+str(rareIdx)+\".txt\", format='lsmat')\n",
    "            \n",
    "        elif betaMetricIn == 'binary_jaccard':\n",
    "            dmCycleIn = beta_binary_jaccard(rareCycle)\n",
    "            rarefiedDMsIn.append(dmCycleIn)\n",
    "            ### SAVE DISTANCE MATRICES ###\n",
    "            if saveFolder != None: \n",
    "                if not os.path.isfile(saveFolder+\"/rareDMs/dm_\"+str(rareIdx)+\".txt\"):\n",
    "                    dmCycleIn.write(saveFolder+\"/rareDMs/dm_\"+str(rareIdx)+\".txt\", format='lsmat')\n",
    "                            \n",
    "        elif betaMetricIn == 'unweighted_unifrac':\n",
    "            dmCycleIn = beta_unweighted_unifrac(rareCycle, treeInIn)\n",
    "            rarefiedDMsIn.append(dmCycleIn)\n",
    "            ### SAVE DISTANCE MATRICES ###\n",
    "            if saveFolder != None: \n",
    "                if not os.path.isfile(saveFolder+\"/rareDMs/dm_\"+str(rareIdx)+\".txt\"):\n",
    "                    dmCycleIn.write(saveFolder+\"/rareDMs/dm_\"+str(rareIdx)+\".txt\", format='lsmat')\n",
    "                 \n",
    "        elif betaMetricIn == 'weighted_unifrac': \n",
    "            dmCycleIn = beta_weighted_unifrac(rareCycle, treeInIn)\n",
    "            rarefiedDMsIn.append(dmCycleIn)\n",
    "            ### SAVE DISTANCE MATRICES ###\n",
    "            if saveFolder != None: \n",
    "                if not os.path.isfile(saveFolder+\"/rareDMs/dm_\"+str(rareIdx)+\".txt\"):\n",
    "                    dmCycleIn.write(saveFolder+\"/rareDMs/dm_\"+str(rareIdx)+\".txt\", format='lsmat')\n",
    "                 \n",
    "        else: print(\"ERROR: Invalid Beta Metric!!!\"); return\n",
    "    ### CALCULATE AVERAGE DISTANCE MATRIX ###\n",
    "    print(\" - Calculating Average Distance Matrix -> consensusDM\")\n",
    "    consensusDMIn = beta_avg_matrices(rarefiedDMsIn)\n",
    "    if saveFolder != None: consensusDMIn.write(saveFolder+\"/consensus_dm.txt\", format='lsmat')\n",
    "    \n",
    "    ### CALCULATE UPGMA TREE FOR EACH DM ###\n",
    "    print(\" - Creating List of UPGMA Trees for Each Distance Matrix -> rareUPGMAs\")\n",
    "    rarefiedUPGMAList = []\n",
    "    for rareUPGMAidx, rareDMsCycle in enumerate(rarefiedDMsIn): \n",
    "        rareUPGMAIn = beta_upgma_cluster(rareDMsCycle)\n",
    "        rarefiedUPGMAList.append(rareUPGMAIn)\n",
    "        ### SAVE UPGMA TREES ###\n",
    "        if saveFolder != None: \n",
    "                if not os.path.isfile(saveFolder+\"/rareUPGMAs/upgma_\"+str(rareUPGMAidx)+\".tre\"):\n",
    "                    rareUPGMAIn.write(saveFolder+\"/rareUPGMAs/upgma_\"+str(rareUPGMAidx)+\".tre\", format='newick') \n",
    "        \n",
    "    ### CALCULATE CONSENSUS UPGMA TREE ###\n",
    "    print(\" - Calculating Average UPGMA Tree -> consensusUPGMA\")\n",
    "    consensusUPGMAIn = beta_upgma_consensus(rarefiedUPGMAList)[0]\n",
    "    if saveFolder != None: consensusUPGMAIn.write(saveFolder+\"/consensus_upgma.tre\", format='newick')\n",
    "        \n",
    "    ##### RETURN: LIST[RAREFIED_TABLES], LIST[RAREFIED_DMS], AVG_RAREFIED_DM, ListRarefiedUPGMA, consensusUPGMAIn\n",
    "    return rarefiedDMsIn, consensusDMIn, rarefiedUPGMAList, consensusUPGMAIn\n",
    "\n",
    "### BETA DIVERSITY ANALYSES ###\n",
    "def table_beta_pipeline_analysis(consensusDMIn, mapDfIn, mapCatIn, displayIn=True, savePath=None):\n",
    "    \n",
    "    ######### SORT DISTANCES INTO DICT[GROUP1][GROUP2]=LIST #########\n",
    "    valueDict = {}; print(\" - Sorting Distances into Dictionary - \")\n",
    "    sampleIndices = consensusDMIn.ids\n",
    "    \n",
    "    ### LOOP THROUGH DISTANCE MATRIX DIAGONALLY ###\n",
    "    for sampleOnePos, sampleOne in enumerate(sampleIndices[:-1]):\n",
    "        #if sampleOnePos%100==0: print(sampleOnePos)\n",
    "        \n",
    "        ### GET METADATA VALUE FOR SAMPLE AND CHECK DICTIONARY ###\n",
    "        sampleOneMeta = mapDfIn.loc[sampleOne][mapCatIn]\n",
    "        for sampleTwo in sampleIndices[(sampleOnePos+1):]:\n",
    "            sampleTwoMeta = mapDfIn.loc[sampleTwo][mapCatIn]\n",
    "            if hash(sampleOneMeta) < hash(sampleTwoMeta): \n",
    "                if sampleOneMeta not in valueDict.keys(): valueDict[sampleOneMeta] = {}\n",
    "                if sampleTwoMeta not in valueDict[sampleOneMeta].keys(): valueDict[sampleOneMeta][sampleTwoMeta] = []\n",
    "                valueDict[sampleOneMeta][sampleTwoMeta].append(consensusDMIn[sampleOne,sampleTwo])\n",
    "            else: \n",
    "                if sampleTwoMeta not in valueDict.keys(): valueDict[sampleTwoMeta] = {}\n",
    "                if sampleOneMeta not in valueDict[sampleTwoMeta].keys(): valueDict[sampleTwoMeta][sampleOneMeta] = []\n",
    "                valueDict[sampleTwoMeta][sampleOneMeta].append(consensusDMIn[sampleOne,sampleTwo])\n",
    "    \n",
    "    ### COLLATE INTRA GROUP DISTANCES ###\n",
    "    intra=[]; outList=[]; outList.append(\" ----- INTRA-GROUP COMPARISONS ----- \")\n",
    "    for group1 in valueDict.keys():\n",
    "        outList.append(\" - Intra: \"+group1+\" - \")\n",
    "        outList.append(\"   Mean Diversity    : \"+str(np.mean(valueDict[group1][group1])))\n",
    "        outList.append(\"   Standard Deviation: \"+str(np.std(valueDict[group1][group1])))\n",
    "        outList.append(\"\")\n",
    "        intra.extend(valueDict[group1][group1])\n",
    "\n",
    "    ### COLLATE INTER GROUP DISTANCES ###\n",
    "    inter=[];outList.append(\" ----- INTER-GROUP COMPARISONS ----- \")\n",
    "    for group1 in valueDict.keys():\n",
    "        for group2 in valueDict[group1].keys():\n",
    "            if group1 != group2:\n",
    "                outList.append(\" - Inter: \"+group1+\" - \"+group2+\" - \")\n",
    "                outList.append(\"   Mean Diversity    : \"+str(np.mean(valueDict[group1][group2])))\n",
    "                outList.append(\"   Standard Deviation: \"+str(np.std(valueDict[group1][group2])))\n",
    "                outList.append(\"\")\n",
    "                inter.extend(valueDict[group1][group2])\n",
    "\n",
    "    ### ALL GROUPS TOGETHER ###\n",
    "    outList.insert(0,\"\")\n",
    "    outList.insert(0,\"   Standard Deviation:\" + str(np.std(inter)))\n",
    "    outList.insert(0,\"   Mean Diversity    :\" + str(np.mean(inter)))\n",
    "    outList.insert(0,\" - ALL INTER-GROUP -\")\n",
    "    outList.insert(0,\"\")\n",
    "    outList.insert(0,\"   Standard Deviation:\" + str(np.std(intra)))\n",
    "    outList.insert(0,\"   Mean Diversity    :\" + str(np.mean(intra)))\n",
    "    outList.insert(0,\" - ALL INTRA-GROUP -\")\n",
    "\n",
    "    ######## STATISTICAL COMPARISONS ########\n",
    "    outList.extend([\" ----- STATISTICAL COMPARISONS -----\",\"\",\" --- ALL GROUPS INTRA VS INTER --- \",\"\"])\n",
    "    outList.extend(list_mannwhitney(intra, inter, l1Name=\"All Intra-Group\", l2Name=\"All Inter-Group\", bonferroniComparisons=1)[0])\n",
    "    outList.extend([\"\",\" --- COMPARISON OF INTRA-GROUP DISTANCES BETWEEN GROUPS --- \",\"\"])\n",
    "\n",
    "\n",
    "    ### INTRA-GROUP STATS ###\n",
    "    bonCorrect = (((len(valueDict.keys())-1)*len(valueDict.keys()))/2)\n",
    "    for group1 in valueDict.keys():\n",
    "        for group2 in valueDict[group1].keys():\n",
    "            if group1 != group2: \n",
    "                outList.extend(list_mannwhitney(valueDict[group1][group1], valueDict[group2][group2], l1Name=(\"Intra-\"+group1), l2Name=(\"Intra-\"+group2), bonferroniComparisons=bonCorrect)[0])\n",
    "                outList.append(\"\")\n",
    "\n",
    "    ### INTRA-GROUP & INTER-GROUP STATS ###\n",
    "    bonCorrect = ((len(valueDict.keys())-1)*len(valueDict.keys()))         \n",
    "    outList.extend([\"\",\" --- COMPARISON OF INTRA-GROUP DISTANCE WITH INTER- TO EACH OTHER GROUP --- \",\"\"])\n",
    "    for group1 in valueDict.keys():\n",
    "        for group2 in valueDict.keys():\n",
    "            if group1 != group2:\n",
    "                try: outList.extend(list_mannwhitney(valueDict[group1][group1], valueDict[group1][group2], l1Name=(\"Intra-\"+group1), l2Name=(\"Inter-\"+group1+\"-\"+group2), bonferroniComparisons=bonCorrect)[0])\n",
    "                except: outList.extend(list_mannwhitney(valueDict[group1][group1], valueDict[group2][group1], l1Name=(\"Intra-\"+group1), l2Name=(\"Inter-\"+group2+\"-\"+group1), bonferroniComparisons=bonCorrect)[0])\n",
    "                outList.append(\"\")\n",
    "\n",
    "    ### OUTPUT STATISTICS ###\n",
    "    if displayIn == True: list_print(outList)\n",
    "    if savePath != None: list_write(outList, savePath+\".txt\")\n",
    "    \n",
    "    ######## VIOLIN PLOT ########\n",
    "    \n",
    "    ##### COLLATE DISTANCES FOR VIOLIN PLOT #####\n",
    "    violinIn = [intra,inter]; violinLabels = [\"All Intra-Group\", \"All Inter-Group\"]; violinType = [0,1] # 0intra, 1inter\n",
    "    ### COLLATE INTER-INTRA ###\n",
    "    for group1 in valueDict.keys():\n",
    "        violinIn.append(valueDict[group1][group1]); violinLabels.append(\"Intra-\"+group1); violinType.append(0)\n",
    "        for group2 in valueDict.keys():\n",
    "            if group1 != group2: \n",
    "                try:    violinIn.append(valueDict[group1][group2]); violinLabels.append(\"Inter-\"+group1+\"-\"+group2); violinType.append(1)\n",
    "                except: violinIn.append(valueDict[group2][group1]); violinLabels.append(\"Inter-\"+group1+\"-\"+group2); violinType.append(1)\n",
    "\n",
    "    ### GENERATE VIOLIN PLOT BY AWB ###\n",
    "    fig, ax = plt.subplots(1,1, figsize=(20,10))\n",
    "    f1 = ax.violinplot(violinIn,np.arange(len(violinIn)),showmeans=True)\n",
    "\n",
    "    ### SET COLORS AND DIRECTIONALITY ###\n",
    "    for idxType, vType in enumerate(violinType):\n",
    "        if vType == 0:   b = f1['bodies'][idxType]; m = np.mean(b.get_paths()[0].vertices[:, 0]); b.get_paths()[0].vertices[:, 0] = np.clip(b.get_paths()[0].vertices[:, 0], -np.inf, m); b.set_color('r')\n",
    "        elif vType == 1: b = f1['bodies'][idxType]; m = np.mean(b.get_paths()[0].vertices[:, 0]); b.get_paths()[0].vertices[:, 0] = np.clip(b.get_paths()[0].vertices[:, 0], m, np.inf); b.set_color('b')\n",
    "\n",
    "    ### SET LABELS ###\n",
    "    ax.set_xticks(np.arange(len(violinLabels)))\n",
    "    ax.set_xticklabels(violinLabels, rotation=90)\n",
    "\n",
    "    ### LABEL AND OUTPUT ###\n",
    "    plt.title(\"Beta Diversity Distances\")\n",
    "    plt.xlim([-1,(len(violinLabels))])\n",
    "    plt.tight_layout()\n",
    "    if savePath != None: plt.savefig(savePath+\".pdf\")\n",
    "    if displayIn == True: plt.show()\n",
    "    plt.clf()\n",
    "    return valueDict\n",
    "\n",
    "### PERFORM ANOSIM TESTS ON LIST OF RAREFIED DISTANCE MATRICES AND COMPUTE AVERAGE STATISTICS ###\n",
    "def table_beta_pipeline_anosim(rareDMsIn, mapDfIn, categoryIn, permutationsIn=99, savePath=None, displayIn=True):\n",
    "    ### CALCULATE ANOSIM RESULTS FOR EACH DM AND STORE IN LIST ###\n",
    "    testStatSum = []; pValSum = []; outList=[]\n",
    "    ### LOOP THROUGH DISTANCE MATRICES ###\n",
    "    outList.append(\" - Calculating ANOSIM for Rarefied DMs Distinguishability by: \"+categoryIn+\" - \")\n",
    "    for idx, rareDMCycle in enumerate(rareDMsIn):\n",
    "        ### CALCULATE ANOSIM RESULTS ###\n",
    "        anosimCycleResults = sk.stats.distance.anosim(rareDMCycle, mapDfIn, column=categoryIn, permutations=permutationsIn)\n",
    "        ### ADD TEST STAT ###\n",
    "        testStatSum.append(anosimCycleResults['test statistic'])\n",
    "        ### ADD P-VALUE ###\n",
    "        pValSum.append(anosimCycleResults['p-value'])\n",
    "        ### SAVE VALUES ###\n",
    "        if savePath != None: anosimCycleResults.to_csv(path=savePath+\"_all.txt\", index=True, \n",
    "                                                       sep='\\t', na_rep='', header=True, mode='a')\n",
    "    \n",
    "    testStatAvg = np.mean(testStatSum); pValAvg = np.mean(pValSum)\n",
    "    outList.append(\"   - Average ANOSIM R-stat : \" + str(testStatAvg))\n",
    "    outList.append(\"   - Average ANOSIM P-val  : \" + str(pValAvg))\n",
    "    outList.append(\"   - Sample Size           : \" + str(anosimCycleResults['sample size']))\n",
    "    outList.append(\"   - Number of Groups      : \" + str(anosimCycleResults['number of groups']))\n",
    "    outList.append(\"   - Number of Permutations: \" + str(anosimCycleResults['number of permutations']))\n",
    "    outList.append(\"\")\n",
    "    if displayIn == True: list_print(outList)\n",
    "    if savePath != None: list_write(outList, savePath+\".txt\")\n",
    "    \"\"\"\n",
    "    if (savePath != None) or (displayIn==True):\n",
    "        try:\n",
    "            ##### ANOSIM PLOTS OF RESULTING R AND P VALUES #####\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "            ##### PLOT ANOSIM RESULTS #####\n",
    "            fig, axs = plt.subplots(2,2,figsize=(12,6))\n",
    "\n",
    "            ### PLOT R-VALUE DISTRIBUTION ###\n",
    "            plt.subplot(2,2,1)\n",
    "            sns.distplot(testStatSum, hist=True, kde=True, rug=True, bins=10, color=\"k\")\n",
    "            # LABEL PLOT #\n",
    "            plt.title(\"ANOSIM R-value Distribution\")\n",
    "            plt.xlabel(\"R-value\")\n",
    "            # ADD MEAN LINE IN RED #\n",
    "            ymin, ymax = plt.gca().get_ylim()\n",
    "            plt.vlines(np.mean(testStatSum),ymin,ymax,color='r')\n",
    "            # HIDE Y AXIS #\n",
    "            plt.gca().get_yaxis().set_visible(False)\n",
    "\n",
    "            ### PLOT R-VALUE DISTRIBUTION (0-1) ###\n",
    "            plt.subplot(2,2,2)\n",
    "            sns.distplot(testStatSum, hist=True, kde=True, rug=True, bins=10, color=\"k\")\n",
    "            # LABEL PLOT #\n",
    "            plt.title(\"ANOSIM R-value Distribution (0-1)\")\n",
    "            plt.xlabel(\"R-value\")\n",
    "            plt.xlim([0,1])\n",
    "            # ADD MEAN LINE IN RED #\n",
    "            ymin, ymax = plt.gca().get_ylim()\n",
    "            plt.vlines(np.mean(testStatSum),ymin,ymax,color='r')\n",
    "            # HIDE Y AXIS #\n",
    "            plt.gca().get_yaxis().set_visible(False)\n",
    "\n",
    "            ### PLOT P-VALUE DISTRIBUTION ###\n",
    "            plt.subplot(2,2,3)\n",
    "            sns.distplot(pValSum, hist=True, kde=True, rug=True, bins=10, color=\"k\")\n",
    "            # LABEL PLOT #\n",
    "            plt.title(\"ANOSIM P-value Distribution\")\n",
    "            plt.xlabel(\"P-value\")\n",
    "            # ADD MEAN LINE IN RED #\n",
    "            ymin, ymax = plt.gca().get_ylim()\n",
    "            plt.vlines(np.mean(pValSum),ymin,ymax,color='r')\n",
    "            # HIDE Y AXIS #\n",
    "            plt.gca().get_yaxis().set_visible(False)\n",
    "\n",
    "            ### PLOT P-VALUE DISTRIBUTION (0-1) ###\n",
    "            plt.subplot(2,2,4)\n",
    "            sns.distplot(pValSum, hist=True, kde=True, rug=True, bins=10, color=\"k\")\n",
    "            # LABEL PLOT #\n",
    "            plt.title(\"ANOSIM P-value Distribution (0-1)\")\n",
    "            plt.xlabel(\"P-value\")\n",
    "            plt.xlim([0,1])\n",
    "            # ADD MEAN LINE IN RED #\n",
    "            ymin, ymax = plt.gca().get_ylim()\n",
    "            plt.vlines(np.mean(pValSum),ymin,ymax,color='r')\n",
    "            # HIDE Y AXIS #\n",
    "            plt.gca().get_yaxis().set_visible(False)\n",
    "            plt.tight_layout()\n",
    "            ### OUTPUT ###\n",
    "            if savePath != None: plt.savefig(savePath+\".pdf\")\n",
    "            if displayIn == True: plt.show()\n",
    "            plt.clf()\n",
    "        except ValueError:\n",
    "            print(\"   Error Plotting ANOSIM!!!\")\n",
    "        warnings.filterwarnings(\"default\")\n",
    "    \"\"\"\n",
    "    return testStatAvg, pValAvg, testStatSum, pValSum\n",
    "\n",
    "##### PERFORM ANOSIM ON ALL SUBSETS OF A METADATA GROUPING #####     \n",
    "def table_beta_anosim_metadata_subsets(consensusDMIn, mapDfIn, mapCatIn, permutationsIn=99, savePath=None):\n",
    "    ### DATAFRAME TO STORE RESULTS ###\n",
    "    anosimDF = pd.DataFrame(data=None, index=None, columns=['test statistic','p-value'])\n",
    "    ### GET SUBSETS OF DM ###\n",
    "    dmCycler = dm_metadata_combinations(consensusDMIn, mapDfIn, mapCatIn)\n",
    "    ### FOR EACH SET OF GROUPS ###\n",
    "    for dmCycleIn in dmCycler:\n",
    "        ### CALCULATE ANOSIM ###\n",
    "        anosimResultsIn = sk.stats.distance.anosim(dmCycleIn[1], mapDfIn, column=mapCatIn, permutations=permutationsIn)\n",
    "        ### STORE RESULTS ###\n",
    "        anosimDF.loc[str(dmCycleIn[0])] = [anosimResultsIn['test statistic'],anosimResultsIn['p-value']]\n",
    "        ### OUTPUT RESULTS ###\n",
    "        anosimResultsIn[mapCatIn] = str(dmCycleIn[0])\n",
    "        if savePath != None: anosimResultsIn.to_csv(path=savePath+\"_table.txt\", index=True, \n",
    "                                                       sep='\\t', na_rep='', header=True, mode='a')\n",
    "        \n",
    "    return anosimDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; color:black;\">------------------------------------------------------------------------------</h1>\n",
    "<h1 style=\"text-align:center; color:orange;\"> - Load Data - </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Making Output Directory test_data/ag_analysis/ - \n",
      " - Importing Map: test_data/ag_analysis/1_1_qc_1000_map.txt - \n",
      " - Loading DF File - test_data/ag_analysis/1_1_qc_1000_map.txt\n",
      " - SUCCESS: Loaded DF from: test_data/ag_analysis/1_1_qc_1000_map.txt - \n",
      " - Importing Table:test_data/ag_analysis/1_0_qc_1000_table/1_0_qc_1000_table.txt\n",
      "   SUCCESS\n",
      " - Getting Metadata Categories - \n",
      " - Constructing Dictionary of Metadata by Samples - \n",
      " - Table Info - \n",
      "   Total Observations: 5591\n",
      "   Total Samples: 1375\n",
      "   Total Counts: 29041078.0\n",
      "   Non-Zero Entries: 616106\n",
      "   Table Density: 0.08014256678753191\n",
      " - Importing Tree: test_data/ag_analysis/1_2_qc_1000_tree.tre\n",
      " - Loading Bacterial Phylogeny - test_data/ag_analysis/1_2_qc_1000_tree.tre\n",
      "   SUCCESS\n",
      " - Importing Alpha: test_data/ag_analysis/3_0_alpha_diversity/3_0_alpha_diversity_1000.txt\n",
      "   SUCCESS\n"
     ]
    }
   ],
   "source": [
    "### LOAD A DATASET GIVEN PATHS ###\n",
    "def load_dataset(dirPath=None,mapPathIn=None,tablePathIn=None,treePathIn=None, alphaPathIn=None):\n",
    "    ### MAKE OUTPUT FOLDER ###\n",
    "    print(\" - Making Output Directory \"+dirPath+\" - \")\n",
    "    if not os.path.isdir(dirPath): os.makedirs(dirPath)\n",
    "    ### IMPORT MAPPING FILE ###\n",
    "    if mapPathIn is not None:\n",
    "        print(' - Importing Map: '+mapPathIn+' - ')\n",
    "        mapDfOut = DF((mapPathIn))\n",
    "        mapDfOut = mapDfOut.set_i(colNameOrList=[\"#SampleID\"], append=False)\n",
    "        mapDfOut = mapDfOut.df\n",
    "    ### IMPORT BIOM TABLE ###\n",
    "    if tablePathIn is not None:\n",
    "        print(' - Importing Table:'+tablePathIn); biomTableOut = load_table(tablePathIn); print('   SUCCESS');\n",
    "        ### GET TABLE METADATA ###\n",
    "        metaCatsOut, metaDictOut = table_metadata(biomTableOut, mapDfOut)\n",
    "        ### ADD METADATA TO BIOM TABLE ###\n",
    "        biomTableOut.add_metadata(metaDictOut)\n",
    "        ##### GET TABLE INFO #####\n",
    "        otus,otusCounts,samples,samplesCounts = table_info(biomTableOut, writePath=None, printOut = True)\n",
    "    ### IMPORT TREE ###\n",
    "    if treePathIn is not None:\n",
    "        try: print(' - Importing Tree: '+treePathIn); treeOut = tree_load(treePathIn); print('   SUCCESS')\n",
    "        except: print('   FAILED')\n",
    "    ##### LOAD ALPHA DIVERSITY #####\n",
    "    if alphaPathIn is not None:\n",
    "        try: print(' - Importing Alpha: '+alphaPathIn); alphaDf = pd.read_csv(alphaPathIn, sep='\\t', index_col=0, skiprows=0, verbose=False, low_memory=False); print('   SUCCESS')\n",
    "        except: print('   FAILED')\n",
    "            \n",
    "    return dirPath,mapDfOut,biomTableOut,treeOut,otus,otusCounts,samples,samplesCounts,metaCatsOut,metaDictOut,alphaDf\n",
    "\n",
    "### ROTATE LOADING CONSENSUS DISTANCE MATRICES FOR VARYING DEPTHS AND BETA DIVERSITY METRICS ###\n",
    "# RETURNS ConsensusIterator[curDepth, curMetric, consensusDMOut, rareDMIterator]\n",
    "def dm_iter_consensus(dirPath, betaMetricsIn=['bray_curtis','unweighted_unifrac'],betaDepthsIn=[1000,10000]):\n",
    "    ### IMPORT BETA ###\n",
    "    for depIn in betaDepthsIn:\n",
    "        for metIn in betaMetricsIn:\n",
    "            ### LOAD CONSENSUS DISTANCE MATRICES ###\n",
    "            print(\"importing: \"+dirPath+'4_0_beta_diversity/4_0_beta_'+str(depIn)+'_'+metIn+'/consensus_dm.txt')\n",
    "            conDMOut = DistanceMatrix.read(dirPath+'4_0_beta_diversity/4_0_beta_'+str(depIn)+'_'+metIn+'/consensus_dm.txt')\n",
    "            ### ITERATOR FOR INDIVIDUAL RAREFIED DMS ###\n",
    "            curRareDMs = dm_iter_folder(dirPath+'4_0_beta_diversity/4_0_beta_'+str(depIn)+'_'+metIn+'/rareDMs/*')\n",
    "                \n",
    "            yield depIn, metIn, conDMOut, curRareDMs\n",
    "\n",
    "\n",
    "### ROTATES LOADING EACH DISTANCE MATRIX FROM A FOLDER ###\n",
    "# INCORPORATES GLOB REGEX #\n",
    "def dm_iter_folder(dirPath='dm_folder/*'): \n",
    "    for i in glob.glob(dirPath): yield DistanceMatrix.read(i)\n",
    "\n",
    "        \n",
    "### LOAD SINGULAR DATA STRUCTURES ###\n",
    "dirPath,mapDf,biomTable,treeIn,otus,otusCounts,samples,samplesCounts,metaCats,metaDict,alphaDf = load_dataset(dirPath='test_data/ag_analysis/',\n",
    "             mapPathIn='test_data/ag_analysis/1_1_qc_1000_map.txt',\n",
    "             tablePathIn='test_data/ag_analysis/1_0_qc_1000_table/1_0_qc_1000_table.txt',\n",
    "             treePathIn='test_data/ag_analysis/1_2_qc_1000_tree.tre',\n",
    "             alphaPathIn='test_data/ag_analysis/3_0_alpha_diversity/3_0_alpha_diversity_1000.txt',\n",
    "            ) \n",
    "\n",
    "### LOAD CYCLER FOR DISTANCE MATRICES UNDER VARYING DEPTHS AND METRICS ###\n",
    "dmCycle = dm_iter_consensus(dirPath='test_data/ag_analysis/',betaMetricsIn=['bray_curtis','unweighted_unifrac'],betaDepthsIn=[1000,10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; color:black;\">------------------------------------------------------------------------------</h1>\n",
    "<h1 style=\"text-align:center; color:orange;\"> - PlayPen - </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; color:black;\">------------------------------------------------------------------------------</h1>\n",
    "<h2 style=\"text-align:center; color:blue;\"> - ANOSIM Variants - </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align:center; color:black;\"> - Examine ANOSIM across varying depths and metrics - </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### PERFORM ANOSIM ON ALL SUBSETS OF A METADATA GROUPING #####     \n",
    "def beta_anosim_metadata_subsets(consensusDMIn, mapDfIn, mapCatIn, permutationsIn=99, savePath=None):\n",
    "    ### DATAFRAME TO STORE RESULTS ###\n",
    "    anosimDF = pd.DataFrame(data=None, index=None, columns=['test statistic','p-value'])\n",
    "    ### GET SUBSETS OF DM ###\n",
    "    dmCycler = dm_metadata_combinations(consensusDMIn, mapDfIn, mapCatIn)\n",
    "    ### FOR EACH SET OF GROUPS ###\n",
    "    for dmCycleIn in dmCycler:\n",
    "        ### CALCULATE ANOSIM ###\n",
    "        anosimResultsIn = sk.stats.distance.anosim(dmCycleIn[1], mapDfIn, column=mapCatIn, permutations=permutationsIn)\n",
    "        ### STORE RESULTS ###\n",
    "        anosimDF.loc[str(dmCycleIn[0])] = [anosimResultsIn['test statistic'],anosimResultsIn['p-value']]\n",
    "        ### OUTPUT RESULTS ###\n",
    "        anosimResultsIn[mapCatIn] = str(dmCycleIn[0])\n",
    "        if savePath != None: anosimResultsIn.to_csv(path=savePath+\"_table.txt\", index=True, \n",
    "                                                       sep='\\t', na_rep='', header=True, mode='a')\n",
    "    ### RETURN DATAFRAME OF RESULTS ###\n",
    "    return anosimDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align:center; color:black;\"> - Examine ANOSIM across varying depths and metrics - </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Reading DM: 1000 bray_curtis\n",
      " - Reading DM: 1000 binary_jaccard\n",
      " - Reading DM: 10000 bray_curtis\n",
      " - Reading DM: 10000 binary_jaccard\n",
      " - Analyzing DM: 1000 binary_jaccard\n",
      " - All Table Combinations of 2 groups - \n",
      "   - Group: ('Caucasian', 'Hispanic')\n",
      "   - Group: ('Caucasian', 'Asian or Pacific Islander')\n",
      "   - Group: ('Caucasian', 'African American')\n",
      "   - Group: ('Hispanic', 'Asian or Pacific Islander')\n",
      "   - Group: ('Hispanic', 'African American')\n",
      "   - Group: ('Asian or Pacific Islander', 'African American')\n",
      " - All Table Combinations of 3 groups - \n",
      "   - Group: ('Caucasian', 'Hispanic', 'Asian or Pacific Islander')\n",
      "   - Group: ('Caucasian', 'Hispanic', 'African American')\n",
      "   - Group: ('Caucasian', 'Asian or Pacific Islander', 'African American')\n",
      "   - Group: ('Hispanic', 'Asian or Pacific Islander', 'African American')\n",
      " - All Table Combinations of 4 groups - \n",
      "   - Group: ('Caucasian', 'Hispanic', 'Asian or Pacific Islander', 'African American')\n",
      " - Analyzing DM: 1000 bray_curtis\n",
      " - All Table Combinations of 2 groups - \n",
      "   - Group: ('Caucasian', 'Hispanic')\n",
      "   - Group: ('Caucasian', 'Asian or Pacific Islander')\n",
      "   - Group: ('Caucasian', 'African American')\n",
      "   - Group: ('Hispanic', 'Asian or Pacific Islander')\n",
      "   - Group: ('Hispanic', 'African American')\n",
      "   - Group: ('Asian or Pacific Islander', 'African American')\n",
      " - All Table Combinations of 3 groups - \n",
      "   - Group: ('Caucasian', 'Hispanic', 'Asian or Pacific Islander')\n",
      "   - Group: ('Caucasian', 'Hispanic', 'African American')\n",
      "   - Group: ('Caucasian', 'Asian or Pacific Islander', 'African American')\n",
      "   - Group: ('Hispanic', 'Asian or Pacific Islander', 'African American')\n",
      " - All Table Combinations of 4 groups - \n",
      "   - Group: ('Caucasian', 'Hispanic', 'Asian or Pacific Islander', 'African American')\n",
      " - Analyzing DM: 10000 binary_jaccard\n",
      " - All Table Combinations of 2 groups - \n",
      "   - Group: ('Caucasian', 'Hispanic')\n",
      "   - Group: ('Caucasian', 'Asian or Pacific Islander')\n",
      "   - Group: ('Caucasian', 'African American')\n",
      "   - Group: ('Hispanic', 'Asian or Pacific Islander')\n",
      "   - Group: ('Hispanic', 'African American')\n",
      "   - Group: ('Asian or Pacific Islander', 'African American')\n",
      " - All Table Combinations of 3 groups - \n",
      "   - Group: ('Caucasian', 'Hispanic', 'Asian or Pacific Islander')\n",
      "   - Group: ('Caucasian', 'Hispanic', 'African American')\n",
      "   - Group: ('Caucasian', 'Asian or Pacific Islander', 'African American')\n",
      "   - Group: ('Hispanic', 'Asian or Pacific Islander', 'African American')\n",
      " - All Table Combinations of 4 groups - \n",
      "   - Group: ('Caucasian', 'Hispanic', 'Asian or Pacific Islander', 'African American')\n",
      " - Analyzing DM: 10000 bray_curtis\n",
      " - All Table Combinations of 2 groups - \n",
      "   - Group: ('Caucasian', 'Hispanic')\n",
      "   - Group: ('Caucasian', 'Asian or Pacific Islander')\n",
      "   - Group: ('Caucasian', 'African American')\n",
      "   - Group: ('Hispanic', 'Asian or Pacific Islander')\n",
      "   - Group: ('Hispanic', 'African American')\n",
      "   - Group: ('Asian or Pacific Islander', 'African American')\n",
      " - All Table Combinations of 3 groups - \n",
      "   - Group: ('Caucasian', 'Hispanic', 'Asian or Pacific Islander')\n",
      "   - Group: ('Caucasian', 'Hispanic', 'African American')\n",
      "   - Group: ('Caucasian', 'Asian or Pacific Islander', 'African American')\n",
      "   - Group: ('Hispanic', 'Asian or Pacific Islander', 'African American')\n",
      " - All Table Combinations of 4 groups - \n",
      "   - Group: ('Caucasian', 'Hispanic', 'Asian or Pacific Islander', 'African American')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>test statistic</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>depth</th>\n",
       "      <th>metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">('Asian or Pacific Islander', 'African American')</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1000</th>\n",
       "      <th>binary_jaccard</th>\n",
       "      <td>0.154271</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bray_curtis</th>\n",
       "      <td>-0.011845</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">10000</th>\n",
       "      <th>binary_jaccard</th>\n",
       "      <td>0.233627</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bray_curtis</th>\n",
       "      <td>-0.004701</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">('Caucasian', 'African American')</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1000</th>\n",
       "      <th>binary_jaccard</th>\n",
       "      <td>0.221099</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bray_curtis</th>\n",
       "      <td>0.081094</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">10000</th>\n",
       "      <th>binary_jaccard</th>\n",
       "      <td>0.277874</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bray_curtis</th>\n",
       "      <td>0.096240</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">('Caucasian', 'Asian or Pacific Islander')</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1000</th>\n",
       "      <th>binary_jaccard</th>\n",
       "      <td>0.108984</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bray_curtis</th>\n",
       "      <td>0.157208</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">10000</th>\n",
       "      <th>binary_jaccard</th>\n",
       "      <td>0.111655</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bray_curtis</th>\n",
       "      <td>0.161586</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">('Caucasian', 'Asian or Pacific Islander', 'African American')</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1000</th>\n",
       "      <th>binary_jaccard</th>\n",
       "      <td>0.126655</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bray_curtis</th>\n",
       "      <td>0.146312</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">10000</th>\n",
       "      <th>binary_jaccard</th>\n",
       "      <td>0.137943</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bray_curtis</th>\n",
       "      <td>0.152374</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">('Caucasian', 'Hispanic')</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1000</th>\n",
       "      <th>binary_jaccard</th>\n",
       "      <td>-0.176563</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bray_curtis</th>\n",
       "      <td>-0.132173</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">10000</th>\n",
       "      <th>binary_jaccard</th>\n",
       "      <td>-0.161706</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bray_curtis</th>\n",
       "      <td>-0.138136</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">('Caucasian', 'Hispanic', 'African American')</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1000</th>\n",
       "      <th>binary_jaccard</th>\n",
       "      <td>-0.052909</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bray_curtis</th>\n",
       "      <td>-0.066255</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">10000</th>\n",
       "      <th>binary_jaccard</th>\n",
       "      <td>-0.024681</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bray_curtis</th>\n",
       "      <td>-0.065601</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">('Caucasian', 'Hispanic', 'Asian or Pacific Islander')</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1000</th>\n",
       "      <th>binary_jaccard</th>\n",
       "      <td>0.027325</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bray_curtis</th>\n",
       "      <td>0.075259</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">10000</th>\n",
       "      <th>binary_jaccard</th>\n",
       "      <td>0.033464</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bray_curtis</th>\n",
       "      <td>0.076664</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">('Caucasian', 'Hispanic', 'Asian or Pacific Islander', 'African American')</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1000</th>\n",
       "      <th>binary_jaccard</th>\n",
       "      <td>0.049993</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bray_curtis</th>\n",
       "      <td>0.076397</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">10000</th>\n",
       "      <th>binary_jaccard</th>\n",
       "      <td>0.062304</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bray_curtis</th>\n",
       "      <td>0.079425</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">('Hispanic', 'African American')</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1000</th>\n",
       "      <th>binary_jaccard</th>\n",
       "      <td>0.366344</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bray_curtis</th>\n",
       "      <td>0.234405</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">10000</th>\n",
       "      <th>binary_jaccard</th>\n",
       "      <td>0.367661</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bray_curtis</th>\n",
       "      <td>0.251902</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">('Hispanic', 'Asian or Pacific Islander')</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1000</th>\n",
       "      <th>binary_jaccard</th>\n",
       "      <td>-0.053665</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bray_curtis</th>\n",
       "      <td>-0.103403</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">10000</th>\n",
       "      <th>binary_jaccard</th>\n",
       "      <td>-0.032071</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bray_curtis</th>\n",
       "      <td>-0.110063</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">('Hispanic', 'Asian or Pacific Islander', 'African American')</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1000</th>\n",
       "      <th>binary_jaccard</th>\n",
       "      <td>0.025353</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bray_curtis</th>\n",
       "      <td>-0.070532</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">10000</th>\n",
       "      <th>binary_jaccard</th>\n",
       "      <td>0.065995</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bray_curtis</th>\n",
       "      <td>-0.071884</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                         test statistic  \\\n",
       "                                                   depth metric                           \n",
       "('Asian or Pacific Islander', 'African American')  1000  binary_jaccard        0.154271   \n",
       "                                                         bray_curtis          -0.011845   \n",
       "                                                   10000 binary_jaccard        0.233627   \n",
       "                                                         bray_curtis          -0.004701   \n",
       "('Caucasian', 'African American')                  1000  binary_jaccard        0.221099   \n",
       "                                                         bray_curtis           0.081094   \n",
       "                                                   10000 binary_jaccard        0.277874   \n",
       "                                                         bray_curtis           0.096240   \n",
       "('Caucasian', 'Asian or Pacific Islander')         1000  binary_jaccard        0.108984   \n",
       "                                                         bray_curtis           0.157208   \n",
       "                                                   10000 binary_jaccard        0.111655   \n",
       "                                                         bray_curtis           0.161586   \n",
       "('Caucasian', 'Asian or Pacific Islander', 'Afr... 1000  binary_jaccard        0.126655   \n",
       "                                                         bray_curtis           0.146312   \n",
       "                                                   10000 binary_jaccard        0.137943   \n",
       "                                                         bray_curtis           0.152374   \n",
       "('Caucasian', 'Hispanic')                          1000  binary_jaccard       -0.176563   \n",
       "                                                         bray_curtis          -0.132173   \n",
       "                                                   10000 binary_jaccard       -0.161706   \n",
       "                                                         bray_curtis          -0.138136   \n",
       "('Caucasian', 'Hispanic', 'African American')      1000  binary_jaccard       -0.052909   \n",
       "                                                         bray_curtis          -0.066255   \n",
       "                                                   10000 binary_jaccard       -0.024681   \n",
       "                                                         bray_curtis          -0.065601   \n",
       "('Caucasian', 'Hispanic', 'Asian or Pacific Isl... 1000  binary_jaccard        0.027325   \n",
       "                                                         bray_curtis           0.075259   \n",
       "                                                   10000 binary_jaccard        0.033464   \n",
       "                                                         bray_curtis           0.076664   \n",
       "('Caucasian', 'Hispanic', 'Asian or Pacific Isl... 1000  binary_jaccard        0.049993   \n",
       "                                                         bray_curtis           0.076397   \n",
       "                                                   10000 binary_jaccard        0.062304   \n",
       "                                                         bray_curtis           0.079425   \n",
       "('Hispanic', 'African American')                   1000  binary_jaccard        0.366344   \n",
       "                                                         bray_curtis           0.234405   \n",
       "                                                   10000 binary_jaccard        0.367661   \n",
       "                                                         bray_curtis           0.251902   \n",
       "('Hispanic', 'Asian or Pacific Islander')          1000  binary_jaccard       -0.053665   \n",
       "                                                         bray_curtis          -0.103403   \n",
       "                                                   10000 binary_jaccard       -0.032071   \n",
       "                                                         bray_curtis          -0.110063   \n",
       "('Hispanic', 'Asian or Pacific Islander', 'Afri... 1000  binary_jaccard        0.025353   \n",
       "                                                         bray_curtis          -0.070532   \n",
       "                                                   10000 binary_jaccard        0.065995   \n",
       "                                                         bray_curtis          -0.071884   \n",
       "\n",
       "                                                                         p-value  \n",
       "                                                   depth metric                   \n",
       "('Asian or Pacific Islander', 'African American')  1000  binary_jaccard     0.08  \n",
       "                                                         bray_curtis        0.50  \n",
       "                                                   10000 binary_jaccard     0.01  \n",
       "                                                         bray_curtis        0.63  \n",
       "('Caucasian', 'African American')                  1000  binary_jaccard     0.04  \n",
       "                                                         bray_curtis        0.20  \n",
       "                                                   10000 binary_jaccard     0.04  \n",
       "                                                         bray_curtis        0.13  \n",
       "('Caucasian', 'Asian or Pacific Islander')         1000  binary_jaccard     0.03  \n",
       "                                                         bray_curtis        0.01  \n",
       "                                                   10000 binary_jaccard     0.01  \n",
       "                                                         bray_curtis        0.01  \n",
       "('Caucasian', 'Asian or Pacific Islander', 'Afr... 1000  binary_jaccard     0.01  \n",
       "                                                         bray_curtis        0.01  \n",
       "                                                   10000 binary_jaccard     0.01  \n",
       "                                                         bray_curtis        0.01  \n",
       "('Caucasian', 'Hispanic')                          1000  binary_jaccard     0.98  \n",
       "                                                         bray_curtis        0.98  \n",
       "                                                   10000 binary_jaccard     0.99  \n",
       "                                                         bray_curtis        0.96  \n",
       "('Caucasian', 'Hispanic', 'African American')      1000  binary_jaccard     0.80  \n",
       "                                                         bray_curtis        0.82  \n",
       "                                                   10000 binary_jaccard     0.64  \n",
       "                                                         bray_curtis        0.85  \n",
       "('Caucasian', 'Hispanic', 'Asian or Pacific Isl... 1000  binary_jaccard     0.17  \n",
       "                                                         bray_curtis        0.04  \n",
       "                                                   10000 binary_jaccard     0.24  \n",
       "                                                         bray_curtis        0.02  \n",
       "('Caucasian', 'Hispanic', 'Asian or Pacific Isl... 1000  binary_jaccard     0.08  \n",
       "                                                         bray_curtis        0.01  \n",
       "                                                   10000 binary_jaccard     0.04  \n",
       "                                                         bray_curtis        0.05  \n",
       "('Hispanic', 'African American')                   1000  binary_jaccard     0.01  \n",
       "                                                         bray_curtis        0.02  \n",
       "                                                   10000 binary_jaccard     0.01  \n",
       "                                                         bray_curtis        0.01  \n",
       "('Hispanic', 'Asian or Pacific Islander')          1000  binary_jaccard     0.87  \n",
       "                                                         bray_curtis        0.98  \n",
       "                                                   10000 binary_jaccard     0.70  \n",
       "                                                         bray_curtis        0.97  \n",
       "('Hispanic', 'Asian or Pacific Islander', 'Afri... 1000  binary_jaccard     0.35  \n",
       "                                                         bray_curtis        0.90  \n",
       "                                                   10000 binary_jaccard     0.17  \n",
       "                                                         bray_curtis        0.91  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "######## ANOSIM VARYING DEPTHS AND METRICS #######\n",
    "##### IMPORT CONSENSUS DISTANCE MATRICES #####\n",
    "# DISTANCE MATRICES WILL BE STORED IN A DICTIONARY consensusDMs[DEPTH][BETA_METRIC] = DISTANCE MATRIX\n",
    "depthsIn = [1000, 10000]\n",
    "metricsIn =['bray_curtis','binary_jaccard']#,'unweighted_unifrac','weighted_unifrac']\n",
    "mapCat = 'race'\n",
    "\n",
    "### EXTRACT DISTANCE MATRICES FROM FILES ###\n",
    "consensusDMs = {}\n",
    "for depIn in depthsIn:\n",
    "    consensusDMs[depIn] = {}\n",
    "    for metIn in metricsIn:\n",
    "        print(' - Reading DM: '+str(depIn)+' '+metIn)\n",
    "        consensusDMs[depIn][metIn] = DistanceMatrix.read('test_data/ag_analysis/4_0_beta_diversity/4_0_beta_'+str(depIn)+'_'+metIn+'/consensus_dm.txt')\n",
    "\n",
    "##### PERFORM ANOSIM SUBSET ANALYSES FOR EACH DEPTH AND METRIC #####\n",
    "anosimSubsets = {}\n",
    "anosimList = []\n",
    "### FOR EACH DEPTH ###\n",
    "for depIn in consensusDMs.keys():\n",
    "    anosimSubsets[depIn] = {}\n",
    "    ### FOR EACH BETA METRIC ###\n",
    "    for metIn in consensusDMs[depIn].keys():\n",
    "        ### ANALYZE ANOSIM ON ALL SUBSETS OF A METADATA CATEGORY FOR DEPTH depIn AND BETA METRIC metIn\n",
    "        print(' - Analyzing DM: '+str(depIn)+' '+metIn)\n",
    "        anosimRes = beta_anosim_metadata_subsets(consensusDMs[depIn][metIn], mapDf, mapCat, permutationsIn=99, savePath=None)\n",
    "        anosimRes['metric'] = metIn; anosimRes['depth']=depIn\n",
    "        anosimSubsets[depIn][metIn] = anosimRes\n",
    "        anosimList.append(anosimRes)\n",
    "\n",
    "anosimAll = pd.concat(anosimList,axis=0)\n",
    "anosimAll.set_index('depth',append=True, inplace=True)\n",
    "anosimAll.set_index('metric',append=True, inplace=True)\n",
    "display(anosimAll.sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align:center; color:black;\"> - Examine ANOSIM when SubSetting Individuals from Each Race - </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Reading DM: 1000 bray_curtis\n",
      " - Reading DM: 1000 binary_jaccard\n",
      " - Reading DM: 10000 bray_curtis\n",
      " - Reading DM: 10000 binary_jaccard\n"
     ]
    }
   ],
   "source": [
    "######## ANOSIM VARYING DEPTHS AND METRICS #######\n",
    "##### IMPORT CONSENSUS DISTANCE MATRICES #####\n",
    "# DISTANCE MATRICES WILL BE STORED IN A DICTIONARY consensusDMs[DEPTH][BETA_METRIC] = DISTANCE MATRIX\n",
    "depthsIn = [1000, 10000]\n",
    "metricsIn =['bray_curtis','binary_jaccard']#,'unweighted_unifrac','weighted_unifrac']\n",
    "mapCat = 'race'\n",
    "\n",
    "### EXTRACT DISTANCE MATRICES FROM FILES ###\n",
    "consensusDMs = {}\n",
    "for depIn in depthsIn:\n",
    "    consensusDMs[depIn] = {}\n",
    "    for metIn in metricsIn:\n",
    "        print(' - Reading DM: '+str(depIn)+' '+metIn)\n",
    "        consensusDMs[depIn][metIn] = DistanceMatrix.read('test_data/ag_analysis/4_0_beta_diversity/4_0_beta_'+str(depIn)+'_'+metIn+'/consensus_dm.txt')\n",
    "\n",
    "##### PERFORM ANOSIM SUBSET ANALYSES FOR EACH DEPTH AND METRIC #####\n",
    "anosimSubsets = {}\n",
    "anosimList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Loading DF File - test_data/ag_analysis/1_1_qc_1000_map.txt\n",
      " - SUCCESS: Loaded DF from: test_data/ag_analysis/1_1_qc_1000_map.txt - \n",
      " - Analyzing DM: 1000 binary_jaccard\n",
      "10 0.0655740740741 0.042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.065574074074073993,\n",
       " 0.065574074074073993,\n",
       " 0.065574074074073993,\n",
       " 0.065574074074073993,\n",
       " 0.065574074074073993,\n",
       " 0.065574074074073993,\n",
       " 0.065574074074073993,\n",
       " 0.065574074074073993,\n",
       " 0.065574074074073993,\n",
       " 0.065574074074073993]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.059999999999999998,\n",
       " 0.029999999999999999,\n",
       " 0.050000000000000003,\n",
       " 0.050000000000000003,\n",
       " 0.070000000000000007,\n",
       " 0.029999999999999999,\n",
       " 0.040000000000000001,\n",
       " 0.040000000000000001,\n",
       " 0.02,\n",
       " 0.029999999999999999]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Analyzing DM: 1000 bray_curtis\n",
      "10 0.0436666666667 0.092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.043666666666666742,\n",
       " 0.043666666666666742,\n",
       " 0.043666666666666742,\n",
       " 0.043666666666666742,\n",
       " 0.043666666666666742,\n",
       " 0.043666666666666742,\n",
       " 0.043666666666666742,\n",
       " 0.043666666666666742,\n",
       " 0.043666666666666742,\n",
       " 0.043666666666666742]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.089999999999999997,\n",
       " 0.089999999999999997,\n",
       " 0.11,\n",
       " 0.11,\n",
       " 0.089999999999999997,\n",
       " 0.080000000000000002,\n",
       " 0.12,\n",
       " 0.10000000000000001,\n",
       " 0.050000000000000003,\n",
       " 0.080000000000000002]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Loading DF File - test_data/ag_analysis/1_1_qc_10000_map.txt\n",
      " - SUCCESS: Loaded DF from: test_data/ag_analysis/1_1_qc_10000_map.txt - \n",
      " - Analyzing DM: 10000 binary_jaccard\n",
      "10 0.0824444444444 0.014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.082444444444444528,\n",
       " 0.082444444444444528,\n",
       " 0.082444444444444528,\n",
       " 0.082444444444444528,\n",
       " 0.082444444444444528,\n",
       " 0.082444444444444528,\n",
       " 0.082444444444444528,\n",
       " 0.082444444444444528,\n",
       " 0.082444444444444528,\n",
       " 0.082444444444444528]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.02, 0.02, 0.01, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.02]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "cannot set a row with mismatched columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-c3d6d0a11fa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrStatsOut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpvalsOut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0manosimTT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepIn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmetIn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminSam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrStatsOut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpvalsOut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetIn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepIn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminSam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincSamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0manosimTT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'metric'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/brooks/miniconda2/envs/qiime2/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_has_valid_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/brooks/miniconda2/envs/qiime2/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    377\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m                                 raise ValueError(\"cannot set a row with \"\n\u001b[0m\u001b[1;32m    380\u001b[0m                                                  \"mismatched columns\")\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot set a row with mismatched columns"
     ]
    }
   ],
   "source": [
    "\n",
    "##### ANOSIM SUBSET INDIVIDUALS FROM GROUPS - DIFFERENTLY SAMPLED GROUPINGS #####\n",
    "anosimTT = pd.DataFrame(data=None, index=None, columns=['test statistic','p-value', 'metric', 'depth', 'subsample', 'numsamples'])\n",
    "\n",
    "minSamples = [10]#,15,20,25,30,35,40,45,50,75,100,250,500,1000,2000] \n",
    "subSampleNumber = 10\n",
    "\n",
    "\n",
    "### PERFORM ANOSIM ON SUBSETS OF EACH RACE ###\n",
    "for depIn in consensusDMs.keys():\n",
    "    if depIn == 1000: mapDf = DF(\"test_data/ag_analysis/1_1_qc_1000_map.txt\").df\n",
    "    if depIn == 10000: mapDf = DF(\"test_data/ag_analysis/1_1_qc_10000_map.txt\").df\n",
    "    mapDf = mapDf.set_index('#SampleID')\n",
    "    anosimSubsets[depIn] = {}\n",
    "    for metIn in consensusDMs[depIn].keys():\n",
    "        print(' - Analyzing DM: '+str(depIn)+' '+metIn)\n",
    "        \n",
    "        ### FOR EACH SAMPLING SIZE ###\n",
    "        for minSam in minSamples:\n",
    "            \n",
    "            pvalsOut = [];rStatsOut=[]\n",
    "            ### FOR THE NUMBER OF SUBSAMPLES ###\n",
    "            for numSubSample in np.arange(subSampleNumber):\n",
    "                ### FOR EACH ETHNICITY EXTRACT SAMPLES ###\n",
    "                incSamples = []\n",
    "                for eIn in mapDf['race'].unique():\n",
    "                    if minSam <= mapDf[mapDf['race'] == eIn].shape[0]: \n",
    "                        incSamples.extend(mapDf[mapDf['race'] == eIn].sample(n=minSam, axis=0, replace=False,  random_state=54321).index)\n",
    "                    else: incSamples.extend(mapDf[mapDf['race'] == eIn].index)\n",
    "                ### TRIM DISTANCE MATRIX ###\n",
    "                #print(incSamples); print()\n",
    "                curDMFiltered = consensusDMs[depIn][metIn].filter(incSamples)\n",
    "                anosimRes = sk.stats.distance.anosim(curDMFiltered, mapDf, column='race', permutations=99)\n",
    "                pvalsOut.append(anosimRes['p-value']); rStatsOut.append(anosimRes['test statistic'])\n",
    "            \n",
    "            print(str(minSam), str(np.mean(rStatsOut)),str(np.mean(pvalsOut)))\n",
    "            display(rStatsOut)\n",
    "            display(pvalsOut)\n",
    "            anosimTT.loc[str(depIn)+\"_\"+metIn+\"_\"+str(minSam)] = [np.mean(rStatsOut),np.mean(pvalsOut), metIn, depIn, minSam,len(incSamples)] \n",
    "    \n",
    "    anosimTT.set_index('metric', append=True, inplace=True)\n",
    "    anosimTT.set_index('depth', append=True, inplace=True)\n",
    "    anosimTT.set_index('subsample', append=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#SampleID</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_years</th>\n",
       "      <th>ageF</th>\n",
       "      <th>bmi</th>\n",
       "      <th>non_food_allergies_beestings</th>\n",
       "      <th>tonsils_removed</th>\n",
       "      <th>country_of_birth</th>\n",
       "      <th>pets_other</th>\n",
       "      <th>...</th>\n",
       "      <th>pku</th>\n",
       "      <th>state</th>\n",
       "      <th>age_corrected</th>\n",
       "      <th>mental_illness_type_unspecified</th>\n",
       "      <th>simple_body_site</th>\n",
       "      <th>title_acronym</th>\n",
       "      <th>title_body_site</th>\n",
       "      <th>hmp_site</th>\n",
       "      <th>age_group_custom</th>\n",
       "      <th>age_group_custom2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10317.000006703</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>male</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>27.47</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>WV</td>\n",
       "      <td>54</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_50_55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10317.000027821</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>male</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25.54</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>VA</td>\n",
       "      <td>25</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_18_29</td>\n",
       "      <td>Age_25_29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10317.000009594</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>female</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>20.70</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>VA</td>\n",
       "      <td>52</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_50_55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10317.000014543</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>21.05</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>NC</td>\n",
       "      <td>46</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_45_49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10317.000014548</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>male</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>35.17</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>NJ</td>\n",
       "      <td>39</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_30_39</td>\n",
       "      <td>Age_35_39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10317.000028894</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>38.38</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>TX</td>\n",
       "      <td>55</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_50_55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10317.000009766</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>19.83</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>OH</td>\n",
       "      <td>41</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_40_44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10317.000004738</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>male</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>21.46</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>TX</td>\n",
       "      <td>37</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_30_39</td>\n",
       "      <td>Age_35_39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10317.000014608</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>17.62</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>IL</td>\n",
       "      <td>30</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_30_39</td>\n",
       "      <td>Age_30_34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10317.000004734</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>23.38</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>GA</td>\n",
       "      <td>31</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_30_39</td>\n",
       "      <td>Age_30_34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10317.000004737</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>19.27</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>IA</td>\n",
       "      <td>33</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_30_39</td>\n",
       "      <td>Age_30_34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10317.000004736</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>24.80</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>Romania</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>IL</td>\n",
       "      <td>50</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_50_55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10317.000022638</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>male</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>23.62</td>\n",
       "      <td>False</td>\n",
       "      <td>Not sure</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>AK</td>\n",
       "      <td>37</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_30_39</td>\n",
       "      <td>Age_35_39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10317.000022636</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>11.46</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>WA</td>\n",
       "      <td>38</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_30_39</td>\n",
       "      <td>Age_35_39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10317.000013332</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>33.56</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>MD</td>\n",
       "      <td>45</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_45_49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10317.000039907</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>20.34</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>false</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>MA</td>\n",
       "      <td>45</td>\n",
       "      <td>true</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_45_49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10317.000003492</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>male</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>20.99</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>NH</td>\n",
       "      <td>47</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_45_49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10317.000002773</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>23.15</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>CA</td>\n",
       "      <td>50</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_50_55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10317.000002774</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>male</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>24.16</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>CA</td>\n",
       "      <td>49</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_45_49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10317.000009507</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>30.49</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>NJ</td>\n",
       "      <td>55</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_50_55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10317.000041935</td>\n",
       "      <td>Asian or Pacific Islander</td>\n",
       "      <td>male</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>20.53</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>false</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>MA</td>\n",
       "      <td>36</td>\n",
       "      <td>true</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_30_39</td>\n",
       "      <td>Age_35_39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10317.000030744</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>27.26</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>MA</td>\n",
       "      <td>23</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_18_29</td>\n",
       "      <td>Age_18_24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10317.000013454</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>male</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>26.64</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>CA</td>\n",
       "      <td>49</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_45_49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10317.000014958</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>24.25</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>CA</td>\n",
       "      <td>54</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_50_55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10317.000014954</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>male</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>19.92</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>NY</td>\n",
       "      <td>34</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_30_39</td>\n",
       "      <td>Age_30_34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10317.000010274</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>male</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>22.04</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>CA</td>\n",
       "      <td>55</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_50_55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10317.000014224</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>male</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>23.48</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>OR</td>\n",
       "      <td>38</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_30_39</td>\n",
       "      <td>Age_35_39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10317.000023223</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>male</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>33.72</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>WA</td>\n",
       "      <td>46</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_45_49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10317.000015577</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>male</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>22.86</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>CA</td>\n",
       "      <td>53</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_50_55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10317.000011055</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>male</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>25.25</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>NC</td>\n",
       "      <td>47</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_45_49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>10317.000016725</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>male</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>21.98</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>Spain</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>CO</td>\n",
       "      <td>26</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_18_29</td>\n",
       "      <td>Age_25_29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>10317.000007746</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>19.71</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>UT</td>\n",
       "      <td>36</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_30_39</td>\n",
       "      <td>Age_35_39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>10317.000016166</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>19.06</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>FL</td>\n",
       "      <td>33</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_30_39</td>\n",
       "      <td>Age_30_34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1348</th>\n",
       "      <td>10317.000009111</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>24.75</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>NH</td>\n",
       "      <td>33</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_30_39</td>\n",
       "      <td>Age_30_34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>10317.000016727</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>male</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>21.98</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>Spain</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>CO</td>\n",
       "      <td>26</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_18_29</td>\n",
       "      <td>Age_25_29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>10317.000043118</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>male</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>24.12</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>Germany</td>\n",
       "      <td>false</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>OR</td>\n",
       "      <td>50</td>\n",
       "      <td>true</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_50_55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>10317.000028723</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>male</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>27.44</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>MI</td>\n",
       "      <td>55</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_50_55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>10317.000002711</td>\n",
       "      <td>Asian or Pacific Islander</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>20.90</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>CA</td>\n",
       "      <td>38</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_30_39</td>\n",
       "      <td>Age_35_39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>10317.000002712</td>\n",
       "      <td>Asian or Pacific Islander</td>\n",
       "      <td>male</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>26.30</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>CA</td>\n",
       "      <td>42</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_40_44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354</th>\n",
       "      <td>10317.000004641</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>23.88</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>FL</td>\n",
       "      <td>39</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_30_39</td>\n",
       "      <td>Age_35_39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1355</th>\n",
       "      <td>10317.000009533</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>39.14</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>IL</td>\n",
       "      <td>23</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_18_29</td>\n",
       "      <td>Age_18_24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356</th>\n",
       "      <td>10317.000011259</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>21.45</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>ID</td>\n",
       "      <td>44</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_40_44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1357</th>\n",
       "      <td>10317.000018035</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>17.30</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>TX</td>\n",
       "      <td>51</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_50_55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1358</th>\n",
       "      <td>10317.000018033</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>male</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>22.10</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>SC</td>\n",
       "      <td>31</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_30_39</td>\n",
       "      <td>Age_30_34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1359</th>\n",
       "      <td>10317.000005878</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>male</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>24.33</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>35</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_30_39</td>\n",
       "      <td>Age_35_39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>10317.000011072</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>male</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>24.67</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>WA</td>\n",
       "      <td>48</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_45_49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>10317.000011073</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>male</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>24.67</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>WA</td>\n",
       "      <td>48</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_45_49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1362</th>\n",
       "      <td>10317.000011074</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>male</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>25.80</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>CA</td>\n",
       "      <td>44</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_40_44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363</th>\n",
       "      <td>10317.000011077</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>20.90</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>ME</td>\n",
       "      <td>29</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_18_29</td>\n",
       "      <td>Age_25_29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364</th>\n",
       "      <td>10317.000011369</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>19.13</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>CA</td>\n",
       "      <td>43</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_40_44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365</th>\n",
       "      <td>10317.000003432</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>male</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>23.51</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>CO</td>\n",
       "      <td>42</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_40_44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366</th>\n",
       "      <td>10317.000003125</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>26.45</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>NC</td>\n",
       "      <td>34</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_30_39</td>\n",
       "      <td>Age_30_34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>10317.000003439</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>23.53</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>TN</td>\n",
       "      <td>50</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_50_55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>10317.000013972</td>\n",
       "      <td>Asian or Pacific Islander</td>\n",
       "      <td>male</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>21.15</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>CO</td>\n",
       "      <td>31</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_30_39</td>\n",
       "      <td>Age_30_34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>10317.000036151</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>26.04</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes</td>\n",
       "      <td>United States</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>CA</td>\n",
       "      <td>52</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_50_55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>10317.000038424</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>29.05</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>OR</td>\n",
       "      <td>52</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_50_55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>10317.000012254</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>male</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>21.30</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>CA</td>\n",
       "      <td>54</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_50_55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1372</th>\n",
       "      <td>10317.000016313</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>23.53</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>NY</td>\n",
       "      <td>39</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_30_39</td>\n",
       "      <td>Age_35_39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>10317.000016315</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>21.26</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>WA</td>\n",
       "      <td>40</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_40_55</td>\n",
       "      <td>Age_40_44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>10317.000003196</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>female</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>22.21</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>United States</td>\n",
       "      <td>no_data</td>\n",
       "      <td>...</td>\n",
       "      <td>I do not have this condition</td>\n",
       "      <td>MN</td>\n",
       "      <td>35</td>\n",
       "      <td>no_data</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>AGP</td>\n",
       "      <td>AGP-FECAL</td>\n",
       "      <td>FECAL</td>\n",
       "      <td>Age_30_39</td>\n",
       "      <td>Age_35_39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1375 rows × 197 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            #SampleID                       race     sex  age_years  ageF  \\\n",
       "0     10317.000006703                  Caucasian    male         54    54   \n",
       "1     10317.000027821                  Caucasian    male         25    25   \n",
       "2     10317.000009594                   Hispanic  female         52    52   \n",
       "3     10317.000014543                  Caucasian  female         46    46   \n",
       "4     10317.000014548                  Caucasian    male         39    39   \n",
       "5     10317.000028894                  Caucasian  female         55    55   \n",
       "6     10317.000009766                  Caucasian  female         41    41   \n",
       "7     10317.000004738                  Caucasian    male         37    37   \n",
       "8     10317.000014608                  Caucasian  female         30    30   \n",
       "9     10317.000004734                  Caucasian  female         31    31   \n",
       "10    10317.000004737                  Caucasian    male         33    33   \n",
       "11    10317.000004736                  Caucasian  female         50    50   \n",
       "12    10317.000022638                  Caucasian    male         37    37   \n",
       "13    10317.000022636                  Caucasian  female         38    38   \n",
       "14    10317.000013332                  Caucasian  female         45    45   \n",
       "15    10317.000039907                  Caucasian  female         45    45   \n",
       "16    10317.000003492                  Caucasian    male         47    47   \n",
       "17    10317.000002773                  Caucasian  female         50    50   \n",
       "18    10317.000002774                  Caucasian    male         49    49   \n",
       "19    10317.000009507                  Caucasian  female         55    55   \n",
       "20    10317.000041935  Asian or Pacific Islander    male         36    36   \n",
       "21    10317.000030744                  Caucasian    male         23    23   \n",
       "22    10317.000013454                  Caucasian    male         49    49   \n",
       "23    10317.000014958                  Caucasian  female         54    54   \n",
       "24    10317.000014954                  Caucasian    male         34    34   \n",
       "25    10317.000010274                  Caucasian    male         55    55   \n",
       "26    10317.000014224                  Caucasian    male         38    38   \n",
       "27    10317.000023223                  Caucasian    male         46    46   \n",
       "28    10317.000015577                  Caucasian    male         53    53   \n",
       "29    10317.000011055                  Caucasian    male         47    47   \n",
       "...               ...                        ...     ...        ...   ...   \n",
       "1345  10317.000016725                   Hispanic    male         26    26   \n",
       "1346  10317.000007746                  Caucasian  female         36    36   \n",
       "1347  10317.000016166                  Caucasian    male         33    33   \n",
       "1348  10317.000009111                  Caucasian  female         33    33   \n",
       "1349  10317.000016727                   Hispanic    male         26    26   \n",
       "1350  10317.000043118                  Caucasian    male         50    50   \n",
       "1351  10317.000028723                  Caucasian    male         55    55   \n",
       "1352  10317.000002711  Asian or Pacific Islander  female         38    38   \n",
       "1353  10317.000002712  Asian or Pacific Islander    male         42    42   \n",
       "1354  10317.000004641                  Caucasian  female         39    39   \n",
       "1355  10317.000009533                  Caucasian  female         23    23   \n",
       "1356  10317.000011259                  Caucasian  female         44    44   \n",
       "1357  10317.000018035                  Caucasian  female         51    51   \n",
       "1358  10317.000018033                  Caucasian    male         31    31   \n",
       "1359  10317.000005878                  Caucasian    male         35    35   \n",
       "1360  10317.000011072                  Caucasian    male         48    48   \n",
       "1361  10317.000011073                  Caucasian    male         48    48   \n",
       "1362  10317.000011074                   Hispanic    male         44    44   \n",
       "1363  10317.000011077                  Caucasian  female         29    29   \n",
       "1364  10317.000011369                  Caucasian  female         43    43   \n",
       "1365  10317.000003432                  Caucasian    male         42    42   \n",
       "1366  10317.000003125                  Caucasian  female         34    34   \n",
       "1367  10317.000003439                  Caucasian  female         50    50   \n",
       "1368  10317.000013972  Asian or Pacific Islander    male         31    31   \n",
       "1369  10317.000036151                  Caucasian  female         52    52   \n",
       "1370  10317.000038424                  Caucasian  female         52    52   \n",
       "1371  10317.000012254                  Caucasian    male         54    54   \n",
       "1372  10317.000016313                  Caucasian  female         39    39   \n",
       "1373  10317.000016315                  Caucasian  female         40    40   \n",
       "1374  10317.000003196                  Caucasian  female         35    35   \n",
       "\n",
       "        bmi non_food_allergies_beestings tonsils_removed country_of_birth  \\\n",
       "0     27.47                        False             Yes    United States   \n",
       "1     25.54                        False              No    United States   \n",
       "2     20.70                        False             Yes    United States   \n",
       "3     21.05                        False             Yes    United States   \n",
       "4     35.17                        False             Yes    United States   \n",
       "5     38.38                        False             Yes    United States   \n",
       "6     19.83                        False             Yes    United States   \n",
       "7     21.46                        False             Yes    United States   \n",
       "8     17.62                        False              No    United States   \n",
       "9     23.38                        False              No    United States   \n",
       "10    19.27                        False             Yes    United States   \n",
       "11    24.80                        False              No          Romania   \n",
       "12    23.62                        False        Not sure    United States   \n",
       "13    11.46                        False             Yes    United States   \n",
       "14    33.56                        False              No    United States   \n",
       "15    20.34                        False              No    United States   \n",
       "16    20.99                        False             Yes    United States   \n",
       "17    23.15                        False              No          Denmark   \n",
       "18    24.16                        False              No          Denmark   \n",
       "19    30.49                        False              No    United States   \n",
       "20    20.53                        False              No    United States   \n",
       "21    27.26                        False              No    United States   \n",
       "22    26.64                        False              No    United States   \n",
       "23    24.25                        False             Yes    United States   \n",
       "24    19.92                        False              No    United States   \n",
       "25    22.04                        False              No    United States   \n",
       "26    23.48                        False              No    United States   \n",
       "27    33.72                        False              No    United States   \n",
       "28    22.86                        False              No    United States   \n",
       "29    25.25                        False              No    United States   \n",
       "...     ...                          ...             ...              ...   \n",
       "1345  21.98                        False              No            Spain   \n",
       "1346  19.71                        False              No    United States   \n",
       "1347  19.06                        False              No    United States   \n",
       "1348  24.75                        False              No    United States   \n",
       "1349  21.98                        False              No            Spain   \n",
       "1350  24.12                        False              No          Germany   \n",
       "1351  27.44                        False             Yes    United States   \n",
       "1352  20.90                        False              No    United States   \n",
       "1353  26.30                        False              No    United States   \n",
       "1354  23.88                        False              No    United States   \n",
       "1355  39.14                        False              No    United States   \n",
       "1356  21.45                        False             Yes    United States   \n",
       "1357  17.30                        False              No    United States   \n",
       "1358  22.10                        False              No    United States   \n",
       "1359  24.33                        False             Yes    United States   \n",
       "1360  24.67                        False              No    United States   \n",
       "1361  24.67                        False              No    United States   \n",
       "1362  25.80                        False              No    United States   \n",
       "1363  20.90                        False              No    United States   \n",
       "1364  19.13                        False              No           Brazil   \n",
       "1365  23.51                        False              No    United States   \n",
       "1366  26.45                        False              No    United States   \n",
       "1367  23.53                        False              No    United States   \n",
       "1368  21.15                        False              No   United Kingdom   \n",
       "1369  26.04                        False             Yes    United States   \n",
       "1370  29.05                        False              No    United States   \n",
       "1371  21.30                        False             Yes    United States   \n",
       "1372  23.53                        False              No    United States   \n",
       "1373  21.26                        False              No    United States   \n",
       "1374  22.21                        False              No    United States   \n",
       "\n",
       "     pets_other        ...                                  pku    state  \\\n",
       "0       no_data        ...         I do not have this condition       WV   \n",
       "1       no_data        ...         I do not have this condition       VA   \n",
       "2       no_data        ...         I do not have this condition       VA   \n",
       "3       no_data        ...         I do not have this condition       NC   \n",
       "4       no_data        ...         I do not have this condition       NJ   \n",
       "5       no_data        ...         I do not have this condition       TX   \n",
       "6       no_data        ...                              Unknown       OH   \n",
       "7       no_data        ...         I do not have this condition       TX   \n",
       "8       no_data        ...         I do not have this condition       IL   \n",
       "9       no_data        ...         I do not have this condition       GA   \n",
       "10      no_data        ...         I do not have this condition       IA   \n",
       "11      no_data        ...         I do not have this condition       IL   \n",
       "12      no_data        ...         I do not have this condition       AK   \n",
       "13      no_data        ...         I do not have this condition       WA   \n",
       "14      no_data        ...         I do not have this condition       MD   \n",
       "15        false        ...         I do not have this condition       MA   \n",
       "16      no_data        ...         I do not have this condition       NH   \n",
       "17      no_data        ...         I do not have this condition       CA   \n",
       "18      no_data        ...         I do not have this condition       CA   \n",
       "19      no_data        ...         I do not have this condition       NJ   \n",
       "20        false        ...         I do not have this condition       MA   \n",
       "21      Unknown        ...         I do not have this condition       MA   \n",
       "22      no_data        ...         I do not have this condition       CA   \n",
       "23      no_data        ...         I do not have this condition       CA   \n",
       "24      no_data        ...         I do not have this condition       NY   \n",
       "25      no_data        ...         I do not have this condition       CA   \n",
       "26      no_data        ...         I do not have this condition       OR   \n",
       "27      no_data        ...         I do not have this condition       WA   \n",
       "28      no_data        ...         I do not have this condition       CA   \n",
       "29      no_data        ...         I do not have this condition       NC   \n",
       "...         ...        ...                                  ...      ...   \n",
       "1345    no_data        ...         I do not have this condition       CO   \n",
       "1346    no_data        ...         I do not have this condition       UT   \n",
       "1347    no_data        ...         I do not have this condition       FL   \n",
       "1348    no_data        ...         I do not have this condition       NH   \n",
       "1349    no_data        ...         I do not have this condition       CO   \n",
       "1350      false        ...         I do not have this condition       OR   \n",
       "1351    no_data        ...         I do not have this condition       MI   \n",
       "1352    no_data        ...         I do not have this condition       CA   \n",
       "1353    no_data        ...         I do not have this condition       CA   \n",
       "1354    no_data        ...         I do not have this condition       FL   \n",
       "1355    no_data        ...         I do not have this condition       IL   \n",
       "1356    no_data        ...         I do not have this condition       ID   \n",
       "1357    no_data        ...         I do not have this condition       TX   \n",
       "1358    no_data        ...         I do not have this condition       SC   \n",
       "1359    no_data        ...         I do not have this condition  Unknown   \n",
       "1360    no_data        ...         I do not have this condition       WA   \n",
       "1361    no_data        ...         I do not have this condition       WA   \n",
       "1362    Unknown        ...         I do not have this condition       CA   \n",
       "1363    no_data        ...         I do not have this condition       ME   \n",
       "1364    no_data        ...         I do not have this condition       CA   \n",
       "1365    no_data        ...         I do not have this condition       CO   \n",
       "1366    no_data        ...         I do not have this condition       NC   \n",
       "1367    no_data        ...         I do not have this condition       TN   \n",
       "1368    no_data        ...         I do not have this condition       CO   \n",
       "1369    Unknown        ...         I do not have this condition       CA   \n",
       "1370    Unknown        ...         I do not have this condition       OR   \n",
       "1371    no_data        ...         I do not have this condition       CA   \n",
       "1372    no_data        ...         I do not have this condition       NY   \n",
       "1373    no_data        ...         I do not have this condition       WA   \n",
       "1374    no_data        ...         I do not have this condition       MN   \n",
       "\n",
       "     age_corrected mental_illness_type_unspecified simple_body_site  \\\n",
       "0               54                         no_data            FECAL   \n",
       "1               25                         no_data            FECAL   \n",
       "2               52                         no_data            FECAL   \n",
       "3               46                         no_data            FECAL   \n",
       "4               39                         no_data            FECAL   \n",
       "5               55                         no_data            FECAL   \n",
       "6               41                         no_data            FECAL   \n",
       "7               37                         no_data            FECAL   \n",
       "8               30                         no_data            FECAL   \n",
       "9               31                         no_data            FECAL   \n",
       "10              33                         no_data            FECAL   \n",
       "11              50                         no_data            FECAL   \n",
       "12              37                         no_data            FECAL   \n",
       "13              38                         no_data            FECAL   \n",
       "14              45                         no_data            FECAL   \n",
       "15              45                            true            FECAL   \n",
       "16              47                         no_data            FECAL   \n",
       "17              50                         no_data            FECAL   \n",
       "18              49                         no_data            FECAL   \n",
       "19              55                         no_data            FECAL   \n",
       "20              36                            true            FECAL   \n",
       "21              23                         Unknown            FECAL   \n",
       "22              49                         no_data            FECAL   \n",
       "23              54                         no_data            FECAL   \n",
       "24              34                         no_data            FECAL   \n",
       "25              55                         no_data            FECAL   \n",
       "26              38                         no_data            FECAL   \n",
       "27              46                         no_data            FECAL   \n",
       "28              53                         no_data            FECAL   \n",
       "29              47                         no_data            FECAL   \n",
       "...            ...                             ...              ...   \n",
       "1345            26                         no_data            FECAL   \n",
       "1346            36                         no_data            FECAL   \n",
       "1347            33                         no_data            FECAL   \n",
       "1348            33                         no_data            FECAL   \n",
       "1349            26                         no_data            FECAL   \n",
       "1350            50                            true            FECAL   \n",
       "1351            55                         no_data            FECAL   \n",
       "1352            38                         no_data            FECAL   \n",
       "1353            42                         no_data            FECAL   \n",
       "1354            39                         no_data            FECAL   \n",
       "1355            23                         no_data            FECAL   \n",
       "1356            44                         no_data            FECAL   \n",
       "1357            51                         no_data            FECAL   \n",
       "1358            31                         no_data            FECAL   \n",
       "1359            35                         no_data            FECAL   \n",
       "1360            48                         no_data            FECAL   \n",
       "1361            48                         no_data            FECAL   \n",
       "1362            44                         Unknown            FECAL   \n",
       "1363            29                         no_data            FECAL   \n",
       "1364            43                         no_data            FECAL   \n",
       "1365            42                         no_data            FECAL   \n",
       "1366            34                         no_data            FECAL   \n",
       "1367            50                         no_data            FECAL   \n",
       "1368            31                         no_data            FECAL   \n",
       "1369            52                         Unknown            FECAL   \n",
       "1370            52                         Unknown            FECAL   \n",
       "1371            54                         no_data            FECAL   \n",
       "1372            39                         no_data            FECAL   \n",
       "1373            40                         no_data            FECAL   \n",
       "1374            35                         no_data            FECAL   \n",
       "\n",
       "     title_acronym title_body_site hmp_site age_group_custom age_group_custom2  \n",
       "0              AGP       AGP-FECAL    FECAL        Age_40_55         Age_50_55  \n",
       "1              AGP       AGP-FECAL    FECAL        Age_18_29         Age_25_29  \n",
       "2              AGP       AGP-FECAL    FECAL        Age_40_55         Age_50_55  \n",
       "3              AGP       AGP-FECAL    FECAL        Age_40_55         Age_45_49  \n",
       "4              AGP       AGP-FECAL    FECAL        Age_30_39         Age_35_39  \n",
       "5              AGP       AGP-FECAL    FECAL        Age_40_55         Age_50_55  \n",
       "6              AGP       AGP-FECAL    FECAL        Age_40_55         Age_40_44  \n",
       "7              AGP       AGP-FECAL    FECAL        Age_30_39         Age_35_39  \n",
       "8              AGP       AGP-FECAL    FECAL        Age_30_39         Age_30_34  \n",
       "9              AGP       AGP-FECAL    FECAL        Age_30_39         Age_30_34  \n",
       "10             AGP       AGP-FECAL    FECAL        Age_30_39         Age_30_34  \n",
       "11             AGP       AGP-FECAL    FECAL        Age_40_55         Age_50_55  \n",
       "12             AGP       AGP-FECAL    FECAL        Age_30_39         Age_35_39  \n",
       "13             AGP       AGP-FECAL    FECAL        Age_30_39         Age_35_39  \n",
       "14             AGP       AGP-FECAL    FECAL        Age_40_55         Age_45_49  \n",
       "15             AGP       AGP-FECAL    FECAL        Age_40_55         Age_45_49  \n",
       "16             AGP       AGP-FECAL    FECAL        Age_40_55         Age_45_49  \n",
       "17             AGP       AGP-FECAL    FECAL        Age_40_55         Age_50_55  \n",
       "18             AGP       AGP-FECAL    FECAL        Age_40_55         Age_45_49  \n",
       "19             AGP       AGP-FECAL    FECAL        Age_40_55         Age_50_55  \n",
       "20             AGP       AGP-FECAL    FECAL        Age_30_39         Age_35_39  \n",
       "21             AGP       AGP-FECAL    FECAL        Age_18_29         Age_18_24  \n",
       "22             AGP       AGP-FECAL    FECAL        Age_40_55         Age_45_49  \n",
       "23             AGP       AGP-FECAL    FECAL        Age_40_55         Age_50_55  \n",
       "24             AGP       AGP-FECAL    FECAL        Age_30_39         Age_30_34  \n",
       "25             AGP       AGP-FECAL    FECAL        Age_40_55         Age_50_55  \n",
       "26             AGP       AGP-FECAL    FECAL        Age_30_39         Age_35_39  \n",
       "27             AGP       AGP-FECAL    FECAL        Age_40_55         Age_45_49  \n",
       "28             AGP       AGP-FECAL    FECAL        Age_40_55         Age_50_55  \n",
       "29             AGP       AGP-FECAL    FECAL        Age_40_55         Age_45_49  \n",
       "...            ...             ...      ...              ...               ...  \n",
       "1345           AGP       AGP-FECAL    FECAL        Age_18_29         Age_25_29  \n",
       "1346           AGP       AGP-FECAL    FECAL        Age_30_39         Age_35_39  \n",
       "1347           AGP       AGP-FECAL    FECAL        Age_30_39         Age_30_34  \n",
       "1348           AGP       AGP-FECAL    FECAL        Age_30_39         Age_30_34  \n",
       "1349           AGP       AGP-FECAL    FECAL        Age_18_29         Age_25_29  \n",
       "1350           AGP       AGP-FECAL    FECAL        Age_40_55         Age_50_55  \n",
       "1351           AGP       AGP-FECAL    FECAL        Age_40_55         Age_50_55  \n",
       "1352           AGP       AGP-FECAL    FECAL        Age_30_39         Age_35_39  \n",
       "1353           AGP       AGP-FECAL    FECAL        Age_40_55         Age_40_44  \n",
       "1354           AGP       AGP-FECAL    FECAL        Age_30_39         Age_35_39  \n",
       "1355           AGP       AGP-FECAL    FECAL        Age_18_29         Age_18_24  \n",
       "1356           AGP       AGP-FECAL    FECAL        Age_40_55         Age_40_44  \n",
       "1357           AGP       AGP-FECAL    FECAL        Age_40_55         Age_50_55  \n",
       "1358           AGP       AGP-FECAL    FECAL        Age_30_39         Age_30_34  \n",
       "1359           AGP       AGP-FECAL    FECAL        Age_30_39         Age_35_39  \n",
       "1360           AGP       AGP-FECAL    FECAL        Age_40_55         Age_45_49  \n",
       "1361           AGP       AGP-FECAL    FECAL        Age_40_55         Age_45_49  \n",
       "1362           AGP       AGP-FECAL    FECAL        Age_40_55         Age_40_44  \n",
       "1363           AGP       AGP-FECAL    FECAL        Age_18_29         Age_25_29  \n",
       "1364           AGP       AGP-FECAL    FECAL        Age_40_55         Age_40_44  \n",
       "1365           AGP       AGP-FECAL    FECAL        Age_40_55         Age_40_44  \n",
       "1366           AGP       AGP-FECAL    FECAL        Age_30_39         Age_30_34  \n",
       "1367           AGP       AGP-FECAL    FECAL        Age_40_55         Age_50_55  \n",
       "1368           AGP       AGP-FECAL    FECAL        Age_30_39         Age_30_34  \n",
       "1369           AGP       AGP-FECAL    FECAL        Age_40_55         Age_50_55  \n",
       "1370           AGP       AGP-FECAL    FECAL        Age_40_55         Age_50_55  \n",
       "1371           AGP       AGP-FECAL    FECAL        Age_40_55         Age_50_55  \n",
       "1372           AGP       AGP-FECAL    FECAL        Age_30_39         Age_35_39  \n",
       "1373           AGP       AGP-FECAL    FECAL        Age_40_55         Age_40_44  \n",
       "1374           AGP       AGP-FECAL    FECAL        Age_30_39         Age_35_39  \n",
       "\n",
       "[1375 rows x 197 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; color:black;\">------------------------------------------------------------------------------</h1>\n",
    "<h2 style=\"text-align:center; color:blue;\"> - Beta Diversity - Intra/Inter Comparison - </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align:center; color:black;\"> - beta_intra_inter(consensusDMIn, mapDfIn, mapCatIn, printOut=False): Compare beta diversity of every intra to inter category combination - </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### BETA DIVERSITY - INTRA/INTER DISTANCE COMPARISONS ###\n",
    "# Calculate average distance from each sample to all individuals of other groups.\n",
    "# input: consensus distance matrix, map, mapping file categorical variable\n",
    "# output: pandas dataframes of p-values and distances\n",
    "def beta_intra_inter(consensusDMIn, mapDfIn, mapCatIn, meanBool = True, printOut=False):\n",
    "    \n",
    "    \n",
    "    # CONVERT TO DATAFRAME & GET INDEX #\n",
    "    consDF = consensusDMIn.to_data_frame(); consIndex = consDF.index\n",
    "    # ADD CATEGORY TO THE DISTANCE DATAFRAME #\n",
    "    consDF = pd.concat([consDF, mapDfIn[mapCatIn]],axis=1)\n",
    "    # ADD INDEX AS HASH TO DATAFRAME #\n",
    "    hashindexOut = []\n",
    "    for curIdx in consDF.index: hashindexOut.append(hash(curIdx))\n",
    "    consDF['hashIndex'] = hashindexOut\n",
    "    # GET UNIQUE GROUPS IN MAPPING CATEGORY #\n",
    "    consCatGroups = list(mapDf[mapCatIn].unique())\n",
    "    # MAKE DATAFRAME TO STORE RESULTS #\n",
    "    betaIIDF = pd.DataFrame(data=None, index=consIndex, columns=(['sample', 'group']+consCatGroups))\n",
    "    ### FOR EACH SAMPLE ###\n",
    "    for sampleCycle in consIndex:\n",
    "        ### STORE DISTANCES \n",
    "        groupDists = []\n",
    "        ### GET MAPPING GROUP FOR SAMPLE ###\n",
    "        curSampleGroup = mapDfIn[mapDfIn.index == sampleCycle][mapCatIn]\n",
    "        \n",
    "        ### STORE SAMPLE AND GROUP ###\n",
    "        betaIIDF.loc[sampleCycle]['sample'] = sampleCycle\n",
    "        betaIIDF.loc[sampleCycle]['group'] = curSampleGroup[0]\n",
    "\n",
    "        ### FOR EACH GROUP ###\n",
    "        for groupCycle in consCatGroups:\n",
    "\n",
    "            ### GET DISTANCE MATRIX FOR ONLY GROUP ###\n",
    "            curDM = consDF[(consDF[mapCatIn] == groupCycle)].copy()\n",
    "            \n",
    "            ### STORE LIST OF DISTANCES ###\n",
    "            \n",
    "            if meanBool == True: ### STORE AVERAGE SAMPLE DISTANCE TO SAMPLES IN GROUP ###\n",
    "                ### REMOVE DIAGONAL DISTANCES == 0 ###\n",
    "                #curDM = consDF[(consDF[mapCatIn] == groupCycle)].copy()\n",
    "                curDM = curDM[curDM['hashIndex'] != hash(sampleCycle)][sampleCycle]\n",
    "                betaIIDF.loc[sampleCycle][groupCycle] = curDM.mean() # STORE #\n",
    "            \n",
    "            else: ### STORE ALL DISTANCES TO SAMPLES IN OTHER GROUP ###\n",
    "                ### REMOVE DIAGONAL DISTANCES AND DUPLICATE SAMPLING ###\n",
    "                curDM = curDM[curDM['hashIndex'] > hash(sampleCycle)][sampleCycle]\n",
    "                betaIIDF.loc[sampleCycle][groupCycle] = curDM.values # STORE #\n",
    "        \n",
    "    # MAKE DATAFRAME TO STORE RESULTS #\n",
    "    betaDFOut = pd.DataFrame(data=None, index=None, columns=(['type','g0','g0dist','g1','g1dist','p-raw'])); idx=0\n",
    "    \n",
    "    \n",
    "    ### FOR EACH COMBINATION OF GROUPS ###\n",
    "    if printOut == True: print(\"- INTRA GROUP COMPARISONS - \"); print()\n",
    "    for combGroups in itertools.combinations(consCatGroups,2):\n",
    "        \n",
    "        ### GET FIRST GROUP AVERAGE DISTANCE ####\n",
    "        if meanBool == True: intraDist0In = betaIIDF[betaIIDF['group'] == combGroups[0]][combGroups[0]].mean()\n",
    "        else: intraDist0In = np.mean([item for sublist in betaIIDF[betaIIDF['group'] == combGroups[0]][combGroups[0]].values.tolist() for item in sublist])   \n",
    "        if printOut == True:print(\" - Average Intra Distance \"+ combGroups[0]+\" : \"+ str(intraDist0In))\n",
    "            \n",
    "        ### GET SECOND GROUP AVERAGE DISTANCE ###\n",
    "        if meanBool == True: intraDist1In = betaIIDF[betaIIDF['group'] == combGroups[1]][combGroups[1]].mean()\n",
    "        else: intraDist1In = np.mean([item for sublist in betaIIDF[betaIIDF['group'] == combGroups[1]][combGroups[1]].values.tolist() for item in sublist])\n",
    "        if printOut == True:print(\" - Average Intra Distance \"+ combGroups[1]+\" : \"+ str(intraDist1In))\n",
    "            \n",
    "        ### PERFORM MANN-WHITNEY-U TEST FIRST-SECOND ###\n",
    "        if meanBool == True: resIn, tIn, pIn = stats_mannwhitney(betaIIDF[betaIIDF['group'] == combGroups[0]][combGroups[0]], betaIIDF[betaIIDF['group'] == combGroups[1]][combGroups[1]], l1Name=str(\"Intra-\"+combGroups[0]), l2Name=str(\"Intra-\"+combGroups[1]), bonferroniComparisons=len(list(itertools.combinations(consCatGroups,2))))\n",
    "        else: resIn, tIn, pIn = stats_mannwhitney([item for sublist in betaIIDF[betaIIDF['group'] == combGroups[0]][combGroups[0]].values.tolist() for item in sublist],[item for sublist in betaIIDF[betaIIDF['group'] == combGroups[1]][combGroups[1]].values.tolist() for item in sublist],l1Name=str(\"Intra-\"+combGroups[0]),l2Name=str(\"Intra-\"+combGroups[1]),bonferroniComparisons=len(list(itertools.combinations(consCatGroups,2))))\n",
    "        \n",
    "        ### STORE RESULTS IN DATAFRAME ###\n",
    "        betaDFOut.loc[idx] = ['Intra-Intra',combGroups[0],intraDist0In,combGroups[1],intraDist1In, pIn];idx+=1\n",
    "        if printOut == True: list_print(resIn); print()\n",
    "\n",
    "    \n",
    "    ### FOR EACH COMBINATION OF GROUPS ###\n",
    "    if printOut == True:print(\"- INTER GROUP COMPARISONS - \"); print()\n",
    "    for combGroups in itertools.permutations(consCatGroups,2):\n",
    "       \n",
    "        ### GET AVERAGE INTER DISTANCE ###\n",
    "        if meanBool == True: interDist0In = betaIIDF[betaIIDF['group'] == combGroups[0]][combGroups[1]].mean()\n",
    "        else: interDist0In = np.mean([item for sublist in betaIIDF[betaIIDF['group'] == combGroups[0]][combGroups[1]].values.tolist() for item in sublist])\n",
    "        if printOut == True:print(\" - Average Inter Distance \"+ combGroups[0]+\" to \"+combGroups[1]+\": \"+ str(interDist0In))\n",
    "            \n",
    "        ### GET AVERAGE INTRA DISTANCE ###\n",
    "        if meanBool == True: intraDist1In = betaIIDF[betaIIDF['group'] == combGroups[0]][combGroups[0]].mean()\n",
    "        else: intraDist1In = np.mean([item for sublist in betaIIDF[betaIIDF['group'] == combGroups[0]][combGroups[0]].values.tolist() for item in sublist])\n",
    "        if printOut == True:print(\" - Average Intra Distance \"+ combGroups[0]+\": \"+ str(intraDist1In))\n",
    "            \n",
    "        ### PERFORM MANN-WHITNEY-U TEST FIRST-SECOND ###\n",
    "        if meanBool == True: resIn, tIn, pIn = stats_mannwhitney(betaIIDF[betaIIDF['group'] == combGroups[0]][combGroups[0]], betaIIDF[betaIIDF['group'] == combGroups[0]][combGroups[1]], l1Name=str(\"Intra-\"+combGroups[0]), l2Name=str(\"Inter-\"+combGroups[0]+\"-\"+combGroups[1]), bonferroniComparisons=len(list(itertools.permutations(consCatGroups,2))))\n",
    "        else: resIn, tIn, pIn = stats_mannwhitney([item for sublist in betaIIDF[betaIIDF['group'] == combGroups[0]][combGroups[0]].values.tolist() for item in sublist],[item for sublist in betaIIDF[betaIIDF['group'] == combGroups[0]][combGroups[1]].values.tolist() for item in sublist],l1Name=str(\"Intra-\"+combGroups[0]),l2Name=str(\"Inter-\"+combGroups[0]+\"-\"+combGroups[1]),bonferroniComparisons=len(list(itertools.permutations(consCatGroups,2))))\n",
    "        \n",
    "        ### STORE RESULTS IN DATAFRAME ###\n",
    "        betaDFOut.loc[idx] = ['Intra-Inter',combGroups[0],intraDist1In,combGroups[1],interDist0In, pIn];idx+=1\n",
    "        if printOut == True: list_print(resIn); print()\n",
    "    \n",
    "    \n",
    "    betaDFOut2 = pd.concat([betaDFOut,stats_pcorrect(betaDFOut['p-raw'],alphaIn=0.05)[['p-fdr','p-bonferroni']]],axis=1) \n",
    "    \n",
    "    ### RETURN DATAFRAMES OF P-VALUES AND DISTANCES ###\n",
    "    return betaDFOut2, betaIIDF\n",
    "\n",
    "#consensusDM = i[2]\n",
    "#betaIntraInterDist  = beta_intra_inter(consensusDM, mapDf, 'race', meanBool=False, printOut=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align:center; color:black;\"> - Perform beta_intra_inter() on mean sample distances across depths and metrics and subsamples - </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##### INTRA/INTER ON MEAN SAMPLE DISTANCES #####\n",
    "\n",
    "### CREATE CONSENSUS DM ITERABLE ###\n",
    "dmCycle = dm_iter_consensus(dirPath='test_data/ag_analysis/',betaMetricsIn=['bray_curtis','weighted_unifrac','binary_jaccard','unweighted_unifrac'],betaDepthsIn=[1000,10000])\n",
    "subSampleNumber = 10\n",
    "minSamples = [10,15,20,25,30,35,40,45,50,75,100,250,500,1000,2000] ### MAXIMUM NUMBER OF SAMPLES FOR EACH RACE ###\n",
    "\n",
    "curCols = ['type','g0','g1','depth','metric']+minSamples\n",
    "outDM = DF(None, columnsIn=curCols)\n",
    "outDict = {}\n",
    "outDict2 = {}\n",
    "mapCat = 'race'\n",
    "outPath = 'test_data/ag_analysis/4_1_beta_intraInter/'\n",
    "if not os.path.isdir(outPath): os.makedirs(outPath)\n",
    "    \n",
    "### FOR EACH CONSENSUS DM ###\n",
    "for idx, i in enumerate(dmCycle):\n",
    "    print(i[0],i[1])\n",
    "    \n",
    "    ### IMPORT APPROPRIATE MAP ###\n",
    "    if i[0] == 1000: mapPathIn = 'test_data/ag_analysis/1_1_qc_1000_map.txt'\n",
    "    else: mapPathIn = 'test_data/ag_analysis/1_1_qc_10000_map.txt'\n",
    "    mapDf = DF((mapPathIn))\n",
    "    mapDf = mapDf.set_i(colNameOrList=[\"#SampleID\"], append=False)\n",
    "    mapDf = mapDf.df\n",
    "\n",
    "    ### FOR EACH SUBSAMPLE LEVEL ###\n",
    "    for subLevel in minSamples:\n",
    "        dfAvg = []\n",
    "        print('   - Selecting Subset of '+str(subLevel)+' samples -')\n",
    "\n",
    "        ### FOR THE NUMBER OF SUBSAMPLES ###\n",
    "        for numSubSample in np.arange(subSampleNumber):\n",
    "            \n",
    "            ### FOR EACH GROUP EXTRACT SAMPLES ###\n",
    "            incSamples = []\n",
    "            for eIn in mapDf[mapCat].unique():\n",
    "                ### IF THE NUMBER OF SAMPLES IS LARGER THAN THE CURRENT SUBSAMPLE \n",
    "                if subLevel < mapDf[mapDf[mapCat] == eIn].shape[0]: # THEN SELECT sublevel # OF SAMPLES\n",
    "                    incSamples.extend(mapDf[mapDf[mapCat] == eIn].sample(n=subLevel, axis=0, replace=False).index)\n",
    "                else: incSamples.extend(mapDf[mapDf[mapCat] == eIn].index)\n",
    "\n",
    "            ### TRIM DISTANCE MATRIX ###\n",
    "            curDMFiltered = i[2].filter(incSamples)\n",
    "            \n",
    "            ### PERFORM BETA ###\n",
    "            iiInHere = beta_intra_inter(curDMFiltered, mapDf, mapCat, printOut=False)[0]\n",
    "            \n",
    "            ### FOR EACH COMPARISON GET TYPE AND DISTANCES ###\n",
    "            for curIdx in iiInHere.index:\n",
    "                typeIn = str(iiInHere.loc[curIdx]['type'])\n",
    "                g0In = str(iiInHere.loc[curIdx]['g0'])\n",
    "                g1In = str(iiInHere.loc[curIdx]['g1'])\n",
    "            \n",
    "                ### ADD DATA TO DICTIONARY ###\n",
    "                if typeIn not in outDict.keys(): outDict[typeIn]={}\n",
    "                if subLevel not in outDict[typeIn].keys(): outDict[typeIn][subLevel] = {}\n",
    "                if g0In not in outDict[typeIn][subLevel].keys(): outDict[typeIn][subLevel][g0In] = {}\n",
    "                if g1In not in outDict[typeIn][subLevel][g0In].keys(): outDict[typeIn][subLevel][g0In][g1In] = {}\n",
    "                if i[1] not in outDict[typeIn][subLevel][g0In][g1In].keys(): outDict[typeIn][subLevel][g0In][g1In][i[1]] = {}\n",
    "                if i[0] not in outDict[typeIn][subLevel][g0In][g1In][i[1]].keys(): outDict[typeIn][subLevel][g0In][g1In][i[1]][i[0]] = []\n",
    "   \n",
    "                ### STORE LIST OF P-VALUES\n",
    "                outDict[typeIn][subLevel][g0In][g1In][i[1]][i[0]].append(iiInHere.loc[curIdx]['p-raw'])\n",
    "                ### SAVE RESULTS FILE ###\n",
    "                iiInHere.to_csv(outPath+'mean_'+[typeIn][0]+'_'+str(i[0])+'_'+[g0In][0].replace(' ','_')+'_'+[g1In][0].replace(' ','_')+'_'+[i[1]][0]+'_'+str([subLevel][0])+'.txt',sep='\\t')\n",
    "                    \n",
    "### MAKE DATAFRAME FROM DICTIONARY ###\n",
    "printOut = pd.DataFrame.from_dict({(i,j,k,l,m,n): outDict[i][j][k][l][m][n] \n",
    "                            for i in outDict.keys() \n",
    "                            for j in outDict[i].keys()\n",
    "                            for k in outDict[i][j].keys()\n",
    "                            for l in outDict[i][j][k].keys()\n",
    "                            for m in outDict[i][j][k][l].keys()\n",
    "                            for n in outDict[i][j][k][l][m].keys()}, orient='columns')\n",
    "\n",
    "### SAVE OUTPUT TO TSV FILE ###\n",
    "printOut = DF(printOut)\n",
    "printOut.out_tsv(outPath=\"test_data/ag_analysis/4_1_beta_intraInter_mean_p_vals.txt\") \n",
    "printEdit = printOut.copy()\n",
    "printEdit.df.loc['Mean-p'] = printOut.df.mean(axis=0)\n",
    "printEdit.df.loc['Median-p'] = printOut.df.median(axis=0)\n",
    "printEdit.df.loc['Std-p'] = printOut.df.std(axis=0)\n",
    "printEdit.df = printEdit.df.drop(printEdit.index[np.arange(10)])\n",
    "printEdit.out_tsv(outPath=\"test_data/ag_analysis/4_1_beta_intraInter_mean_p_stats.txt\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align:center; color:black;\"> - Perform beta_intra_inter() on all sample distances across depths and metrics and subsamples - </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##### INTRA/INTER ON ALL PAIRWISE DISTANCES #####\n",
    "### CREATE CONSENSUS DM ITERABLE ###\n",
    "dmCycle = dm_iter_consensus(dirPath='test_data/ag_analysis/',betaMetricsIn=['bray_curtis','weighted_unifrac','binary_jaccard','unweighted_unifrac'],betaDepthsIn=[1000,10000])\n",
    "subSampleNumber = 10\n",
    "minSamples = [10,15,20,25,30,35,40,45,50,75,100,250,500,1000,2000] ### MAXIMUM NUMBER OF SAMPLES FOR EACH RACE ###\n",
    "\n",
    "curCols = ['type','g0','g1','depth','metric']+minSamples\n",
    "outDM = DF(None, columnsIn=curCols)\n",
    "outDict = {}\n",
    "outDict2 = {}\n",
    "mapCat = 'race'\n",
    "outPath = 'test_data/ag_analysis/4_1_beta_intraInter/'\n",
    "if not os.path.isdir(outPath): os.makedirs(outPath)\n",
    "    \n",
    "### FOR EACH CONSENSUS DM ###\n",
    "for idx, i in enumerate(dmCycle):\n",
    "    print(i[0],i[1])\n",
    "    \n",
    "    ### IMPORT APPROPRIATE MAP ###\n",
    "    if i[0] == 1000: mapPathIn = 'test_data/ag_analysis/1_1_qc_1000_map.txt'\n",
    "    else: mapPathIn = 'test_data/ag_analysis/1_1_qc_10000_map.txt'\n",
    "    mapDf = DF((mapPathIn))\n",
    "    mapDf = mapDf.set_i(colNameOrList=[\"#SampleID\"], append=False)\n",
    "    mapDf = mapDf.df\n",
    "\n",
    "    ### FOR EACH SUBSAMPLE LEVEL ###\n",
    "    for subLevel in minSamples:\n",
    "        dfAvg = []\n",
    "        print('   - Selecting Subset of '+str(subLevel)+' samples -')\n",
    "\n",
    "        ### FOR THE NUMBER OF SUBSAMPLES ###\n",
    "        for numSubSample in np.arange(subSampleNumber):\n",
    "            \n",
    "            ### FOR EACH GROUP EXTRACT SAMPLES ###\n",
    "            incSamples = []\n",
    "            for eIn in mapDf[mapCat].unique():\n",
    "                ### IF THE NUMBER OF SAMPLES IS LARGER THAN THE CURRENT SUBSAMPLE \n",
    "                if subLevel < mapDf[mapDf[mapCat] == eIn].shape[0]: # THEN SELECT sublevel # OF SAMPLES\n",
    "                    incSamples.extend(mapDf[mapDf[mapCat] == eIn].sample(n=subLevel, axis=0, replace=False).index)\n",
    "                else: incSamples.extend(mapDf[mapDf[mapCat] == eIn].index)\n",
    "\n",
    "            ### TRIM DISTANCE MATRIX ###\n",
    "            curDMFiltered = i[2].filter(incSamples)\n",
    "            \n",
    "            ### PERFORM BETA ###\n",
    "            iiInHere = beta_intra_inter(curDMFiltered, mapDf, mapCat, meanBool=False, printOut=False)[0]\n",
    "            \n",
    "            ### FOR EACH COMPARISON GET TYPE AND DISTANCES ###\n",
    "            for curIdx in iiInHere.index:\n",
    "                typeIn = str(iiInHere.loc[curIdx]['type'])\n",
    "                g0In = str(iiInHere.loc[curIdx]['g0'])\n",
    "                g1In = str(iiInHere.loc[curIdx]['g1'])\n",
    "            \n",
    "                ### ADD DATA TO DICTIONARY ###\n",
    "                if typeIn not in outDict.keys(): outDict[typeIn]={}\n",
    "                if subLevel not in outDict[typeIn].keys(): outDict[typeIn][subLevel] = {}\n",
    "                if g0In not in outDict[typeIn][subLevel].keys(): outDict[typeIn][subLevel][g0In] = {}\n",
    "                if g1In not in outDict[typeIn][subLevel][g0In].keys(): outDict[typeIn][subLevel][g0In][g1In] = {}\n",
    "                if i[1] not in outDict[typeIn][subLevel][g0In][g1In].keys(): outDict[typeIn][subLevel][g0In][g1In][i[1]] = {}\n",
    "                if i[0] not in outDict[typeIn][subLevel][g0In][g1In][i[1]].keys(): outDict[typeIn][subLevel][g0In][g1In][i[1]][i[0]] = []\n",
    "   \n",
    "                ### STORE LIST OF P-VALUES\n",
    "                outDict[typeIn][subLevel][g0In][g1In][i[1]][i[0]].append(iiInHere.loc[curIdx]['p-raw'])\n",
    "                ### SAVE RESULTS FILE ###\n",
    "                iiInHere.to_csv(outPath+'nonmean_'+[typeIn][0]+'_'+str(i[0])+'_'+[g0In][0].replace(' ','_')+'_'+[g1In][0].replace(' ','_')+'_'+[i[1]][0]+'_'+str([subLevel][0])+'_nonmean.txt',sep='\\t')\n",
    "                    \n",
    "### MAKE DATAFRAME FROM DICTIONARY ###\n",
    "printOut = pd.DataFrame.from_dict({(i,j,k,l,m,n): outDict[i][j][k][l][m][n] \n",
    "                            for i in outDict.keys() \n",
    "                            for j in outDict[i].keys()\n",
    "                            for k in outDict[i][j].keys()\n",
    "                            for l in outDict[i][j][k].keys()\n",
    "                            for m in outDict[i][j][k][l].keys()\n",
    "                            for n in outDict[i][j][k][l][m].keys()}, orient='columns')\n",
    "\n",
    "### SAVE OUTPUT TO TSV FILE ###\n",
    "printOut = DF(printOut)\n",
    "printOut.out_tsv(outPath=\"test_data/ag_analysis/4_1_beta_intraInter_nonmean_p_vals.txt\") \n",
    "printEdit = printOut.copy()\n",
    "printEdit.df.loc['Mean-p'] = printOut.df.mean(axis=0)\n",
    "printEdit.df.loc['Median-p'] = printOut.df.median(axis=0)\n",
    "printEdit.df.loc['Std-p'] = printOut.df.std(axis=0)\n",
    "printEdit.df = printEdit.df.drop(printEdit.index[np.arange(10)])\n",
    "printEdit.out_tsv(outPath=\"test_data/ag_analysis/4_1_beta_intraInter_nonmean_p_stats.txt\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#kf = sklearn.model_selection.KFold(n_splits=3, shuffle=False, random_state=54321)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; color:black;\">------------------------------------------------------------------------------</h1>\n",
    "<h2 style=\"text-align:center; color:blue;\"> - BioEnv - </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CORRELATION BETWEEN DISTANCE MATRIX AND COVARIATE DATAFRAME FROM REGRESSION STYLE EQUATION\n",
    "# TEST MODIFIED TO MEASURE CORRELATION OF ONLY SPECIFIED EQUATION, NOT ALL\n",
    "# POSSIBLE SUBSETS OF VARIABLES\n",
    "# ----------------------------------------------------------------------------\n",
    "# Copyright (c) 2013--, scikit-bio development team.\n",
    "#\n",
    "# Distributed under the terms of the Modified BSD License.\n",
    "#\n",
    "# The full license is in the file COPYING.txt, distributed with this software.\n",
    "# ----------------------------------------------------------------------------\n",
    "def beta_bioenv(dmIn, mapDfIn, regEquation=\"[ANY CONTINUOUS COLUMN] ~ age + sex:race + bmi\", columns=None):\n",
    "    ### EXTRACT REGRESSION EQUATION FROM COVARIATES DATAFRAME ###\n",
    "    y, mapDfIn = patsy.dmatrices(regEquation, mapDfIn, return_type='dataframe')\n",
    "    ### MAKE SURE DISTACE MATRIX IS OBJECT ###\n",
    "    if not isinstance(dmIn, DistanceMatrix): raise TypeError(\"Must provide a DistanceMatrix as input.\")\n",
    "    ### MAKE SURE MAP IS PANDAS DF ###\n",
    "    if not isinstance(mapDfIn, pd.DataFrame): raise TypeError(\"Must provide a pandas.DataFrame as input.\")  \n",
    "    ### IF NO LIST OF COLUMNS TO USE THEN USE ALL ###\n",
    "    columns = mapDfIn.columns.values.tolist()\n",
    "    ### CHECK FOR DUPLICATE COLUMNS ###\n",
    "    if len(set(columns)) != len(columns): raise ValueError(\"Duplicate column names are not supported.\")\n",
    "    ### CHECK TO MAKE SURE AT LEAST ON COLUMN FACTOR ###\n",
    "    if len(columns) < 1: raise ValueError(\"Must provide at least one column.\")\n",
    "    ### MAKE SURE COLUMNS ARE IN DATAFRAME ###\n",
    "    for column in columns:\n",
    "        if column not in mapDfIn: raise ValueError(\"Column '%s' not in data frame.\" % column)\n",
    "    ### ORDER COVARIATES TO SAMPLES IN DISTANCE MATRIX ###\n",
    "    variablesDF = mapDfIn.loc[dmIn.ids, columns]\n",
    "    ### CHECK FOR MISSING IDS IN DF FROM DM ###\n",
    "    if variablesDF.isnull().any().any(): raise ValueError(\"One or more IDs in the distance matrix are not in the data frame, or there is missing data in the data frame.\")\n",
    "    ### CHECK ALL COVARIATES ARE CASTABLE AS FLOATS ##\n",
    "    try: vars_df = vars_df.astype(float)\n",
    "    except ValueError: raise TypeError(\"All specified columns in the data frame must be numeric.\")\n",
    "    ##### CENTER DATAFRAME AROUND MEAN OF EACH COVARIATE AND COMPUTE AS STD #####\n",
    "    # Modified from http://stackoverflow.com/a/18005745\n",
    "    dfCenterScale = variablesDF.copy()\n",
    "    dfCenterScale -= dfCenterScale.mean()\n",
    "    dfCenterScale /= dfCenterScale.std()\n",
    "    ### MAKE SURE COULD BE CENTERED AROUND STD ###\n",
    "    if dfCenterScale.isnull().any().any(): raise ValueError(\"Column(s) in the data frame could not be scaled, likely because the column(s) had no variance.\")\n",
    "    ### CAST AS ARRAY ###\n",
    "    dfCovariatesArray = dfCenterScale.values\n",
    "    ##### FLATTEN THE DISTANCE MATRIX #####\n",
    "    dmInFlattened = dmIn.condensed_form()\n",
    "    ### COMPUTE PREDICTOR DISTANCE MATRIX ###\n",
    "    dfEuclidianDistance = sp.spatial.distance.pdist(dfCovariatesArray,metric='euclidean')\n",
    "    ### CALCULATE CORRELATION BETWEEN DISTANCE MATRIX AND COVARIATE DISTANCES ###\n",
    "    rhoOut = sp.stats.spearmanr(dmInFlattened, dfEuclidianDistance)[0]\n",
    "    return rhoOut\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; color:black;\">------------------------------------------------------------------------------</h1>\n",
    "<h2 style=\"text-align:center; color:blue;\"> - PCoA - </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pcoa(btIn, mapDfIn, regEquation):\n",
    "    pcoaBiom = sk.stats.ordination.pcoa(dmIn, mapDfIn)\n",
    "    Emperor(pcoaBiom, mapDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; color:black;\">------------------------------------------------------------------------------</h1>\n",
    "<h2 style=\"text-align:center; color:blue;\"> - CCA - </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cca(btIn, mapDfIn, regEquation):\n",
    "    ### GET DATAFRAME OF RELATIVE BIOM TABLE COUNTS ###\n",
    "    taIn = table_dataframe(table_relative(btIn))\n",
    "    \n",
    "    predy, covX = patsy.dmatrices(regEquation, mapDfIn, return_type='dataframe')\n",
    "    del covX['Intercept']\n",
    "    ccaBiom = sk.stats.ordination.cca(taIn, X)\n",
    "    Emperor(ccaBiom, mapDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "### METHOD ###\n",
    "ccaBiom.long_method_name\n",
    "\n",
    "### EIGEN-VALUES ###\n",
    "#ccaBiom.eigvals\n",
    "\n",
    "### SAMPLE LOCATIONS ###\n",
    "#ccaBiom.samples\n",
    "\n",
    "### OTU LOCATIONS ###\n",
    "#ccaBiom.features\n",
    "\n",
    "### CONSTRAINING COVARIATES ###\n",
    "ccaBiom.biplot_scores\n",
    "\n",
    "\n",
    "#ccaBiom.sample_constraints\n",
    "\n",
    "#ccaBiom.proportion_explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### COMPARISONS OF EACH GROUP #####\n",
    "# SHOULD ALL PAIRS\n",
    "singleBoolIn = True\n",
    "pairBoolIn = True\n",
    "\n",
    "numGroupsIn = len(mapDf['race'].unique())\n",
    "dmResults = sk.stats.distance.DistanceMatrix(data=np.zeros((numGroupsIn,numGroupsIn)), ids=mapDf['race'].unique())\n",
    "dmResults = dmResults.to_data_frame()\n",
    "allResOut = pd.DataFrame(columns=['size','correlation'])\n",
    "##### IF ALL GROUPS CONTRASTED TO MEAN #####\n",
    "if singleBoolIn == True:\n",
    "    ### CREATE DISTANCE MATRIX OF PREDICTORS ###\n",
    "    y, X = patsy.dmatrices(\" age_years ~ age_years + sex + bmi \", mapDf, return_type='dataframe')\n",
    "    ### REMOVE INTERCEPT COLUMN ###\n",
    "    del X['Intercept']\n",
    "    ### FOR EACH GROUP ###\n",
    "    for i in mapDf['race'].unique():\n",
    "        X[('race_'+i)] = np.where(mapCopy[('race')]==i, 0.75, -0.25)\n",
    "        ### DISPLAY CORRELATION ALONE ###\n",
    "        resIn = sk.stats.distance.bioenv(consensusDM, pd.DataFrame(data=[X['race_'+i]], index=['race_'+i]).T)\n",
    "        allResOut = pd.concat([allResOut, resIn],axis=0)\n",
    "        dmResults[i][i] = resIn['correlation'][0]\n",
    "    ### PERFORM CORRELATION ANALYSIS WITH ALL ###\n",
    "    print(list(X.columns))\n",
    "    display(sk.stats.distance.bioenv(consensusDM, X))\n",
    "    del X['age_years']\n",
    "    del X['bmi']\n",
    "    del X['sex[T.male]']\n",
    "    print(list(X.columns))\n",
    "    display(sk.stats.distance.bioenv(consensusDM, X))\n",
    "    \n",
    "##### IF ALL PAIRS OF GROUPS CONTRASTED #####\n",
    "if pairBoolIn == True:\n",
    "    ### CREATE DISTANCE MATRIX OF PREDICTORS ###\n",
    "    y, X = patsy.dmatrices(\" age_years ~ age_years + sex + bmi \", mapDf, return_type='dataframe')\n",
    "    ### REMOVE INTERCEPT COLUMN ###\n",
    "    del X['Intercept']\n",
    "    ### FOR EACH PAIR OF GROUPS SHOW INDIVIDUAL CORRELATION ###\n",
    "    for i in list(itertools.combinations(mapDf['race'].unique(),2)):\n",
    "        X[('race_'+i[0]+'_'+i[1])] = np.where(mapCopy[('race')]==i[0], 1, 0)\n",
    "        X[('race_'+i[0]+'_'+i[1])].ix[np.where(mapCopy[('race')]==i[1])] = -1\n",
    "        resIn = sk.stats.distance.bioenv(consensusDM, pd.DataFrame(data=[X[('race_'+i[0]+'_'+i[1])]], index=[('race_'+i[0]+'_'+i[1])]).T)\n",
    "        allResOut = pd.concat([allResOut, resIn],axis=0)\n",
    "        dmResults[i[0]][i[1]] = resIn['correlation'][0]\n",
    "        dmResults[i[1]][i[0]] = resIn['correlation'][0]\n",
    "    ### PERFORM CORRELATION ANALYSIS WITH ALL ###\n",
    "    print(list(X.columns))\n",
    "    display(sk.stats.distance.bioenv(consensusDM, X))\n",
    "    del X['age_years']\n",
    "    del X['bmi']\n",
    "    del X['sex[T.male]']\n",
    "    print(list(X.columns))\n",
    "    display(sk.stats.distance.bioenv(consensusDM, X))\n",
    "    \n",
    "    \n",
    "\n",
    "sns.heatmap(dmResults)\n",
    "plt.show()\n",
    "display(allResOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "\n",
    "### CALCULATE A DICTIONARY OF INFO ABOUT OTUS ###\n",
    "def observation_info(bt, mapDfIn, mapCatsIn=None):\n",
    "    print(\" - COLLATING OBSERVATION INFORMATION -\")\n",
    "    ### CREATE DICTIONARY FOR INFO WITH TAXA FILLED###\n",
    "    obsDfCounts = table_dataframe(bt).T\n",
    "    obsDfTaxa = table_dataframe_taxa(bt)\n",
    "    \n",
    "    ##### ANALYSES OF OTUS ##########\n",
    "    obsDfTaxa['OTU Mean'] = pd.Series(obsDfCounts.apply(np.mean, axis=1), index=obsDfCounts.index)\n",
    "    obsDfTaxa['OTU Std All'] = pd.Series(obsDfCounts.apply(lambda x: np.std(x), axis=1), index=obsDfCounts.index)\n",
    "    obsDfTaxa['NNZ'] = pd.Series(obsDfCounts.apply(lambda x: np.count_nonzero(x), axis=1), index=obsDfCounts.index)\n",
    "    obsDfTaxa['NNZ Mean'] = pd.Series(obsDfCounts[obsDfCounts > 0].apply(np.mean, axis=1), index=obsDfCounts.index)\n",
    "    return obsDfTaxa, obsDfCounts\n",
    "\n",
    "def observation_correlation(mapDfIn, obsDfTaxaIn, obsDfCountsIn, independentEquation=\"col1 + col2 + col1:col2\", zeroReplace='skip'):\n",
    "    print(\" - PERFORMING OBSERVATION REGRESSIONS -\")\n",
    "    print(\"   - OTU Abundance = \"+independentEquation)\n",
    "    \n",
    "    ### FORM REGRESSION DATAFRAME ###\n",
    "    inDFConcat = pd.concat([mapDfIn, obsDfCountsIn.T],axis=1)\n",
    "    \n",
    "    ### DATA STRUCTURE FOR RESULTS ###\n",
    "    y, X = patsy.dmatrices(\"patsy.builtins.Q('\"+obsDfTaxaIn.index[0]+\"') ~ \"+independentEquation, inDFConcat, return_type='dataframe')    \n",
    "    colsIn = ['rsquared','f_pvalue']; colsIn.extend(X.columns)\n",
    "    storeRegRes = pd.DataFrame(data=None,columns=colsIn,index=list(obsDfTaxaIn.index))\n",
    "\n",
    "    ### HOW TO DEAL WITH ZERO VALUES ###\n",
    "    #if zeroReplace != None: obsDfCountsIn = obsDfCountsIn.replace(0.0, zeroReplace)\n",
    "\n",
    "    ### FOR EACH OTU ###\n",
    "    for obsIdx, obsCycle in enumerate(obsDfCounts.index):\n",
    "        \n",
    "        ### Y = INDX... + B ||| SETUP INPUT MATRIX & CODE CATEGORICAL FACTORS ###\n",
    "        y, X = patsy.dmatrices(\"np.log10(patsy.builtins.Q('\"+str(obsCycle)+\"')) ~ \"+independentEquation, inDFConcat[inDFConcat[obsCycle] > 0], return_type='dataframe')\n",
    "    \n",
    "        ### GENERATE & FIT MODEL ###\n",
    "        statsModelReg = sm.OLS(y, X)\n",
    "        statsModelRes = statsModelReg.fit()\n",
    "\n",
    "        ### COLLATE AND ADD RESULTS TO DATAFRAME ###\n",
    "        storeRegRes.set_value(str(obsCycle), 'rsquared', statsModelRes.rsquared)\n",
    "        storeRegRes.set_value(str(obsCycle), 'f_pvalue', statsModelRes.f_pvalue)\n",
    "        for statIdx, statCur in enumerate(statsModelRes.pvalues.keys()): \n",
    "            storeRegRes.set_value(str(obsCycle), statCur, statsModelRes.pvalues.values[statIdx])\n",
    "        \n",
    "        ### PRINT PROGRESS ###\n",
    "        if obsIdx %1000 == 0: \n",
    "            print(\"     - Progress: \"+str(obsIdx))\n",
    "    \n",
    "    return pd.concat([obsDfTaxaIn, storeRegRes],axis=1)\n",
    "\n",
    "\n",
    "### CALCULATE OTU INFO ###\n",
    "obsDfTaxa, obsDfCounts = observation_info(table_relative(biomTable), mapDf, 'race')\n",
    "\n",
    "### PERFORM REGRESSION AGAINST METADATA ###\n",
    "#obsDfRegRes = observation_correlation(mapDf, obsDfTaxa, obsDfCounts, independentEquation=\"age_years + bmi + race + sex + race:sex\", zeroReplace=None)\n",
    "obsDfRegRes = observation_correlation(mapDf, obsDfTaxa, obsDfCounts, independentEquation=\" race \", zeroReplace=None)\n",
    "\n",
    "display(obsDfRegRes.sort_values(by='f_pvalue', ascending=True))\n",
    "#display(obsDfRegRes.groupby([obsDfTaxa['phylum'],obsDfTaxa['class'],obsDfTaxa['order'],obsDfTaxa['family'],obsDfTaxa['genus']], ).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1 = obsDfCounts.T['325850']\n",
    "\n",
    "\n",
    "sns.distplot(np.log10(x1[x1 > 0.0]), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1 = obsDfCounts.T['325850']\n",
    "\n",
    "\n",
    "sns.distplot(np.arcsin(np.sqrt(x1[x1>0.0])), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sm.graphics.plot_partregress('bmi', 'age_years', ['sex', 'race'], data=pd.concat([mapDf, obsDfCounts.T],axis=1), obs_labels=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### ANCOM PIPELINE - UNFUNCTION ATM #####\n",
    "\"\"\"\n",
    "def pipeline_ancom(bt, mapDfIn):\n",
    "    ### TABLE AS RELATIVE ABUNDANCE ###\n",
    "    btRelative = table_relative(bt)\n",
    "    ### CONVERT TO PANDAS DATAFRAME (COLS = OTUS) ###\n",
    "    btRelativePd = counts_as_pandas(btRelative)\n",
    "    ### USE MULTIPLICITIVE REPLACEMENT TO REPLACE 0'S WITH 10^-8 VALUES ###\n",
    "    btRelativePdRep = sk.stats.composition.multiplicative_replacement(btRelativePd, delta=None)\n",
    "    ### CONVERT BACK TO PANDAS DF ###\n",
    "    btRelativePdRepPd = pd.DataFrame(btRelativePdRep, index=bt.ids(axis='sample'), columns=bt.ids(axis='observation'))\n",
    "    return sk.stats.composition.ancom(btRelativePdRepPd, \n",
    "                               mapDfIn, \n",
    "                               alpha=0.05, tau=0.02, theta=0.1, \n",
    "                               multiple_comparisons_correction='holm-bonferroni', \n",
    "                               significance_test=sp.stats.kruskal, \n",
    "                               percentiles=(0.0, 25.0, 50.0, 75.0, 100.0))\n",
    "\n",
    "#x = pipeline_ancom(filter_otu_mincount(biomTable, 10), mapDf['race'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sk.diversity.get_alpha_diversity_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sk.diversity.get_alpha_diversity_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center; color:black;\">------------------------------------------------------------------------------</h3>\n",
    "<h2 style=\"text-align:center; color:orange;\"> - Custom - </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# betaMetrics = ['braycurtis', 'jaccard', 'weighted_unifrac','unweighted_unifrac']\n",
    "#http://scikit-bio.org/docs/0.1.3/generated/skbio.core.distance.DistanceMatrix.html#skbio.core.distance.DistanceMatrix \n",
    "#sk.core.distance.DistanceMatrix.filter(ids)\n",
    "#sk.core.distance.DistanceMatrix.to_file(out_f[, delimiter])\n",
    "#sk.core.distance.DistanceMatrix.permute([condensed])\n",
    "#sk.core.distance.DistanceMatrix.copy()\n",
    "#sk.core.distance.DistanceMatrix.condensed_form()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Gneiss Log ratio Abudnances toolkit\n",
    "\n",
    "import gneiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align:center; color:blue;\"> - Partition by Experiment (Host Clade) - </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "firstRun = False # Change this to True if you want to rewrite output to folder\n",
    "\n",
    "### METADATA CATEGORY TO PARTITION BY ###\n",
    "metaPartCat = \"Experiment\"\n",
    "\n",
    "### PARTITION TABLE BY EXPERIMENT ###\n",
    "expTables = partition_meta(biomTable, metaPartCat, print_summary=False)\n",
    "\n",
    "### FOR EACH PARTITION...\n",
    "for expName in expTables.keys():\n",
    "    \n",
    "    ### MAKE SUBDIRECTORY ###\n",
    "    if firstRun == True: \n",
    "        if not os.path.isdir(dirPath+\"0_2_\"+expName): os.makedirs(dirPath+\"0_2_\"+expName)\n",
    "\n",
    "    print()\n",
    "    ### PRINT EXPERIMENT NAME ###\n",
    "    print(expName)\n",
    "    \n",
    "    ### FILTER OUT OTUS WITH <2 COUNTS ###\n",
    "    expTables[expName] = filter_otu_mincount(expTables[expName], 2)\n",
    "    \n",
    "    ### PRINT EXPERIMENT INFO ###\n",
    "    if firstRun == True: get_table_info(expTables[expName], writePath=dirPath+\"0_2_\"+expName+\"/\"+\"0_2_table_summary_\"+expName+\".txt\", printOut = True)\n",
    "    else: get_table_info(expTables[expName], printOut = True)\n",
    "\n",
    "    ### PRINT COUNTS PER SAMPLE ###\n",
    "    print(\"Sample Counts:  \"+str(expTables[expName].sum('sample')))\n",
    "    print(\"Minimum Sample: \"+str(min(expTables[expName].sum('sample')))) \n",
    "    \n",
    "    ### ADD METADATA TO EXPERIMENT TABLE ###\n",
    "    expTables[expName].add_metadata(metaDict)\n",
    "    \n",
    "    ### WRITE TABLE AS TSV AND JSON ###\n",
    "    if firstRun == True: write_tsv(expTables[expName], dirPath+\"0_2_\"+expName+\"\"+\"0_2_table_\"+expName+\".txt\")\n",
    "    if firstRun == True: write_json(expTables[expName], dirPath+\"0_2_\"+expName+\"/\"+\"0_2_table_\"+expName+\".biom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align:center; color:blue;\"> - Perform Rarefactions within Each Clade 100x to Depth: (Minimum Sample-1) - </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### CREATE DICTIONARY TO HOLD RAREFIED TABLES ###\n",
    "expTablesRarefied = {}\n",
    "### FOR EACH PARTITION...\n",
    "for expName in expTables.keys():\n",
    "    ### CREATE ARRAY OF 100 RAREFIED TABLES AT A DEPTH OF (THE MINIMUM SAMPLE -1) ###\n",
    "    print(\"Rarefying: \"+expName+\" to a depth of \"+str(int(min(expTables[expName].sum('sample')) - 1)))\n",
    "    expTablesRarefied[expName] = multiple_rarefactions_even_depth(expTables[expName], int(min(expTables[expName].sum('sample')) - 1), 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align:center; color:blue;\"> - Calculate Beta Diversity for the 100 Rarefied Tables of Each Clade - </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### CREATE DICTIONARY TO HOLD BETA DIVERSITY DISTANCE MATRICES\n",
    "expTablesBC = {}\n",
    "### FOR EACH PARTITION...\n",
    "for expName in expTablesRarefied.keys():\n",
    "    print(\"Calculating Beta Diversity: \" + expName); expTablesBC[expName] = []\n",
    "    ### FOR EACH RAREFIED TABLE\n",
    "    for rareTableIncrement in expTablesRarefied[expName]:\n",
    "        expTablesBC[expName].append(beta_braycurtis(rareTableIncrement))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for expName in expTablesBC.keys():\n",
    "    print(expName)\n",
    "    ### CALCULATE CONSENSUS UPGMA TREE ###\n",
    "    print(beta_upgma_consensus(expTablesBC[expName])[0].ascii_art())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in dmAvgDict.keys():\n",
    "    dmAvgDict[i].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dmAvg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# write output\n",
    "fTreeOut = open(output_file, 'w')\n",
    "fTreeOut.write(upgmaTree.to_newick(with_distances=True))\n",
    "fTreeOut.close()\n",
    "\n",
    "\n",
    "    \n",
    "# SciPy uses average as UPGMA:\n",
    "# http://docs.scipy.org/doc/scipy/reference/generated/\n",
    "# scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "__author__ = \"Justin Kuczynski\"\n",
    "__copyright__ = \"Copyright 2011, The QIIME Project\"\n",
    "__credits__ = [\"Justin Kuczynski\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.9.1\"\n",
    "__maintainer__ = \"Justin Kuczynski\"\n",
    "__email__ = \"justinak@gmail.com\"\n",
    "\n",
    "\"\"\"runs upgma or nj on a distance matrix file, writes the resulting tree\n",
    "with distances to specified output file\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', 'Not using MPI as mpi4py not found')\n",
    "from optparse import OptionParser\n",
    "from qiime.parse import parse_distmat\n",
    "from  import linkage\n",
    "from  import TreeNode\n",
    "from skbio.tree import nj\n",
    "import os.path\n",
    "\n",
    "\n",
    "def multiple_file_upgma(input_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    file_names = os.listdir(input_dir)\n",
    "    file_names = [fname for fname in file_names if not fname.startswith('.')]\n",
    "\n",
    "    for fname in file_names:\n",
    "        base_fname, ext = os.path.splitext(fname)\n",
    "        single_file_upgma(os.path.join(input_dir, fname),\n",
    "                          os.path.join(output_dir, 'upgma_' + base_fname + '.tre'))\n",
    "\n",
    "\n",
    "def single_file_upgma(input_file, output_file):\n",
    "    # read in dist matrix\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def single_file_nj(input_file, output_file):\n",
    "    dm = DistanceMatrix.read(input_file)\n",
    "\n",
    "    tree = nj(dm)\n",
    "\n",
    "    # write output\n",
    "    f = open(output_file, 'w')\n",
    "    f.write(tree.to_newick(with_distances=True))\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def multiple_file_nj(input_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    file_names = os.listdir(input_dir)\n",
    "    file_names = [fname for fname in file_names if not fname.startswith('.')]\n",
    "\n",
    "    for fname in file_names:\n",
    "        base_fname, ext = os.path.splitext(fname)\n",
    "        single_file_nj(os.path.join(input_dir, fname),\n",
    "                       os.path.join(output_dir, 'nj_' + base_fname + '.tre'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center; color:black;\">------------------------------------------------------------------------------</h3>\n",
    "<h3 style=\"text-align:center; color:orange;\"> - IMPORT FAMILY LEVEL RELATIVE ABUNDANCE BIOM TABLE - </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "famPath = \"/Users/brooks/Dropbox/American_Gut/Data/21_0_summarize_taxa/20_2_filter_country_L5.txt\"\n",
    "famTable = load_table(famPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center; color:orange;\"> - EXTRACT CHRISTENSENELLACEAE & ADD TO MAPPING FILE - </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### ITERATE OVER OTUS - EXTRACT CHRISTENSENELLACEAE ###\n",
    "iterOTUs = famTable.iter(axis='observation')\n",
    "for values, id, metadata in iterOTUs:\n",
    "    if id == \"k__Bacteria;p__Firmicutes;c__Clostridia;o__Clostridiales;f__Christensenellaceae\":\n",
    "        print id\n",
    "        break\n",
    "\n",
    "mapChrist = pd.concat([mapDf, pd.DataFrame(data=values, index=get_samples(famTable), columns=['Christ'])], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center; color:orange;\"> - AVERAGE BMI WITH AND WITHOUT CHRISTENSENELLACEAE - </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### AVERAGE BMI'S ###\n",
    "\n",
    "mapAllWith = mapChrist[mapChrist['Christ'] > 0]\n",
    "mapAllWithout = mapChrist[mapChrist['Christ'] == 0]\n",
    "\n",
    "### ALL INDIVIDUALS ###\n",
    "print \"Number of Individuals :\" + str(len(mapChrist))\n",
    "print \"Average BMI: \" + str(mapChrist['bmi'].mean())\n",
    "print\n",
    "\n",
    "### ALL WITH CHRISTENSENELLACEAE ###\n",
    "print \"ALL WITH CHRISTENSENELLACEAE\"\n",
    "print \"Number of Individuals :\" + str(len(mapAllWith))\n",
    "print \"Average BMI: \" + str(mapAllWith['bmi'].mean())\n",
    "print\n",
    "\n",
    "### ALL WITHOUT CHRISTENSENELLACEAE ###\n",
    "print \"ALL WITHOUT CHRISTENSENELLACEAE\"\n",
    "print \"Number of Individuals :\" + str(len(mapAllWithout))\n",
    "print \"Average BMI: \" + str(mapAllWithout['bmi'].mean())\n",
    "print\n",
    "\n",
    "##################################################################################\n",
    "### Function - Mann-Whitney U Test - Nonparametric rank test of lists\n",
    "# Null hypothesis: two samples from the same population \n",
    "# Alternative hypothesis: one population tends to have larger values than the other [Wikipedia]\n",
    "# N samples is > 20 and you have 2 independent samples of ranks (can be unequal lengths) [Scipy]\n",
    "# For two tailed test multiply P-Value*2\n",
    "import scipy.stats as sp\n",
    "# IN: Two independent lists of floats\n",
    "# OUT: Mann Whitney Test Statistic and P-Value\n",
    "def list_mannwhitney(l1, l2, outFile=None, bonferroniComparisons=1):\n",
    "    # use_continuity = Whether a continuity correction (1/2.) should be taken into account. Default is True. [Scipy]\n",
    "    outMann = sp.mannwhitneyu(l1, l2, use_continuity=True)\n",
    "    print \"Mann Whitney U - Nonparametric Rank Test\"\n",
    "    if outFile != None: outFile.write(\"Mann Whitney U - Nonparametric Rank Test\" + \"\\n\")\n",
    "    print \"    List #1 Length: \"+str(len(l1))+\" | List #2 Length: \"+str(len(l2))\n",
    "    if outFile != None: outFile.write(\"    List #1 Length: \"+str(len(l1))+\" | List #2 Length: \"+str(len(l2)) + \"\\n\")\n",
    "    print \"    Test Statistic: \"+str(outMann[0])\n",
    "    if outFile != None: outFile.write(\"    Test Statistic: \"+str(outMann[0]) + \"\\n\")\n",
    "    print \"    P-Value (onetailed): \"+str(outMann[1])\n",
    "    if outFile != None: outFile.write(\"    P-Value (onetailed): \"+str(outMann[1]) + \"\\n\")\n",
    "    print \"    P-Value (twotailed): \"+str(outMann[1]*2)\n",
    "    if outFile != None: outFile.write(\"    P-Value (twotailed): \"+str(outMann[1]*2) + \"\\n\")\n",
    "    \n",
    "    print \"    P-Value Bonferroni Corrected (twotailed): \"+str((outMann[1]*2)*bonferroniComparisons)\n",
    "    if outFile != None: outFile.write(\"    P-Value Bonferroni Corrected (twotailed): \"+str((outMann[1]*2)*bonferroniComparisons) + \"\\n\")\n",
    "    print\n",
    "    return outMann\n",
    "##################################################################################\n",
    "\n",
    "### COMPARE THOSE WITH TO THOSE WITHOUT USING LIST_MANNWHITNEY ###\n",
    "list_mannwhitney(mapAllWith['bmi'], mapAllWithout['bmi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center; color:orange;\"> - UBIQUITY OF CHRISTENSENELLACEAE - </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### GET UBIQUITY BY RACE - ISOLATE RACES AS SEPARATE TABLES ###\n",
    "mapAA = mapChrist[mapChrist['race'] == \"African American\"]\n",
    "mapAS = mapChrist[mapChrist['race'] == \"Asian or Pacific Islander\"]\n",
    "mapCA = mapChrist[mapChrist['race'] == \"Caucasian\"]\n",
    "mapHI = mapChrist[mapChrist['race'] == \"Hispanic\"]\n",
    "\n",
    "#### UBIQUITY FUNCTION ####\n",
    "def ubiquityCount(dfIn):\n",
    "    print \"Non-Zero Individuals      : \" + str(dfIn['Christ'].astype(bool).sum())\n",
    "    print \"Total Individuals         : \" + str(len(dfIn['Christ']))\n",
    "    print \"Ubiquity                  : \" + str( float(dfIn['Christ'].astype(bool).sum()) / float(len(dfIn['Christ'])))\n",
    "    print \n",
    "\n",
    "# CALCULATE UBIQUITY #\n",
    "print \"AA\"\n",
    "ubiquityCount(mapAA)\n",
    "\n",
    "print \"AS\"\n",
    "ubiquityCount(mapAS)\n",
    "\n",
    "print \"CA\"\n",
    "ubiquityCount(mapCA)\n",
    "\n",
    "print \"HI\"\n",
    "ubiquityCount(mapHI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center; color:orange;\"> - AVERAGE BMI AND CHRISTENSENELLACEAE BY RACE - </h3>\n",
    "<h4 style=\"text-align:center; color:orange;\"> - This includes the boxplots generated for figure 4 - </h4>\n",
    "<h4 style=\"text-align:center; color:orange;\"> - and the statistics of BMI for those with and without Christensenellaceae used in figure 4 - </h4>\n",
    "<h4 style=\"text-align:center; color:orange;\"> - Also includes some additional stats and an additional regression figure not used in manuscript - </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### GET AVERAGE BMI AND CHRISTENSENELLACEAE FOR EACH RACE - INCLUDING FOR THOSE WITH AND WITHOUT PRESENCE SUBSET ###\n",
    "# Get log10 transformation of counts\n",
    "mapChrist[\"ChristLog\"] = np.log10(mapChrist[\"Christ\"])\n",
    "# Get arcsin sqrt transformation of counts (see Structure, Function, and Diversity of the Healthy Human Microbiome)\n",
    "mapChrist[\"ChristArcSin\"] = np.arcsin(np.sqrt(mapChrist[\"Christ\"]))\n",
    "\n",
    "print \"GET AVERAGE BMI AND CHRISTENSENELLACEAE FOR EACH RACE - INCLUDING FOR THOSE WITH AND WITHOUT PRESENCE SUBSET\"\n",
    "# FOR ALL INDIVIDUALS\n",
    "print 'AA Mean BMI                : ' +str(np.mean(mapChrist[mapChrist['race'] == 'African American']['bmi'] ))\n",
    "print 'AA Mean Christensenellaceae: ' +str(np.mean(mapChrist[mapChrist['race'] == 'African American']['Christ'] ))\n",
    "print\n",
    "print 'AS Mean BMI                : ' +str(np.mean(mapChrist[mapChrist['race'] == 'Asian or Pacific Islander']['bmi'] ))\n",
    "print 'AS Mean Christensenellaceae: ' +str(np.mean(mapChrist[mapChrist['race'] == 'Asian or Pacific Islander']['Christ'] ))\n",
    "print\n",
    "print 'CA Mean BMI                : ' +str(np.mean(mapChrist[mapChrist['race'] == 'Caucasian']['bmi'] ))\n",
    "print 'CA Mean Christensenellaceae: ' +str(np.mean(mapChrist[mapChrist['race'] == 'Caucasian']['Christ'] ))\n",
    "print\n",
    "print 'HI Mean BMI                : ' +str(np.mean(mapChrist[mapChrist['race'] == 'Hispanic']['bmi'] ))\n",
    "print 'HI Mean Christensenellaceae: ' +str(np.mean(mapChrist[mapChrist['race'] == 'Hispanic']['Christ'] ))\n",
    "\n",
    "# Remove samples w/ no Christ\n",
    "mapChristR = mapChrist[mapChrist[\"Christ\"] != 0]\n",
    "# FOR ONLY INDIVIDUALS WITH CHRISTENSENELLACEAE #\n",
    "print\n",
    "print '-----------------------------------------------'\n",
    "print 'AA Mean BMI                : ' +str(np.mean(mapChristR[mapChristR['race'] == 'African American']['bmi'] ))\n",
    "print 'AA Mean Christensenellaceae: ' +str(np.mean(mapChristR[mapChristR['race'] == 'African American']['Christ'] ))\n",
    "print\n",
    "print 'AS Mean BMI                : ' +str(np.mean(mapChristR[mapChristR['race'] == 'Asian or Pacific Islander']['bmi'] ))\n",
    "print 'AS Mean Christensenellaceae: ' +str(np.mean(mapChristR[mapChristR['race'] == 'Asian or Pacific Islander']['Christ'] ))\n",
    "print\n",
    "print 'CA Mean BMI                : ' +str(np.mean(mapChristR[mapChristR['race'] == 'Caucasian']['bmi'] ))\n",
    "print 'CA Mean Christensenellaceae: ' +str(np.mean(mapChristR[mapChristR['race'] == 'Caucasian']['Christ'] ))\n",
    "print\n",
    "print 'HI Mean BMI                : ' +str(np.mean(mapChristR[mapChristR['race'] == 'Hispanic']['bmi'] ))\n",
    "print 'HI Mean Christensenellaceae: ' +str(np.mean(mapChristR[mapChristR['race'] == 'Hispanic']['Christ'] ))\n",
    "\n",
    "# Remove samples w/ Christ\n",
    "mapChristRWO = mapChrist[mapChrist[\"Christ\"] == 0]\n",
    "# FOR ONLY INDIVIDUALS WITHOUT CHRISTENSENELLACEAE #\n",
    "print\n",
    "print '-----------------------------------------------'\n",
    "print 'AA Mean BMI                : ' +str(np.mean(mapChristRWO[mapChristRWO['race'] == 'African American']['bmi'] ))\n",
    "print 'AA Mean Christensenellaceae: ' +str(np.mean(mapChristRWO[mapChristRWO['race'] == 'African American']['Christ'] ))\n",
    "print \n",
    "print 'AS Mean BMI                : ' +str(np.mean(mapChristRWO[mapChristRWO['race'] == 'Asian or Pacific Islander']['bmi'] ))\n",
    "print 'AS Mean Christensenellaceae: ' +str(np.mean(mapChristRWO[mapChristRWO['race'] == 'Asian or Pacific Islander']['Christ'] ))\n",
    "print\n",
    "print 'CA Mean BMI                : ' +str(np.mean(mapChristRWO[mapChristRWO['race'] == 'Caucasian']['bmi'] ))\n",
    "print 'CA Mean Christensenellaceae: ' +str(np.mean(mapChristRWO[mapChristRWO['race'] == 'Caucasian']['Christ'] ))\n",
    "print\n",
    "print 'HI Mean BMI                : ' +str(np.mean(mapChristRWO[mapChristRWO['race'] == 'Hispanic']['bmi'] ))\n",
    "print 'HI Mean Christensenellaceae: ' +str(np.mean(mapChristRWO[mapChristRWO['race'] == 'Hispanic']['Christ'] ))\n",
    "\n",
    "##################################################################################\n",
    "##################################################################################\n",
    "##################################################################################\n",
    "\n",
    "### COMPARE WITH AND WITHOUT BMIs - EXTRACT THOSE WITH AND WITHOUT CHRISTENSENELLACEAE FOR EACH RACE ###\n",
    "# EACH RACE WITH CHRISTENSENELLACEAE #\n",
    "mapAAr = mapChristR[mapChristR['race'] == \"African American\"]\n",
    "mapASr = mapChristR[mapChristR['race'] == \"Asian or Pacific Islander\"]\n",
    "mapCAr = mapChristR[mapChristR['race'] == \"Caucasian\"]\n",
    "mapHIr = mapChristR[mapChristR['race'] == \"Hispanic\"]\n",
    "# EACH RACE WITHOUT CHRISTENSENELLACEAE #\n",
    "mapAArwo = mapChristRWO[mapChristRWO['race'] == \"African American\"]\n",
    "mapASrwo = mapChristRWO[mapChristRWO['race'] == \"Asian or Pacific Islander\"]\n",
    "mapCArwo = mapChristRWO[mapChristRWO['race'] == \"Caucasian\"]\n",
    "mapHIrwo = mapChristRWO[mapChristRWO['race'] == \"Hispanic\"]\n",
    "\n",
    "##################################################################################\n",
    "### Function - Mann-Whitney U Test - Nonparametric rank test of lists\n",
    "# Null hypothesis: two samples from the same population \n",
    "# Alternative hypothesis: one population tends to have larger values than the other [Wikipedia]\n",
    "# N samples is > 20 and you have 2 independent samples of ranks (can be unequal lengths) [Scipy]\n",
    "# For two tailed test multiply P-Value*2\n",
    "import scipy.stats as sp\n",
    "# IN: Two independent lists of floats\n",
    "# OUT: Mann Whitney Test Statistic and P-Value\n",
    "def list_mannwhitney(l1, l2, outFile=None, bonferroniComparisons=1):\n",
    "    # use_continuity = Whether a continuity correction (1/2.) should be taken into account. Default is True. [Scipy]\n",
    "    outMann = sp.mannwhitneyu(l1, l2, use_continuity=True)\n",
    "    print \"Mann Whitney U - Nonparametric Rank Test\"\n",
    "    if outFile != None: outFile.write(\"Mann Whitney U - Nonparametric Rank Test\" + \"\\n\")\n",
    "    print \"    List #1 Length: \"+str(len(l1))+\" | List #2 Length: \"+str(len(l2))\n",
    "    if outFile != None: outFile.write(\"    List #1 Length: \"+str(len(l1))+\" | List #2 Length: \"+str(len(l2)) + \"\\n\")\n",
    "    print \"    Test Statistic: \"+str(outMann[0])\n",
    "    if outFile != None: outFile.write(\"    Test Statistic: \"+str(outMann[0]) + \"\\n\")\n",
    "    print \"    P-Value (onetailed): \"+str(outMann[1])\n",
    "    if outFile != None: outFile.write(\"    P-Value (onetailed): \"+str(outMann[1]) + \"\\n\")\n",
    "    print \"    P-Value (twotailed): \"+str(outMann[1]*2)\n",
    "    if outFile != None: outFile.write(\"    P-Value (twotailed): \"+str(outMann[1]*2) + \"\\n\")\n",
    "    \n",
    "    print \"    P-Value Bonferroni Corrected (twotailed): \"+str((outMann[1]*2)*bonferroniComparisons)\n",
    "    if outFile != None: outFile.write(\"    P-Value Bonferroni Corrected (twotailed): \"+str((outMann[1]*2)*bonferroniComparisons) + \"\\n\")\n",
    "    print\n",
    "    return outMann\n",
    "##################################################################################\n",
    "\n",
    "### COMPARE BMIS OF THOSE WITH CHRISTENSENELLACEAE TO THOSE WITHOUT FOR EACH RACE USING MANN WHITNEY U TEST ###\n",
    "print \"COMPARE BMIS OF THOSE WITH CHRISTENSENELLACEAE TO THOSE WITHOUT FOR EACH RACE USING MANN WHITNEY U TEST\"\n",
    "print \"COMPARE EACH RACES BMI WITH AND WITHOUT CHRISTENSENELLACEAE #\"\n",
    "print\n",
    "print \"AA\"\n",
    "list_mannwhitney(mapAAr['bmi'], mapAArwo['bmi'])\n",
    "\n",
    "print\n",
    "print \"AS\"\n",
    "list_mannwhitney(mapASr['bmi'], mapASrwo['bmi'])\n",
    "\n",
    "print\n",
    "print \"CA\"\n",
    "list_mannwhitney(mapCAr['bmi'], mapCArwo['bmi'])\n",
    "\n",
    "print\n",
    "print \"HI\"\n",
    "list_mannwhitney(mapHIr['bmi'], mapHIrwo['bmi'])\n",
    "print\n",
    "\n",
    "##################################################################################\n",
    "##################################################################################\n",
    "##################################################################################\n",
    "\n",
    "import scipy\n",
    "#### PLOT OF AVERAGES - THIS IS A SCATTER PLOT OF THE AVERAGE BMI BY THE AVERAGE CHRISTENSENELLACEAE FOR THOSE WITH AND WITHOUT THE PRESENECE OF CHRISTENSENELLACEAE ####\n",
    "#### NOT USED IN MANUSCRIPT - BUT HAS REGRESSIONS LINES AND REGRESSIONS STATISTICS OUTPUT WHICH ARE REALLY ONLY INTERESTING TO EVALUATE SLOPE (P-VALS USELESS) \n",
    "plt.figure(figsize=[10,5])\n",
    "print \"PLOT OF AVERAGES - THIS IS A SCATTER PLOT OF THE AVERAGE BMI BY THE AVERAGE CHRISTENSENELLACEAE FOR THOSE WITH AND WITHOUT THE PRESENECE OF CHRISTENSENELLACEAE\"\n",
    "print \"NOT USED IN MANUSCRIPT - BUT HAS REGRESSIONS LINES AND REGRESSIONS STATISTICS OUTPUT WHICH ARE REALLY ONLY INTERESTING TO EVALUATE SLOPE (P-VALS USELESS)\"\n",
    "# PLOT ALL INDIVIDUALS #\n",
    "plt.scatter(np.mean(mapChrist[mapChrist['race'] == 'African American']['bmi'] ),np.mean(mapChrist[mapChrist['race'] == 'African American']['Christ'] ), c='y', s=100, marker='^')\n",
    "plt.scatter(np.mean(mapChrist[mapChrist['race'] == 'Asian or Pacific Islander']['bmi'] ),np.mean(mapChrist[mapChrist['race'] == 'Asian or Pacific Islander']['Christ'] ), c='b', s=100, marker='^')\n",
    "plt.scatter(np.mean(mapChrist[mapChrist['race'] == 'Caucasian']['bmi'] ),np.mean(mapChrist[mapChrist['race'] == 'Caucasian']['Christ'] ), c='r', s=100, marker='^')\n",
    "plt.scatter(np.mean(mapChrist[mapChrist['race'] == 'Hispanic']['bmi'] ),np.mean(mapChrist[mapChrist['race'] == 'Hispanic']['Christ'] ), c='g', s=100, marker='^')\n",
    "\n",
    "# PLOT ALL INDIVIDUALS WITH #\n",
    "plt.scatter(np.mean(mapChristR[mapChristR['race'] == 'African American']['bmi'] ),np.mean(mapChristR[mapChristR['race'] == 'African American']['Christ'] ), c='y', s=100, marker='>')\n",
    "plt.scatter(np.mean(mapChristR[mapChristR['race'] == 'Asian or Pacific Islander']['bmi'] ),np.mean(mapChristR[mapChristR['race'] == 'Asian or Pacific Islander']['Christ'] ), c='b', s=100, marker='>')\n",
    "plt.scatter(np.mean(mapChristR[mapChristR['race'] == 'Caucasian']['bmi'] ),np.mean(mapChristR[mapChristR['race'] == 'Caucasian']['Christ'] ), c='r', s=100, marker='>')\n",
    "plt.scatter(np.mean(mapChristR[mapChristR['race'] == 'Hispanic']['bmi'] ),np.mean(mapChristR[mapChristR['race'] == 'Hispanic']['Christ'] ), c='g', s=100, marker='>')\n",
    "\n",
    "# PLOT ALL INDIVIDUALS WITH #\n",
    "plt.scatter(np.mean(mapChristRWO[mapChristRWO['race'] == 'African American']['bmi'] ),np.mean(mapChristRWO[mapChristRWO['race'] == 'African American']['Christ'] ), c='y', s=100, marker='*')\n",
    "plt.scatter(np.mean(mapChristRWO[mapChristRWO['race'] == 'Asian or Pacific Islander']['bmi'] ),np.mean(mapChristRWO[mapChristRWO['race'] == 'Asian or Pacific Islander']['Christ'] ), c='b', s=100, marker='*')\n",
    "plt.scatter(np.mean(mapChristRWO[mapChristRWO['race'] == 'Caucasian']['bmi'] ),np.mean(mapChristRWO[mapChristRWO['race'] == 'Caucasian']['Christ'] ), c='r', s=100, marker='*')\n",
    "plt.scatter(np.mean(mapChristRWO[mapChristRWO['race'] == 'Hispanic']['bmi'] ),np.mean(mapChristRWO[mapChristRWO['race'] == 'Hispanic']['Christ'] ), c='g', s=100, marker='*')\n",
    "\n",
    "# REGRESSION ACROSS RACES OF INDIVIDIUALS WITH \n",
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(\n",
    "    [np.mean(mapChristR[mapChristR['race'] == 'African American']['bmi'] ), np.mean(mapChristR[mapChristR['race'] == 'Asian or Pacific Islander']['bmi'] ),np.mean(mapChristR[mapChristR['race'] == 'Caucasian']['bmi'] ),np.mean(mapChristR[mapChristR['race'] == 'Hispanic']['bmi'] )],\n",
    "    [np.mean(mapChristR[mapChristR['race'] == 'African American']['Christ'] ), np.mean(mapChristR[mapChristR['race'] == 'Asian or Pacific Islander']['Christ'] ),np.mean(mapChristR[mapChristR['race'] == 'Caucasian']['Christ'] ),np.mean(mapChristR[mapChristR['race'] == 'Hispanic']['Christ'] )])\n",
    "line = slope*mapChrist[\"bmi\"]+intercept\n",
    "plt.plot(mapChrist[\"bmi\"],line, 'k--')\n",
    "print\n",
    "print \"All With Regression\"\n",
    "print \"Slope: \" + str(slope)\n",
    "print \"R^2  : \" + str(r_value*r_value)\n",
    "print \"p-val: \" + str(p_value)\n",
    "\n",
    "# REGRESSION ACROSS RACES OF INDIVIDIUALS WITH TRANSFORMED\n",
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(\n",
    "    [np.mean(mapChristR[mapChristR['race'] == 'African American']['Christ'] ), np.mean(mapChristR[mapChristR['race'] == 'Asian or Pacific Islander']['Christ'] ),np.mean(mapChristR[mapChristR['race'] == 'Caucasian']['Christ'] ),np.mean(mapChristR[mapChristR['race'] == 'Hispanic']['Christ'] )],\n",
    "    [np.mean(mapChristR[mapChristR['race'] == 'African American']['bmi'] ), np.mean(mapChristR[mapChristR['race'] == 'Asian or Pacific Islander']['bmi'] ),np.mean(mapChristR[mapChristR['race'] == 'Caucasian']['bmi'] ),np.mean(mapChristR[mapChristR['race'] == 'Hispanic']['bmi'] )])\n",
    "\n",
    "print\n",
    "print \"All With Regression Transformed\"\n",
    "print \"Slope: \" + str(slope)\n",
    "print \"R^2  : \" + str(r_value*r_value)\n",
    "print \"p-val: \" + str(p_value)\n",
    "\n",
    "# ADD REGRESSION LINES #\n",
    "# AA\n",
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(\n",
    "    [np.mean(mapChrist[mapChrist['race'] == 'African American']['bmi'] ),np.mean(mapChristR[mapChristR['race'] == 'African American']['bmi'] ), np.mean(mapChristRWO[mapChristRWO['race'] == 'African American']['bmi'] ) ],\n",
    "    [np.mean(mapChrist[mapChrist['race'] == 'African American']['Christ'] ), np.mean(mapChristR[mapChristR['race'] == 'African American']['Christ'] ),np.mean(mapChristRWO[mapChristRWO['race'] == 'African American']['Christ'] ) ])\n",
    "line = slope*mapChrist[\"bmi\"]+intercept\n",
    "plt.plot(mapChrist[\"bmi\"],line, 'y--')\n",
    "print\n",
    "print \"African American Regression\"\n",
    "print \"Slope: \" + str(slope)\n",
    "print \"R^2  : \" + str(r_value*r_value)\n",
    "print \"p-val: \" + str(p_value)\n",
    "\n",
    "# AS\n",
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(\n",
    "    [np.mean(mapChrist[mapChrist['race'] == 'Asian or Pacific Islander']['bmi'] ),np.mean(mapChristR[mapChristR['race'] == 'Asian or Pacific Islander']['bmi'] ), np.mean(mapChristRWO[mapChristRWO['race'] == 'Asian or Pacific Islander']['bmi'] ) ],\n",
    "    [np.mean(mapChrist[mapChrist['race'] == 'Asian or Pacific Islander']['Christ'] ), np.mean(mapChristR[mapChristR['race'] == 'Asian or Pacific Islander']['Christ'] ),np.mean(mapChristRWO[mapChristRWO['race'] == 'Asian or Pacific Islander']['Christ'] ) ])\n",
    "line = slope*mapChrist[\"bmi\"]+intercept\n",
    "plt.plot(mapChrist[\"bmi\"],line, 'b--')\n",
    "print\n",
    "print \"Asian or Pacific Islander Regression\"\n",
    "print \"Slope: \" + str(slope)\n",
    "print \"R^2  : \" + str(r_value*r_value)\n",
    "print \"p-val: \" + str(p_value)\n",
    "\n",
    "# CA\n",
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(\n",
    "    [np.mean(mapChrist[mapChrist['race'] == 'Caucasian']['bmi'] ),np.mean(mapChristR[mapChristR['race'] == 'Caucasian']['bmi'] ), np.mean(mapChristRWO[mapChristRWO['race'] == 'Caucasian']['bmi'] ) ],\n",
    "    [np.mean(mapChrist[mapChrist['race'] == 'Caucasian']['Christ'] ), np.mean(mapChristR[mapChristR['race'] == 'Caucasian']['Christ'] ),np.mean(mapChristRWO[mapChristRWO['race'] == 'Caucasian']['Christ'] ) ])\n",
    "line = slope*mapChrist[\"bmi\"]+intercept\n",
    "plt.plot(mapChrist[\"bmi\"],line, 'r--')\n",
    "print\n",
    "print \"Caucasian Regression\"\n",
    "print \"Slope: \" + str(slope)\n",
    "print \"R^2  : \" + str(r_value*r_value)\n",
    "print \"p-val: \" + str(p_value)\n",
    "\n",
    "# HI\n",
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(\n",
    "    [np.mean(mapChrist[mapChrist['race'] == 'Hispanic']['bmi'] ),np.mean(mapChristR[mapChristR['race'] == 'Hispanic']['bmi'] ), np.mean(mapChristRWO[mapChristRWO['race'] == 'Hispanic']['bmi'] ) ],\n",
    "    [np.mean(mapChrist[mapChrist['race'] == 'Hispanic']['Christ'] ), np.mean(mapChristR[mapChristR['race'] == 'Hispanic']['Christ'] ),np.mean(mapChristRWO[mapChristRWO['race'] == 'Hispanic']['Christ'] ) ])\n",
    "line = slope*mapChrist[\"bmi\"]+intercept\n",
    "plt.plot(mapChrist[\"bmi\"],line, 'g--')\n",
    "print\n",
    "print \"Hispanic Regression\"\n",
    "print \"Slope: \" + str(slope)\n",
    "print \"R^2  : \" + str(r_value*r_value)\n",
    "print \"p-val: \" + str(p_value)\n",
    "\n",
    "##########\n",
    "plt.xlabel(\"BMI\")\n",
    "plt.ylabel(\"Relative Christensenellaceae\")\n",
    "plt.title(\"Christensenellaceae by BMI for Each Race - All Individuals & with / without\")\n",
    "plt.tight_layout()\n",
    "plt.ylim(-0.001, 0.007)\n",
    "plt.savefig(\"/Users/brooks/Dropbox/American_Gut/Figures/S5_HMP_Overlap/AVG_PLOT2.pdf\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### PRINT REGRESSION TRANSFORMED ###\n",
    "# ADD REGRESSION LINES #\n",
    "print \"NOT USED IN MANUSCRIPT - REGRESSIONS STATISTICS FOR TRANSFORMED CHRISTENSENELLACEAE ABUNDANCE OUTPUT WHICH ARE REALLY ONLY INTERESTING TO EVALUATE SLOPE (P-VALS USELESS)\"\n",
    "\n",
    "# AA\n",
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(\n",
    "    [np.mean(mapChrist[mapChrist['race'] == 'African American']['Christ'] ), np.mean(mapChristR[mapChristR['race'] == 'African American']['Christ'] ),np.mean(mapChristRWO[mapChristRWO['race'] == 'African American']['Christ'] ) ],\n",
    "    [np.mean(mapChrist[mapChrist['race'] == 'African American']['bmi'] ),np.mean(mapChristR[mapChristR['race'] == 'African American']['bmi'] ), np.mean(mapChristRWO[mapChristRWO['race'] == 'African American']['bmi'] ) ]\n",
    "\n",
    ")\n",
    "print\n",
    "print \"African American Regression\"\n",
    "print \"Slope: \" + str(slope)\n",
    "print \"R^2  : \" + str(r_value*r_value)\n",
    "print \"p-val: \" + str(p_value)\n",
    "\n",
    "# AS\n",
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(\n",
    "    [np.mean(mapChrist[mapChrist['race'] == 'Asian or Pacific Islander']['Christ'] ), np.mean(mapChristR[mapChristR['race'] == 'Asian or Pacific Islander']['Christ'] ),np.mean(mapChristRWO[mapChristRWO['race'] == 'Asian or Pacific Islander']['Christ'] ) ],\n",
    "    [np.mean(mapChrist[mapChrist['race'] == 'Asian or Pacific Islander']['bmi'] ),np.mean(mapChristR[mapChristR['race'] == 'Asian or Pacific Islander']['bmi'] ), np.mean(mapChristRWO[mapChristRWO['race'] == 'Asian or Pacific Islander']['bmi'] ) ]\n",
    ")\n",
    "print\n",
    "print \"Asian or Pacific Islander Regression\"\n",
    "print \"Slope: \" + str(slope)\n",
    "print \"R^2  : \" + str(r_value*r_value)\n",
    "print \"p-val: \" + str(p_value)\n",
    "\n",
    "# CA\n",
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(    \n",
    "    [np.mean(mapChrist[mapChrist['race'] == 'Caucasian']['Christ'] ), np.mean(mapChristR[mapChristR['race'] == 'Caucasian']['Christ'] ),np.mean(mapChristRWO[mapChristRWO['race'] == 'Caucasian']['Christ'] ) ],\n",
    "    [np.mean(mapChrist[mapChrist['race'] == 'Caucasian']['bmi'] ),np.mean(mapChristR[mapChristR['race'] == 'Caucasian']['bmi'] ), np.mean(mapChristRWO[mapChristRWO['race'] == 'Caucasian']['bmi'] ) ]\n",
    "\n",
    ")\n",
    "print\n",
    "print \"Caucasian Regression\"\n",
    "print \"Slope: \" + str(slope)\n",
    "print \"R^2  : \" + str(r_value*r_value)\n",
    "print \"p-val: \" + str(p_value)\n",
    "\n",
    "# HI\n",
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(\n",
    "    [np.mean(mapChrist[mapChrist['race'] == 'Hispanic']['Christ'] ), np.mean(mapChristR[mapChristR['race'] == 'Hispanic']['Christ'] ),np.mean(mapChristRWO[mapChristRWO['race'] == 'Hispanic']['Christ'] ) ],\n",
    "    [np.mean(mapChrist[mapChrist['race'] == 'Hispanic']['bmi'] ),np.mean(mapChristR[mapChristR['race'] == 'Hispanic']['bmi'] ), np.mean(mapChristRWO[mapChristRWO['race'] == 'Hispanic']['bmi'] ) ]\n",
    ")\n",
    "print\n",
    "print \"Hispanic Regression\"\n",
    "print \"Slope: \" + str(slope)\n",
    "print \"R^2  : \" + str(r_value*r_value)\n",
    "print \"p-val: \" + str(p_value)\n",
    "\n",
    "\n",
    "### BOXPLOTS OF BMIS FOR INDIVDIDUALS WITH AND WITHOUT CHRISTENSENELLACEAE USED IN MANUSCRIPT - MEANS ARE INCLUDED AS RED BOXES ###\n",
    "print \"BOXPLOTS OF BMIS FOR INDIVDIDUALS WITH AND WITHOUT CHRISTENSENELLACEAE USED IN MANUSCRIPT - MEANS ARE INCLUDED AS RED BOXES\"\n",
    "ax1 = mapChrist.boxplot(column='bmi', by='race', figsize=[15,8], showmeans=True)\n",
    "plt.title('BMI of All Individuals')\n",
    "ax1.set_ylim(0,60)\n",
    "plt.savefig(\"/Users/brooks/Dropbox/American_Gut/Figures/S5_HMP_Overlap/bmi_all.pdf\")\n",
    "\n",
    "ax2 = mapChristR.boxplot(column='bmi', by='race', figsize=[15,8], showmeans=True)\n",
    "plt.title('BMI With Christensenellaceae')\n",
    "ax2.set_ylim(0,60)\n",
    "plt.savefig(\"/Users/brooks/Dropbox/American_Gut/Figures/S5_HMP_Overlap/bmi_with.pdf\")\n",
    "\n",
    "ax3 = mapChristRWO.boxplot(column='bmi', by='race', figsize=[15,8], showmeans=True)\n",
    "plt.title('BMI Without Christensenellaceae')\n",
    "ax3.set_ylim(0,60)\n",
    "plt.savefig(\"/Users/brooks/Dropbox/American_Gut/Figures/S5_HMP_Overlap/bmi_without.pdf\")\n",
    "\n",
    "ax5 = mapChristR.boxplot(column='ChristLog', by='race', figsize=[15,8], showmeans=True)\n",
    "plt.title('Christensenellaceae Log10 With Only')\n",
    "plt.savefig(\"/Users/brooks/Dropbox/American_Gut/Figures/S5_HMP_Overlap/log_abund.pdf\")\n",
    "ax5.set_ylim(-5,0)\n",
    "\n",
    "ax5 = mapChrist.boxplot(column='ChristLog', by='race', figsize=[15,8], showmeans=True)\n",
    "plt.title('Christensenellaceae Log10 All')\n",
    "plt.savefig(\"/Users/brooks/Dropbox/American_Gut/Figures/S5_HMP_Overlap/log_abund_all.pdf\")\n",
    "ax5.set_ylim(-5,0)\n",
    "\n",
    "ax6 = mapChristR.boxplot(column='ChristArcSin', by='race', figsize=[15,8], showmeans=True)\n",
    "plt.title('Christensenellaceae ArcSin Sqrt With Only')\n",
    "plt.savefig(\"/Users/brooks/Dropbox/American_Gut/Figures/S5_HMP_Overlap/arcsin_sqrt_abund.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center; color:orange;\"> - REGRESSION OF BMI AGAINST CHRISTENSENELLACEAE ABUNDANCE - </h3>\n",
    "<h4 style=\"text-align:center; color:orange;\"> - These are regressions only including individuals with Christensenellaceae - </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get log10 transformation of counts\n",
    "mapChrist[\"ChristLog\"] = np.log10(mapChrist[\"Christ\"])\n",
    "# Get arcsin sqrt transformation of counts (see Structure, Function, and Diversity of the Healthy Human Microbiome)\n",
    "mapChrist[\"ChristArcSin\"] = np.arcsin(np.sqrt(mapChrist[\"Christ\"]))\n",
    "\n",
    "# Set Colors\n",
    "colors = {'African American':'red', 'Asian or Pacific Islander':'blue', 'Caucasian':'green', 'Hispanic':'yellow'}\n",
    "\n",
    "# Remove samples w/ no Christ\n",
    "mapChrist = mapChrist[mapChrist[\"Christ\"] != 0]\n",
    "\n",
    "# Plot ArcSin\n",
    "mapChrist.plot(kind='scatter', x='bmi', y='ChristArcSin', c=mapChrist['race'].apply(lambda x: colors[x]), figsize=[10,10])\n",
    "plt.title(\"ArcSin sqrt Transformation\")\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(mapChrist[\"bmi\"],mapChrist[\"ChristArcSin\"])\n",
    "line = slope*mapChrist[\"bmi\"]+intercept\n",
    "plt.plot(mapChrist[\"bmi\"],line, 'r--')\n",
    "print \"Slope: \" + str(slope)\n",
    "print \"R^2  : \" + str(r_value*r_value)\n",
    "print \"p-val: \" + str(p_value)\n",
    "plt.savefig(\"/Users/brooks/Dropbox/American_Gut/Figures/S5_HMP_Overlap/reg_ArcSin_Chr_only.pdf\")\n",
    "plt.show()\n",
    "\n",
    "# Plot Log10\n",
    "mapChrist.plot(kind='scatter', x='bmi', y='ChristLog', c=mapChrist['race'].apply(lambda x: colors[x]), figsize=[10,10])\n",
    "plt.title(\"Log10 Transformation\")\n",
    "\n",
    "mapChrist[\"ChristLog\"] = mapChrist[\"ChristLog\"].replace(to_replace='-inf', value=-10)\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(mapChrist[\"bmi\"],mapChrist[\"ChristLog\"])\n",
    "line = slope*mapChrist[\"bmi\"]+intercept\n",
    "plt.plot(mapChrist[\"bmi\"],line, 'r--')\n",
    "print \"Slope: \" + str(slope)\n",
    "print \"R^2  : \" + str(r_value*r_value)\n",
    "print \"p-val: \" + str(p_value)\n",
    "plt.savefig(\"/Users/brooks/Dropbox/American_Gut/Figures/S5_HMP_Overlap/reg_Log_Chr_only.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center; color:orange;\"> - REGRESSION OF BMI AGAINST CHRISTENSENELLACEAE ABUNDANCE - </h3>\n",
    "<h4 style=\"text-align:center; color:orange;\"> - These are regressions including individuals WITH & WITHOUT Christensenellaceae - </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get log10 transformation of counts\n",
    "mapChrist[\"ChristLog\"] = np.log10(mapChrist[\"Christ\"])\n",
    "# Get arcsin sqrt transformation of counts (see Structure, Function, and Diversity of the Healthy Human Microbiome)\n",
    "mapChrist[\"ChristArcSin\"] = np.arcsin(np.sqrt(mapChrist[\"Christ\"]))\n",
    "\n",
    "# Set Colors\n",
    "colors = {'African American':'red', 'Asian or Pacific Islander':'blue', 'Caucasian':'green', 'Hispanic':'yellow'}\n",
    "\n",
    "# Remove samples w/ no Christ\n",
    "#mapChrist = mapChrist[mapChrist[\"Christ\"] != 0]\n",
    "\n",
    "# Plot ArcSin\n",
    "mapChrist.plot(kind='scatter', x='bmi', y='ChristArcSin', c=mapChrist['race'].apply(lambda x: colors[x]), figsize=[10,10])\n",
    "plt.title(\"ArcSin sqrt Transformation\")\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(mapChrist[\"bmi\"],mapChrist[\"ChristArcSin\"])\n",
    "line = slope*mapChrist[\"bmi\"]+intercept\n",
    "plt.plot(mapChrist[\"bmi\"],line, 'r--')\n",
    "print \"Slope: \" + str(slope)\n",
    "print \"R^2  : \" + str(r_value*r_value)\n",
    "print \"p-val: \" + str(p_value)\n",
    "plt.savefig(\"/Users/brooks/Dropbox/American_Gut/Figures/S5_HMP_Overlap/reg_ArcSin_All.pdf\")\n",
    "plt.show()\n",
    "\n",
    "# Plot Log10\n",
    "mapChrist.plot(kind='scatter', x='bmi', y='ChristLog', c=mapChrist['race'].apply(lambda x: colors[x]), figsize=[10,10])\n",
    "plt.title(\"Log10 Transformation\")\n",
    "\n",
    "mapChrist[\"ChristLog\"] = mapChrist[\"ChristLog\"].replace(to_replace='-inf', value=-20)\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(mapChrist[\"bmi\"],mapChrist[\"ChristLog\"])\n",
    "line = slope*mapChrist[\"bmi\"]+intercept\n",
    "plt.plot(mapChrist[\"bmi\"],line, 'r--')\n",
    "print \"Slope: \" + str(slope)\n",
    "print \"R^2  : \" + str(r_value*r_value)\n",
    "print \"p-val: \" + str(p_value)\n",
    "plt.savefig(\"/Users/brooks/Dropbox/American_Gut/Figures/S5_HMP_Overlap/reg_Log_All_-20.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center; color:orange;\"> - REGRESSION OF BMI AGAINST CHRISTENSENELLACEAE ABUNDANCE SUBSET BY RACE - NOT USED IN MANUSCRIPT - </h3>\n",
    "<h4 style=\"text-align:center; color:orange;\"> - These are regressions only including individuals with Christensenellaceae - </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "# Get log10 transformation of counts\n",
    "mapChrist[\"ChristLog\"] = np.log10(mapChrist[\"Christ\"])\n",
    "# Get arcsin sqrt transformation of counts (see Structure, Function, and Diversity of the Healthy Human Microbiome)\n",
    "mapChrist[\"ChristArcSin\"] = np.arcsin(np.sqrt(mapChrist[\"Christ\"]))\n",
    "\n",
    "\n",
    "def plotChrist(mapTableChrist, raceIn, colorPattern):\n",
    "    mapTableChrist = mapChrist[mapChrist['race'] == raceIn]\n",
    "    \n",
    "    # Exclude Individuals w/o Christ\n",
    "    mapTableChrist = mapTableChrist[mapTableChrist[\"Christ\"] != 0]\n",
    "    \n",
    "    #print mapTableChrist[\"ChristArcSin\"]\n",
    "    mapTableChrist.plot(kind='scatter', x='bmi', y='ChristArcSin', figsize=[10,10])\n",
    "    \n",
    "    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(mapTableChrist[\"bmi\"],mapTableChrist[\"ChristArcSin\"])\n",
    "\n",
    "    line = slope*mapTableChrist[\"bmi\"]+intercept\n",
    "    plt.plot(mapTableChrist[\"bmi\"],line, colorPattern)\n",
    "    plt.title(raceIn, size=30)\n",
    "    \n",
    "    print\n",
    "    print raceIn\n",
    "    print \"Slope: \" + str(slope)\n",
    "    print \"R^2  : \" + str(r_value*r_value)\n",
    "    print \"p-val: \" + str(p_value)\n",
    "    plt.show()\n",
    "\n",
    "plotChrist(mapChrist, \"African American\", 'r--')\n",
    "plotChrist(mapChrist, \"Asian or Pacific Islander\", 'r--')\n",
    "plotChrist(mapChrist, \"Caucasian\", 'r--')\n",
    "plotChrist(mapChrist, \"Hispanic\", 'r--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center; color:orange;\"> - REGRESSION OF BMI AGAINST CHRISTENSENELLACEAE ABUNDANCE SUBSET BY RACE - NOT USED IN MANUSCRIPT - </h3>\n",
    "<h4 style=\"text-align:center; color:orange;\"> - These are regressions including individuals WITH & WITHOUT Christensenellaceae - </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "\n",
    "# Get log10 transformation of counts\n",
    "mapChrist[\"ChristLog\"] = np.log10(mapChrist[\"Christ\"])\n",
    "# Get arcsin sqrt transformation of counts (see Structure, Function, and Diversity of the Healthy Human Microbiome)\n",
    "mapChrist[\"ChristArcSin\"] = np.arcsin(np.sqrt(mapChrist[\"Christ\"]))\n",
    "\n",
    "\n",
    "def plotChrist(mapTableChrist, raceIn, colorPattern):\n",
    "    mapTableChrist = mapChrist[mapChrist['race'] == raceIn]\n",
    "    \n",
    "    # Exclude Individuals w/o Christ\n",
    "    mapTableChrist = mapTableChrist[mapTableChrist[\"Christ\"] != 0]\n",
    "    \n",
    "    # Exclude outliers w/ bmi > 35\n",
    "    #mapTableChrist = mapTableChrist[mapTableChrist[\"bmi\"]  < 35]\n",
    "    \n",
    "    # Include Individuals w/o Christ\n",
    "    #mapTableChrist[\"ChristLog\"] = mapTableChrist[\"ChristLog\"].replace(to_replace='-inf', value=-10)\n",
    "    \n",
    "    \n",
    "    #print mapTableChrist[\"ChristLog\"]\n",
    "    mapTableChrist.plot(kind='scatter', x='bmi', y='ChristLog', figsize=[10,10])\n",
    "    \n",
    "    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(mapTableChrist[\"bmi\"],mapTableChrist[\"ChristLog\"])\n",
    "\n",
    "    line = slope*mapTableChrist[\"bmi\"]+intercept\n",
    "    plt.plot(mapTableChrist[\"bmi\"],line, colorPattern)\n",
    "    plt.title(raceIn, size=30)\n",
    "    \n",
    "    print\n",
    "    print raceIn\n",
    "    print \"Slope: \" + str(slope)\n",
    "    print \"R^2  : \" + str(r_value*r_value)\n",
    "    print \"p-val: \" + str(p_value)\n",
    "    plt.show()\n",
    "\n",
    "plotChrist(mapChrist, \"African American\", 'r--')\n",
    "plotChrist(mapChrist, \"Asian or Pacific Islander\", 'r--')\n",
    "plotChrist(mapChrist, \"Caucasian\", 'r--')\n",
    "plotChrist(mapChrist, \"Hispanic\", 'r--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h1 style=\"text-align:center; color:black;\">------------------------------------------------------------------------------</h1>\n",
    "<h1 style=\"text-align:center; color:orange;\"> - Additional Imports - </h1>\n",
    "<h3 style=\"text-align:center; color:blue;\"> - Rarefied Tables - </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PATH TO RAREFIED TABLES #\n",
    "tablesFP = \"23_0_multiple_rarefactions_1000\"\n",
    "\n",
    "def import_rare_tables(rareFolderIn):\n",
    "    print \" - Importing Rarefied Tables - \"\n",
    "    # ARRAY TO STORE ALL RAREFIED TABLES #\n",
    "    rareTablesIn = []\n",
    "    # DICTIONARY (SAMPLE) OF DICTIONARY (OTUS) OF ARRAY (EACH TABLES COUNT) #\n",
    "    countsDict = {}\n",
    "    \n",
    "    # LOOP THROUGH ALL TABLES #\n",
    "    for idx, i in enumerate(glob.glob(rareFolderIn + \"/*\")):\n",
    "        \n",
    "        if idx % 10 == 0: print idx\n",
    "        # INPUT RAREFIED TABLE #\n",
    "        rareTableIn = load_table(i)\n",
    "        # STORE RAREFIED TABLE #\n",
    "        rareTablesIn.append(rareTableIn)\n",
    "        \n",
    "        # GET NON-ZERO LOCATIONS [[otuLocs], [sampleLocs]] #\n",
    "        nnzLocs = rareTableIn.matrix_data.nonzero()\n",
    "        # GET SAMPLES #\n",
    "        samplesRare = rareTableIn.ids(axis='sample')\n",
    "        # GET OTUS #\n",
    "        otusRare = rareTableIn.ids(axis='observation')\n",
    "        \n",
    "        # INITIATE DICTIONARY SAMPLES #\n",
    "        if idx == 0:\n",
    "            for j in samplesRare: countsDict[j] = {}\n",
    "\n",
    "        # FOR EACH NON-ZERO LOCATION #\n",
    "        for k in np.arange(len(nnzLocs[0])):\n",
    "            \n",
    "            # CHECK IF OTU IS IN SAMPLE'S DICTIONARY AND IF NOT INITIATE AS ARRAY #\n",
    "            if otusRare[nnzLocs[0][k]] not in countsDict[samplesRare[nnzLocs[1][k]]]: countsDict[samplesRare[nnzLocs[1][k]]][otusRare[nnzLocs[0][k]]] = []\n",
    "            \n",
    "            # ADD NON-ZERO VALUE TO THE ARRAY FOR THE SPECIFIC SAMPLE AND OTU\n",
    "            countsDict[samplesRare[nnzLocs[1][k]]][otusRare[nnzLocs[0][k]]].append(rareTableIn.get_value_by_ids(otusRare[nnzLocs[0][k]], samplesRare[nnzLocs[1][k]]))\n",
    "        \n",
    "        \n",
    "    return rareTablesIn, countsDict\n",
    "\n",
    "rareTables, rareCounts = import_rare_tables(tablesFP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center; color:blue;\"> - Beta Diversity - </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PATH TO BETA DIVERSITY DISTANCE MATRICES #\n",
    "betasFP = \"24_0_beta_diversity_1000_bray_curtis\"\n",
    "# CREATE EMPTY LIST TO STORE DATA FRAMES #\n",
    "betasDFs = []; \n",
    "# FOR EACH DATAFRAME... GO THROUGH AND IMPORT #\n",
    "print \" - Importing Dataframes - \"\n",
    "for idx, i in enumerate(glob.glob(betasFP + \"/*\")):\n",
    "    #print idx, i\n",
    "    betasDFs.append(pd.read_csv(i, sep='\\t', index_col=0, skiprows=None, verbose=False))\n",
    "\n",
    "# COLLATE THE DATAFRAMES #\n",
    "print \" - Collating Dataframes - \"\n",
    "betasCollate = pd.concat(betasDFs[:]) \n",
    "\n",
    "# FIND AND WRITE MEAN DATAFRAME #\n",
    "print \" - Calculating and Saving Mean - \"\n",
    "betasMean = betasCollate.groupby(betasCollate.index).mean()\n",
    "#pandasMean.to_csv(dirPath + \"mean_distances.txt\", sep=\"\\t\")\n",
    "\n",
    "# FIND AND WRITE STANDARD DEVIATION DATAFRAME #\n",
    "print \" - Calculating and Saving Standard Deviation - \"\n",
    "betasStd = betasCollate.groupby(betasCollate.index).std()\n",
    "#pandasStd.to_csv(dirPath + \"std_distances.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center; color:blue;\"> - PCoA - </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pcoasFP = \"24_3_principle_coordinates_1000_bray_curtis/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h1 style=\"text-align:center; color:black;\">------------------------------------------------------------------------------</h1>\n",
    "<h1 style=\"text-align:center; color:orange;\"> - Additional Imports - </h1>\n",
    "<h3 style=\"text-align:center; color:blue;\"> - Rarefied Tables - </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FILTER OTUS WITH LESS THAN 10 COUNTS #\n",
    "filterTable = filter_otu_mincount(biomTable, 10)\n",
    "get_table_info(filterTable)\n",
    "\n",
    "# GET TABLE AS RELATIVE ABUNDANCE #\n",
    "relativeTable = get_table_relative(filterTable)\n",
    "\n",
    "# GET 2D COUNT ARRAY OF RELAITVE TABLE #\n",
    "relativeCountArray = count_array(relativeTable)\n",
    "\n",
    "# CONVERT 2D ARRAY TO MATRIX #\n",
    "rMat = np.matrix(relativeCountArray)\n",
    "\n",
    "# GET OTU CORRELATION MATRIX #\n",
    "otuCorr = np.corrcoef(rMat.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.random.seed(51990) # random seed for consistency\n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def pca_2d(dataIn, figBool=False):\n",
    "    \n",
    "    # ESTABLISH PCA MODEL AND NUMBER OF COMPONENTS #\n",
    "    pca = PCA(n_components=2)\n",
    "    pcaVec = pca.fit_transform(dataIn)\n",
    "\n",
    "    if figBool == True:\n",
    "        plt.figure(figsize=[10,10])\n",
    "        plt.scatter(pcaVec[:, 0], pcaVec[:, 1], alpha=.8, lw=2)\n",
    "        plt.title('PCA')\n",
    "        plt.xlabel(\"PC1: \" + str(pca.explained_variance_ratio_[0]))\n",
    "        plt.ylabel(\"PC2: \" + str(pca.explained_variance_ratio_[1]))\n",
    "        plt.show()\n",
    "    else: \n",
    "        print(\"Explained variance of components: \" + str(pca.explained_variance_ratio_))\n",
    "    \n",
    "    return pcaVec\n",
    "    \n",
    "pca_2d(otuCorr, figBool=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h1 style=\"text-align:center; color:black;\">------------------------------------------------------------------------------</h1>\n",
    "<h1 style=\"text-align:center; color:orange;\"> - Beta Diversity - </h1>\n",
    "<h2 style=\"text-align:center; color:blue;\"> - Calculate Beta Diversity - </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### MANTEL CORRELATION OF DISTANCE MATRICES ###\n",
    "from skbio.stats.distance import mantel\n",
    "def mantel(dm1In, dm2In):\n",
    "    print \" - Performing Mantel Test - \"\n",
    "    rMantel, pMantel, dimMantel = mantel(dm1In, dm2In)\n",
    "    print \"   Mantel R-value   | \",rMantel\n",
    "    print \"   Mantel P-value   | \",pMantel\n",
    "    print \"   Mantel Dimensions| \",dimMantel\n",
    "    return rMantel, pMantel, dimMantel\n",
    "mantel(dmBrayCurtis, dmJaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### ANOSIM ASSOCIATION OF DISTANCE MATRIX AND METADATA CATEGORY ###\n",
    "from skbio.stats.distance import ANOSIM\n",
    "print \" - Perform Anosim Test - \"\n",
    "anosim = ANOSIM(dmBraycurtis, mapDf, column='race')\n",
    "results = anosim(999)\n",
    "print \"   Test Statistic| \",results.statistic\n",
    "print \"   P-Value       | \",results.p_value\n",
    "print \"   Permutations  | \",results.permutations\n",
    "print \"   Groups        | \",results.groups\n",
    "print \"   Sample Size   | \",results.sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skbio.stats.distance import PERMANOVA\n",
    "print \" - Perform Permanova Test - \"\n",
    "permanovaResults = PERMANOVA(dmBraycurtis, mapDf, column='race')\n",
    "results = permanovaResults(9999)\n",
    "print \"   Test Statistic| \",results.statistic\n",
    "print \"   P-Value       | \",results.p_value\n",
    "print \"   Permutations  | \",results.permutations\n",
    "print \"   Groups        | \",results.groups\n",
    "print \"   Sample Size   | \",results.sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skbio.stats.distance import bioenv\n",
    "print \" - Perform BioEnv Test - \"\n",
    "print \"   (Correlation b/t Distance Matrix and Continuous Variable)\"\n",
    "bioenvResults = bioenv(dmBraycurtis, mapDf, columns=['age_years', 'bmi'])\n",
    "print bioenvResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; color:black;\">------------------------------------------------------------------------------</h1>\n",
    "<h1 style=\"text-align:center; color:orange;\"> - Custom Pipelines - </h1>\n",
    "<h3 style=\"text-align:center; color:red;\"> - Generate Taxonomic Bar Chart - </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center; color:red;\"> - OTU Abundance Cutoff Analysis - </h3>\n",
    "<h5 style=\"text-align:left; color:black;\">Filter OTUs by minimum abundance cutoffs and write tables</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### BIOM ABUNDANT ### - filter the otu table to different min_counts and analyze\n",
    "def biom_abundant(bt, out_fp):\n",
    "    # MIN_COUNTS TO FILTER BY\n",
    "    min_nums = [2, 3, 4, 5, 10, 20, 30, 40, 50, 75, 100, 250, 500, 750, 1000, 2500, 5000, 10000]\n",
    "    # GENERATE ABUNDANT FOLDER\n",
    "    abun_folder = dirPath+'abundant_partition/'\n",
    "    if not os.path.isdir(abun_folder): os.makedirs(abun_folder)\n",
    "    # LOOP THROUGH MIN_COUNTS\n",
    "    for min_num in min_nums:        \n",
    "        # FILTER BIOM TABLE\n",
    "        ft = filter_otu_mincount(bt, min_num)\n",
    "        # WRITE THE FILTERED TABLE\n",
    "        write_table(ft, abun_folder + str(min_num) + '_filtered_table.txt')\n",
    "        # GET GENERAL INFO\n",
    "        get_table_info(ft, writePath=dirPath+'abundant_partition/'+str(min_num)+\"_summary_filtered_otus.txt\", printOut=False)\n",
    "\n",
    "biom_abundant(biomTable, dirPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center; color:red;\"> - Filter Map to Samples in Table - </h3>\n",
    "<h5 style=\"text-align:left; color:black;\">Filter a mapping file to </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; color:black;\">------------------------------------------------------------------------------</h1>\n",
    "<h1 style=\"text-align:center; color:orange;\"> - In Progress - </h1>\n",
    "<h3 style=\"text-align:center; color:red;\"> - Perform Rarefactions - </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rarefaction(bt, rarefactionCutoff=None, seed=200):\n",
    "    prng = np.random.RandomState(seed) # Set random seed for reproducible results\n",
    "    if rarefactionCutoff == None: rarefactionCutoff = min(bt.sum('sample')) # If no rarefaction cutoff provided = minimum sample sum of counts\n",
    "    iterSamples = bt.copy() # Make a copy of the table to store rarefied values\n",
    "    for idx, (values, id, metadata) in enumerate(iterSamples): # for each sample\n",
    "        probability_measured = values / float(sum(values)) # probability of sampling = sample relative frequency\n",
    "        choice = prng.choice(len(bt.ids(axis='observation')), rarefactionCutoff, p=probability_measured)\n",
    "        print idx, choice\n",
    "\"\"\" \n",
    "    trarefied = np.empty_like(bt)\n",
    "    for i in range(bt.shape[0]): # for each sample\n",
    "        \n",
    "        p = bt[i] / float(sample_counts_rarefaction[i]) # relative frequency / probability\n",
    "        choice = prng.choice(number_of_observations, rarefactionCutoff, p=p)\n",
    "        trarefied[i] = np.bincount(choice, minlength=nvar)\n",
    "\n",
    "    return trarefied\"\"\"\n",
    "rarefaction(biomTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
