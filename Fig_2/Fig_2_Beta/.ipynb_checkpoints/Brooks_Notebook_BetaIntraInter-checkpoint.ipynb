{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; color:black;\">------------------------------------------------------------------------</h1>\n",
    "<h1 style=\"text-align:center; color:orange;\"> - Load Data - </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd # PANDAS DATAFRAMES\n",
    "from skbio.stats.distance import DistanceMatrix # DISTANCE MATRIX OBJECT FOR BETA DIVERSITY\n",
    "import itertools # ACCESS DATA ITERATION COMBINATIONS\n",
    "import numpy as np  # NUMPY NUMERICAL AND LINEAR ALGEBRA TOOLKIT\n",
    "import scipy as sp  # SCIPY SCIENTIFIC TOOLKIT\n",
    "import random       # TO GENERATE RANDOM VALUES\n",
    "random.seed(54321)  # SET RANDOM SEED FOR REPRODUCIBILITY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; color:black;\">------------------------------------------------------------------------</h1>\n",
    "<h1 style=\"text-align:center; color:orange;\"> - Beta Intra Inter Function - </h1>\n",
    "<h3 style=\"text-align:center; color:blue;\"> Compare beta diversity distances within and between categorical groups of samples. </h3>\n",
    "<h4 style=\"text-align:left; color:black;\"> Input the path to a beta diversity distance matrix (dmPath), a QIIME format mapping file (mapPath), and specify a categorical variable in the mapping file to separate distances (mapCategory). Subsample specifies the number of samples to subsample for each group (subsample=)  </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### BETA DIVERSITY - INTRA/INTER DISTANCE COMPARISONS ###\n",
    "def beta_intra_inter(dmPath, mapPath, mapCategory, subsample=100000):\n",
    "\n",
    "    ##### IMPORT DISTANCE MATRIX AS DATAFRAME #####\n",
    "    dmIn = DistanceMatrix.read(dmPath).to_data_frame()\n",
    "\n",
    "    ##### IMPORT MAP #####\n",
    "    mapIn = pd.read_csv(mapPath,sep='\\t',low_memory=False)\n",
    "    ### Set sample id as index ###\n",
    "    mapIn.set_index('#SampleID',append=False,inplace=True)\n",
    "    ### Get unique values in mapping category ###\n",
    "    mapCats = list(mapIn[mapCategory].unique())\n",
    "\n",
    "    ##### SORTING / ORGANIZING DATA #####\n",
    "    ### Add category to distance matrix ###\n",
    "    dmCat = pd.concat([dmIn, mapIn[mapCategory]],axis=1,join='inner')\n",
    "\n",
    "    ##### STORAGE STRUCTURES #####\n",
    "    allDists = {}\n",
    "    meanDists = {}\n",
    "\n",
    "    ##### SUBSAMPLE INDICES #####\n",
    "    ### Create a dictionary to hold indices for each grouping ###\n",
    "    groupIndices = {}\n",
    "    ### For each group... store indices ###\n",
    "    for curGroup in mapCats:\n",
    "        ### If more samples than subsample depth... Subsample and store ###\n",
    "        if len(dmCat[dmCat[mapCategory] == curGroup].index) > subsample: \n",
    "            groupIndices[curGroup] = list(dmCat[dmCat[mapCategory] == curGroup].sample(n=subsample, axis=0, replace=False).index)\n",
    "        ### Else just use all samples from that group ###\n",
    "        else: groupIndices[curGroup] = list(dmCat[dmCat[mapCategory] == curGroup].index)\n",
    "\n",
    "    ##### CALCULATING GROUP DISTANCES #####\n",
    "    ### For every combination of groups ###\n",
    "    for curCompare in itertools.product(mapCats,repeat=2):\n",
    "\n",
    "        ##### FORMAT DM #####\n",
    "        ### Get Distances ###\n",
    "        curDM = dmCat.loc[groupIndices[curCompare[0]],groupIndices[curCompare[1]]]\n",
    "        ### If intra group distance we only want to take values from the top half of matrix ###\n",
    "        if curCompare[0] == curCompare[1]:\n",
    "            ### Set all values below diagonal to 0.0 ###\n",
    "            curDM = pd.DataFrame(np.triu(curDM, k=0), index=curDM.index, columns=curDM.columns)\n",
    "            ### Set all 0.0 values to np.nan (if two samples are identical (i.e. 0.0 dist) will be excluded!!! You probably did something wrong if that is the case :)\n",
    "            curDM[curDM==0.0]=np.nan\n",
    "        ### Flatten all distances into a list ###\n",
    "        allCurDists = curDM.values.flatten()\n",
    "        ### Remove nan (for intra) ###\n",
    "        allCurDists = allCurDists[~np.isnan(allCurDists)]\n",
    "\n",
    "        ##### AVERAGE SAMPLE DISTANCES BETWEEN GROUPS #####\n",
    "        if curCompare[0] not in meanDists.keys(): meanDists[curCompare[0]] = {}\n",
    "        meanDists[curCompare[0]][curCompare[1]] = curDM.mean(axis=1, skipna=True)\n",
    "\n",
    "        ##### ALL SAMPLE DISTANCES BETWEEN GROUPS #####\n",
    "        if curCompare[0] not in allDists.keys(): allDists[curCompare[0]] = {}\n",
    "        allDists[curCompare[0]][curCompare[1]] = allCurDists\n",
    "\n",
    "    ### Dataframe for Results ###\n",
    "    dfOut = pd.DataFrame(columns=['dists','type','subsample','depth', 'metric','g0','g0mean','g0count','g1','g1mean','g1count','pval','tstat']); curIDX = 0\n",
    "\n",
    "    ##### COMPARE GROUP DISTANCES #####\n",
    "    ### For each combination of groups... ###\n",
    "    for curCompare in itertools.combinations(mapCats,2):\n",
    "\n",
    "        ### Calculate INTRA-INTRA Mann-Whitney-U on ALL Distances ###\n",
    "        outMann = sp.stats.mannwhitneyu(allDists[curCompare[0]][curCompare[0]], allDists[curCompare[1]][curCompare[1]], use_continuity=True, alternative='two-sided')\n",
    "        dfOut.loc[curIDX] = ['all','intra-intra',subsample,np.nan,np.nan,curCompare[0],np.mean(allDists[curCompare[0]][curCompare[0]]), len(allDists[curCompare[0]][curCompare[0]]),curCompare[1],np.mean(allDists[curCompare[1]][curCompare[1]]), len(allDists[curCompare[1]][curCompare[1]]),outMann[1], outMann[0]]; curIDX+=1\n",
    "\n",
    "        ### Calculate INTRA-INTER Mann-Whitney-U on ALL Distances ###\n",
    "        outMann = sp.stats.mannwhitneyu(allDists[curCompare[0]][curCompare[0]],allDists[curCompare[0]][curCompare[1]], use_continuity=True, alternative='two-sided')\n",
    "        dfOut.loc[curIDX] = ['all','intra-inter',subsample,np.nan,np.nan,curCompare[0],np.mean(allDists[curCompare[0]][curCompare[0]]),len(allDists[curCompare[0]][curCompare[0]]),(curCompare[0]+'-'+curCompare[1]),np.mean(allDists[curCompare[0]][curCompare[1]]),len(allDists[curCompare[0]][curCompare[1]]),outMann[1], outMann[0]]; curIDX+=1\n",
    "\n",
    "        ### Calculate INTRA-INTER Mann-Whitney-U on ALL Distances ###\n",
    "        outMann = sp.stats.mannwhitneyu(allDists[curCompare[1]][curCompare[1]],allDists[curCompare[0]][curCompare[1]], use_continuity=True, alternative='two-sided')\n",
    "        dfOut.loc[curIDX] = ['all','intra-inter',subsample,np.nan,np.nan,curCompare[1],np.mean(allDists[curCompare[1]][curCompare[1]]),len(allDists[curCompare[1]][curCompare[1]]),(curCompare[1]+'-'+curCompare[0]),np.mean(allDists[curCompare[0]][curCompare[1]]),len(allDists[curCompare[0]][curCompare[1]]),outMann[1], outMann[0]]; curIDX+=1\n",
    "\n",
    "        ### Calculate INTRA-INTRA Mann-Whitney-U on MEAN Distances ### \n",
    "        outMann = sp.stats.mannwhitneyu(meanDists[curCompare[0]][curCompare[0]],meanDists[curCompare[1]][curCompare[1]],use_continuity=True, alternative='two-sided')\n",
    "        dfOut.loc[curIDX] = ['mean','intra-intra',subsample,np.nan,np.nan,curCompare[0],np.mean(meanDists[curCompare[0]][curCompare[0]]), len(meanDists[curCompare[0]][curCompare[0]]),curCompare[1],np.mean(meanDists[curCompare[1]][curCompare[1]]), len(meanDists[curCompare[1]][curCompare[1]]),outMann[1], outMann[0]]; curIDX+=1\n",
    "\n",
    "        ### Calculate INTRA-INTER Mann-Whitney-U on MEAN Distances ### \n",
    "        outMann = sp.stats.mannwhitneyu(meanDists[curCompare[0]][curCompare[0]],meanDists[curCompare[0]][curCompare[1]],use_continuity=True, alternative='two-sided')\n",
    "        dfOut.loc[curIDX] = ['mean','intra-inter',subsample,np.nan,np.nan,curCompare[0],np.mean(meanDists[curCompare[0]][curCompare[0]]), len(meanDists[curCompare[0]][curCompare[0]]),curCompare[0]+'-'+curCompare[1],np.mean(meanDists[curCompare[0]][curCompare[1]]), len(meanDists[curCompare[0]][curCompare[1]]),outMann[1], outMann[0]]; curIDX+=1\n",
    "\n",
    "        ### Calculate INTRA-INTER Mann-Whitney-U on MEAN Distances ### \n",
    "        outMann = sp.stats.mannwhitneyu(meanDists[curCompare[1]][curCompare[1]],meanDists[curCompare[1]][curCompare[0]],use_continuity=True, alternative='two-sided')\n",
    "        dfOut.loc[curIDX] = ['mean','intra-inter',subsample,np.nan,np.nan,curCompare[1],np.mean(meanDists[curCompare[1]][curCompare[1]]), len(meanDists[curCompare[1]][curCompare[1]]),curCompare[1]+'-'+curCompare[0],np.mean(meanDists[curCompare[1]][curCompare[0]]), len(meanDists[curCompare[1]][curCompare[0]]),outMann[1], outMann[0]]; curIDX+=1\n",
    "\n",
    "    return dfOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; color:black;\">------------------------------------------------------------------------</h1>\n",
    "<h1 style=\"text-align:center; color:orange;\"> - Beta Intra Inter Pipeline - </h1>\n",
    "<h4 style=\"text-align:left; color:black;\"> Pick a mapping category and the number of times to subsample, then loop through depths, beta diversity metrics, and the subsample set of samples.  </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - All Intra-Group Average Caucasian-Caucasian: 0.805934447523\n",
      "   - All Intra-Group Average Hispanic-Hispanic: 0.754242972973\n",
      "   - All Intra-Group Average Asian or Pacific Islander-Asian or Pacific Islander: 0.829118986416\n",
      "   - All Intra-Group Average African American-African American: 0.812741666667\n",
      "\n",
      "   - All Inter-Group Average Caucasian-Hispanic: 0.788113244554\n",
      "   - All Inter-Group Average Caucasian-Asian or Pacific Islander: 0.823720624403\n",
      "   - All Inter-Group Average Caucasian-African American: 0.817233919532\n",
      "   - All Inter-Group Average Hispanic-Caucasian: 0.788113244554\n",
      "   - All Inter-Group Average Hispanic-Asian or Pacific Islander: 0.809960307125\n",
      "   - All Inter-Group Average Hispanic-African American: 0.798771954262\n",
      "   - All Inter-Group Average Asian or Pacific Islander-Caucasian: 0.823720624403\n",
      "   - All Inter-Group Average Asian or Pacific Islander-Hispanic: 0.809960307125\n",
      "   - All Inter-Group Average Asian or Pacific Islander-African American: 0.8306\n",
      "   - All Inter-Group Average African American-Caucasian: 0.817233919532\n",
      "   - All Inter-Group Average African American-Hispanic: 0.798771954262\n",
      "   - All Inter-Group Average African American-Asian or Pacific Islander: 0.8306\n",
      "\n",
      "\n",
      " - All Intra-Group Average Distance: 0.806005776711\n",
      "   - Number of Intra Values: 769038\n",
      " - All Inter-Group Average Distance: 0.813566337542\n",
      "   - Number of Inter Values: 351174\n",
      " - Compare All Intra-Inter: p=0.0 stat=127994469102.0\n"
     ]
    }
   ],
   "source": [
    "### BETA DIVERSITY - INTRA/INTER DISTANCE COMPARISONS ###\n",
    "def beta_summarize_category(dmPath, mapPath, mapCategory):\n",
    "\n",
    "    ##### IMPORT DISTANCE MATRIX AS DATAFRAME #####\n",
    "    dmIn = DistanceMatrix.read(dmPath).to_data_frame()\n",
    "\n",
    "    ##### IMPORT MAP #####\n",
    "    mapIn = pd.read_csv(mapPath,sep='\\t',low_memory=False)\n",
    "    ### Set sample id as index ###\n",
    "    mapIn.set_index('#SampleID',append=False,inplace=True)\n",
    "    ### Get unique values in mapping category ###\n",
    "    mapCats = list(mapIn[mapCategory].unique())\n",
    "\n",
    "    ##### SORTING / ORGANIZING DATA #####\n",
    "    ### Add category to distance matrix ###\n",
    "    dmCat = pd.concat([dmIn, mapIn[mapCategory]],axis=1,join='inner')\n",
    "\n",
    "    ##### STORAGE STRUCTURES #####\n",
    "    allDists = {}\n",
    "\n",
    "    ##### SUBSAMPLE INDICES #####\n",
    "    ### Create a dictionary to hold indices for each grouping ###\n",
    "    groupIndices = {}\n",
    "    ### For each group... store indices ###\n",
    "    for curGroup in mapCats: groupIndices[curGroup] = list(dmCat[dmCat[mapCategory] == curGroup].index)\n",
    "\n",
    "    ##### CALCULATING GROUP DISTANCES #####\n",
    "    ### For every combination of groups ###\n",
    "    for curCompare in itertools.product(mapCats,repeat=2):\n",
    "\n",
    "        ##### FORMAT DM #####\n",
    "        ### Get Distances ###\n",
    "        curDM = dmCat.loc[groupIndices[curCompare[0]],groupIndices[curCompare[1]]]\n",
    "        ### If intra group distance we only want to take values from the top half of matrix ###\n",
    "        if curCompare[0] == curCompare[1]:\n",
    "            ### Set all values below diagonal to 0.0 ###\n",
    "            curDM = pd.DataFrame(np.triu(curDM, k=0), index=curDM.index, columns=curDM.columns)\n",
    "            ### Set all 0.0 values to np.nan (if two samples are identical (i.e. 0.0 dist) will be excluded!!! You probably did something wrong if that is the case :)\n",
    "            curDM[curDM==0.0]=np.nan\n",
    "        ### Flatten all distances into a list ###\n",
    "        allCurDists = curDM.values.flatten()\n",
    "        ### Remove nan (for intra) ###\n",
    "        allCurDists = allCurDists[~np.isnan(allCurDists)]\n",
    "\n",
    "        ##### ALL SAMPLE DISTANCES BETWEEN GROUPS #####\n",
    "        if curCompare[0] not in allDists.keys(): allDists[curCompare[0]] = {}\n",
    "        allDists[curCompare[0]][curCompare[1]] = allCurDists\n",
    "    \n",
    "    allIntra = []\n",
    "    allInter = []\n",
    "    ### Intra Group Average Distances ###\n",
    "    for curCompare in itertools.product(mapCats,repeat=2):\n",
    "        if curCompare[0] == curCompare[1]: \n",
    "            allIntra.extend(allDists[curCompare[0]][curCompare[1]])\n",
    "            print('   - All Intra-Group Average '+curCompare[0]+'-'+curCompare[1]+': '+str(np.mean(allDists[curCompare[0]][curCompare[1]])))\n",
    "    ### Inter Group Average Distances ###\n",
    "    print()\n",
    "    for curCompare in itertools.product(mapCats,repeat=2):\n",
    "        if curCompare[0] != curCompare[1]: \n",
    "            allInter.extend(allDists[curCompare[0]][curCompare[1]])\n",
    "            print('   - All Inter-Group Average '+curCompare[0]+'-'+curCompare[1]+': '+str(np.mean(allDists[curCompare[0]][curCompare[1]])))\n",
    "    print('\\n')\n",
    "    ### Calculate Average of all Intra and Inter-Group Distances ###\n",
    "    print(' - All Intra-Group Average Distance: '+str(np.mean(allIntra)))\n",
    "    print('   - Number of Intra Values: '+str(len(allIntra)))\n",
    "    print(' - All Inter-Group Average Distance: '+str(np.mean(allInter)))\n",
    "    print('   - Number of Inter Values: '+str(len(allInter)))\n",
    "    ### Calculate Significance ###\n",
    "    outMann = sp.stats.mannwhitneyu(allIntra, allInter, use_continuity=True, alternative='two-sided')\n",
    "    print(' - Compare All Intra-Inter: p='+str(outMann[1])+' stat='+str(outMann[0]))\n",
    "\n",
    "\n",
    "    \n",
    "### Run Results for Bray Curtis Consensus Distances ###    \n",
    "curDepth = 1000\n",
    "curMetric = 'bray_curtis'\n",
    "dmPath = 'test_data/ag_analysis/4_0_beta_diversity/4_0_beta_'+str(curDepth)+'_'+curMetric+'/consensus_dm.txt' \n",
    "mapPath = 'test_data/ag_analysis/1_1_qc_'+str(curDepth)+'_map.txt'\n",
    "\n",
    "beta_summarize_category(dmPath, mapPath, mapCategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 - bray_curtis - 10\n",
      "1000 - bray_curtis - 15\n",
      "1000 - bray_curtis - 20\n",
      "1000 - bray_curtis - 25\n",
      "1000 - bray_curtis - 30\n",
      "1000 - bray_curtis - 35\n",
      "1000 - bray_curtis - 40\n",
      "1000 - bray_curtis - 45\n",
      "1000 - bray_curtis - 50\n",
      "1000 - bray_curtis - 75\n",
      "1000 - bray_curtis - 100\n",
      "1000 - bray_curtis - 250\n",
      "1000 - bray_curtis - 500\n",
      "1000 - bray_curtis - 1000\n",
      "1000 - bray_curtis - 2000\n",
      "1000 - weighted_unifrac - 10\n",
      "1000 - weighted_unifrac - 15\n",
      "1000 - weighted_unifrac - 20\n",
      "1000 - weighted_unifrac - 25\n",
      "1000 - weighted_unifrac - 30\n",
      "1000 - weighted_unifrac - 35\n",
      "1000 - weighted_unifrac - 40\n",
      "1000 - weighted_unifrac - 45\n",
      "1000 - weighted_unifrac - 50\n",
      "1000 - weighted_unifrac - 75\n",
      "1000 - weighted_unifrac - 100\n",
      "1000 - weighted_unifrac - 250\n",
      "1000 - weighted_unifrac - 500\n",
      "1000 - weighted_unifrac - 1000\n",
      "1000 - weighted_unifrac - 2000\n",
      "1000 - binary_jaccard - 10\n",
      "1000 - binary_jaccard - 15\n",
      "1000 - binary_jaccard - 20\n",
      "1000 - binary_jaccard - 25\n",
      "1000 - binary_jaccard - 30\n",
      "1000 - binary_jaccard - 35\n",
      "1000 - binary_jaccard - 40\n",
      "1000 - binary_jaccard - 45\n",
      "1000 - binary_jaccard - 50\n",
      "1000 - binary_jaccard - 75\n",
      "1000 - binary_jaccard - 100\n",
      "1000 - binary_jaccard - 250\n",
      "1000 - binary_jaccard - 500\n",
      "1000 - binary_jaccard - 1000\n",
      "1000 - binary_jaccard - 2000\n",
      "1000 - unweighted_unifrac - 10\n",
      "1000 - unweighted_unifrac - 15\n",
      "1000 - unweighted_unifrac - 20\n",
      "1000 - unweighted_unifrac - 25\n",
      "1000 - unweighted_unifrac - 30\n",
      "1000 - unweighted_unifrac - 35\n",
      "1000 - unweighted_unifrac - 40\n",
      "1000 - unweighted_unifrac - 45\n",
      "1000 - unweighted_unifrac - 50\n",
      "1000 - unweighted_unifrac - 75\n",
      "1000 - unweighted_unifrac - 100\n",
      "1000 - unweighted_unifrac - 250\n",
      "1000 - unweighted_unifrac - 500\n",
      "1000 - unweighted_unifrac - 1000\n",
      "1000 - unweighted_unifrac - 2000\n",
      "10000 - bray_curtis - 10\n",
      "10000 - bray_curtis - 15\n",
      "10000 - bray_curtis - 20\n",
      "10000 - bray_curtis - 25\n",
      "10000 - bray_curtis - 30\n",
      "10000 - bray_curtis - 35\n",
      "10000 - bray_curtis - 40\n",
      "10000 - bray_curtis - 45\n",
      "10000 - bray_curtis - 50\n",
      "10000 - bray_curtis - 75\n",
      "10000 - bray_curtis - 100\n",
      "10000 - bray_curtis - 250\n",
      "10000 - bray_curtis - 500\n",
      "10000 - bray_curtis - 1000\n",
      "10000 - bray_curtis - 2000\n",
      "10000 - weighted_unifrac - 10\n",
      "10000 - weighted_unifrac - 15\n",
      "10000 - weighted_unifrac - 20\n",
      "10000 - weighted_unifrac - 25\n",
      "10000 - weighted_unifrac - 30\n",
      "10000 - weighted_unifrac - 35\n",
      "10000 - weighted_unifrac - 40\n",
      "10000 - weighted_unifrac - 45\n",
      "10000 - weighted_unifrac - 50\n",
      "10000 - weighted_unifrac - 75\n",
      "10000 - weighted_unifrac - 100\n",
      "10000 - weighted_unifrac - 250\n",
      "10000 - weighted_unifrac - 500\n",
      "10000 - weighted_unifrac - 1000\n",
      "10000 - weighted_unifrac - 2000\n",
      "10000 - binary_jaccard - 10\n",
      "10000 - binary_jaccard - 15\n",
      "10000 - binary_jaccard - 20\n",
      "10000 - binary_jaccard - 25\n",
      "10000 - binary_jaccard - 30\n",
      "10000 - binary_jaccard - 35\n",
      "10000 - binary_jaccard - 40\n",
      "10000 - binary_jaccard - 45\n",
      "10000 - binary_jaccard - 50\n",
      "10000 - binary_jaccard - 75\n",
      "10000 - binary_jaccard - 100\n",
      "10000 - binary_jaccard - 250\n",
      "10000 - binary_jaccard - 500\n",
      "10000 - binary_jaccard - 1000\n",
      "10000 - binary_jaccard - 2000\n",
      "10000 - unweighted_unifrac - 10\n",
      "10000 - unweighted_unifrac - 15\n",
      "10000 - unweighted_unifrac - 20\n",
      "10000 - unweighted_unifrac - 25\n",
      "10000 - unweighted_unifrac - 30\n",
      "10000 - unweighted_unifrac - 35\n",
      "10000 - unweighted_unifrac - 40\n",
      "10000 - unweighted_unifrac - 45\n",
      "10000 - unweighted_unifrac - 50\n",
      "10000 - unweighted_unifrac - 75\n",
      "10000 - unweighted_unifrac - 100\n",
      "10000 - unweighted_unifrac - 250\n",
      "10000 - unweighted_unifrac - 500\n",
      "10000 - unweighted_unifrac - 1000\n",
      "10000 - unweighted_unifrac - 2000\n"
     ]
    }
   ],
   "source": [
    "### Map Category ###\n",
    "mapCategory = 'race'\n",
    "\n",
    "### Number of times to repeatedly subsample ###\n",
    "numSubSample = 10\n",
    "### Store Resulting Dataframes in List ###\n",
    "xList = []\n",
    "\n",
    "### For each Rarefaction Depth ###\n",
    "for curDepth in [1000,10000]:\n",
    "    \n",
    "    ### For each beta diversity metric ###\n",
    "    for curMetric in ['bray_curtis','weighted_unifrac','binary_jaccard','unweighted_unifrac']:\n",
    "        \n",
    "        ### For each desired depth of subsampling ###\n",
    "        for subsample in [10,15,20,25,30,35,40,45,50,75,100,250,500,1000,2000]:\n",
    "    \n",
    "            print(str(curDepth)+' - '+curMetric+' - '+str(subsample))\n",
    "            ### For the Number of Repeated Subsamples ###\n",
    "            for numSub in np.arange(numSubSample):\n",
    "                ### Perform the beta intra / inter analysis ###\n",
    "                xOut = beta_intra_inter('test_data/ag_analysis/4_0_beta_diversity/4_0_beta_'+str(curDepth)+'_'+curMetric+'/consensus_dm.txt', \n",
    "                                        'test_data/ag_analysis/1_1_qc_'+str(curDepth)+'_map.txt', mapCategory, subsample=subsample)\n",
    "                ### Set metric and depth in output ###\n",
    "                xOut['metric'] = curMetric\n",
    "                xOut['depth'] = curDepth\n",
    "                ### Store Results ###\n",
    "                xList.append(xOut)\n",
    "\n",
    "### Concatenate Results ###\n",
    "xFinal = pd.concat(xList, axis=0)\n",
    "### Update Index ###\n",
    "xFinal = xFinal.groupby(['dists','type','subsample','g0','g1','metric','depth']).mean().T\n",
    "### Save all statistics ###\n",
    "xFinal.to_csv('test_data/ag_analysis/4_1_beta_intra_inter_stats.txt',sep='\\t')\n",
    "\n",
    "##### Table with P-values stacked and other stats removed #####\n",
    "### Let's make another copy to stack the pvalues by subsample level ###\n",
    "xManipulate = pd.concat(xList, axis=0)\n",
    "xStacked = 'none'\n",
    "### For each subsample depth ###\n",
    "subsUnique = xManipulate['subsample'].unique()\n",
    "for subS in subsUnique:\n",
    "    ### Initialize as single subsample depth ###\n",
    "    if isinstance(xStacked, str): xStacked = xManipulate[xManipulate['subsample'] == subS].copy()\n",
    "    ### And always set column name to subsample depth ###\n",
    "    xStacked[subS] = list(xManipulate[xManipulate['subsample'] == subS]['pval'])\n",
    "\n",
    "### Remove Non-pvalue columns ###\n",
    "xStacked = xStacked.drop(['g0mean','g1mean','subsample','pval','tstat','g0count','g1count'], 1)\n",
    "### Groupby ###\n",
    "xStacked = xStacked.groupby(['dists','type','g0','g1','metric','depth']).mean().T  \n",
    "### Save ###\n",
    "xStacked.to_csv('test_data/ag_analysis/4_1_beta_intra_inter_pstacked.txt',sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:qiime2]",
   "language": "python",
   "name": "conda-env-qiime2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
